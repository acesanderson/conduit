{
  "_default": {
    "18": {
      "model": "llama3.1:latest",
      "description": "Llama3.1:latest is an earlier release in Meta's Llama 3 series, available in multiple parameter sizes (e.g., 8B, 70B). It is a text-only model for tasks like chat, code generation, summarization, and general language understanding. The model does not process images, audio, or video. Context window is typically around 8,000 to 32,000 tokens. Strong in reasoning and instruction following.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32000,
      "parameter_count": "varies",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "24": {
      "model": "gpt-4.1",
      "description": "GPT-4.1 is OpenAI's flagship large language model, designed for complex tasks, reasoning, and high-fidelity text generation. It is based on a transformer architecture and supports both text and image inputs (multimodal). The model features an expanded context window — typically 128k tokens — enabling it to manage lengthy interactions and context-rich documents. GPT-4.1 supports advanced reasoning and function calling. Known for robust performance on diverse benchmarks, it was released in early 2025 and is intended for applications requiring nuanced understanding and generation, including content creation, education, and consultation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "25": {
      "model": "gpt-4o",
      "description": "GPT-4o (\"omni\") is OpenAI's most advanced flagship model as of April 2025. It is fully multimodal, accepting text, image, and audio inputs, and can generate text and audio outputs. It features real-time, low-latency capabilities for conversation and supports a large context window, reportedly up to 128k tokens. GPT-4o is suitable for complex reasoning, function calling, and integration into conversational and assistant applications. Designed for seamless multimodal interaction, it excels in accessibility and real-world user engagement.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "26": {
      "model": "gpt-4-turbo",
      "description": "GPT-4 Turbo is an optimized variant of GPT-4 offering faster inference and lower costs, while maintaining the core GPT-4 architecture's capabilities. It is primarily focused on text and code generation with an extended context window (up to 128k tokens). Turbo does not natively support image, audio, or video processing. Released in late 2023, it is intended for scalable applications such as chatbots, code assistance, and generative tools where throughput and performance are important.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "27": {
      "model": "gpt-3.5-turbo-0125",
      "description": "GPT-3.5 Turbo (0125) is a cost-effective, high-performance text generation model based on the GPT-3.5 architecture. It supports text generation and completion tasks with a context window of up to 16,385 tokens. While capable in conversation, coding, and basic reasoning, GPT-3.5 Turbo does not have multimodal capabilities (image, audio, or video). It was released in early 2024 and is most commonly used for chatbots, drafting, summarization, and general text-based automation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16385,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "28": {
      "model": "gpt-4o-mini",
      "description": "GPT-4o Mini is a lighter, more efficient variant of the GPT-4o model. It is designed to balance speed and capability, featuring support for text and image input while maintaining strong reasoning abilities. Its context window is smaller than GPT-4o, optimized for lower latency and cost. Intended use cases include lightweight conversational agents and tasks where full GPT-4o performance is not required. Unlike GPT-4o, it does not natively process audio or video.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "29": {
      "model": "gpt-4o-audio-preview",
      "description": "GPT-4o Audio Preview is a specialized variant of GPT-4o, focused on low-latency speech recognition (speech-to-text), speech translation, and text-to-speech (audio generation). It is designed for \"speech in, speech out\" conversational systems and supports multimodal input. With real-time audio capabilities and strong reasoning, it is well-suited for accessibility tools, voice assistants, and rapid dialogue applications. Its context window size and parameter count are not publicly detailed; released in 2025.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "35": {
      "model": "claude-3-5-haiku-20241022",
      "description": "Claude 3.5 Haiku (October 2024) is part of Anthropic's third-generation model lineup, designed as the fastest and most cost-effective tier. Built on a large transformer architecture, it excels in rapid text processing, general-purpose text and code tasks, and supports image analysis (multimodal input) but does not generate images or audio. It typically features a substantial context window (up to 200K tokens in advanced versions) and is optimized for applications requiring high throughput and responsiveness. Main use cases include content generation, summarization, and analysis at scale. Notable limitations include a lack of image, audio, and video generation capabilities.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "36": {
      "model": "claude-3-7-sonnet-20250219",
      "description": "Claude 3.7 Sonnet (February 2025) is Anthropic's mid-size transformer-based model in the Claude 3 family, targeting a balance of capability, speed, and operating cost. It supports robust text generation, strong reasoning, and image analysis through multimodal inputs, but does not generate images, audio, or video. With a large context window (likely up to 200K tokens), it is intended for high-volume applications such as data analysis, code synthesis, search, and workflow orchestration. Limitations include no image or audio output and no video understanding or production.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "38": {
      "model": "claude-sonnet-4-20250514",
      "description": "Claude Sonnet 4 (May 2025) is a mid-size transformer model in Anthropic’s Claude portfolio, optimized for quality, responsiveness, and cost-efficiency. With a 200K token context window, it tackles high-volume use cases such as workflow automation, data analysis, and code generation. Sonnet 4 supports robust text generation, advanced reasoning, and image understanding (vision input), but does not create images, audio, or video. It is well-suited for scalable task-specific applications within larger AI systems.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "41": {
      "model": "gemini-2.5-pro-preview-05-06",
      "description": "Gemini 2.5 Pro (Preview 05-06) is Google's most advanced reasoning model in the Gemini series, released in 2025. It is capable of handling and integrating information from large, diverse datasets, including text, audio, imagery, video, and code repositories. Its context window is similar to the Gemini 2.5 series (on the order of 1 million tokens). The model is optimized for deep reasoning, multimodal analysis, research, and enterprise use. It can analyze but not generate images, audio, or video.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "46": {
      "model": "llama3-8b-8192",
      "description": "Llama3-8b-8192 is an 8 billion parameter transformer-based language model, featuring an 8,192 token context window. Designed primarily for text-based tasks, it excels in text completion, dialogue, and reasoning. The model supports function calling and is optimized for rapid inference. It is primarily intended for chatbot, code generation, and conversational AI use cases, leveraging Groq’s fast inference hardware for low-latency outputs.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "8B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "47": {
      "model": "llama3-70b-8192",
      "description": "Llama3-70b-8192 is a 70 billion parameter transformer-based language model with an 8,192 token context window. It is highly optimized for dialogue, content generation, and complex reasoning tasks. The model maintains a strong MMLU score (79.5%) and supports function calling and tool use. Designed for production-ready, fast, and consistent outputs, it is widely used for advanced chatbots, research, and large-scale automation.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "70B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "48": {
      "model": "mixtral-8x7b-32768",
      "description": "Mixtral-8x7b-32768 is a mixture-of-experts model with 8 experts of 7B parameters each (total active parameters per forward pass: ~12.9B), featuring a 32,768 token context window. It is designed for advanced text generation, reasoning, and cost-effective inference at scale. The model focuses on open-ended dialogue, code generation, and large-document processing, leveraging Groq’s speed for real-time interaction.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 32768,
      "parameter_count": "8x7B (active ~12.9B per pass)",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "49": {
      "model": "gemma2-9b-it",
      "description": "Gemma2-9b-it is a 9 billion parameter language model from Google’s Gemma 2 series, likely deployed on Groq for fast inference. It features a transformer architecture and is optimized for instruction following and text-based tasks. The model supports text completion and reasoning, focusing on instruction-based dialogue, summarization, and knowledge extraction. Multimodal or image/audio/video capabilities are not available.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "9B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "50": {
      "model": "llama-3.3-70b-versatile",
      "description": "Llama-3.3-70b-versatile is a 70 billion parameter transformer-based model, likely a variant of Meta’s Llama 3, optimized for versatile use cases with a large context window. It supports advanced reasoning, function calling, and tool use. The model is designed for complex language understanding, automation, and research applications, but it does not support image, audio, or video analysis or generation.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "70B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "53": {
      "model": "sonar-reasoning",
      "description": "Sonar-reasoning is likely a variant of the Sonar model, optimized for logical reasoning tasks. It is built on top of the Llama 3.3 70B architecture and focuses on providing accurate and factual responses. It does not support multimodal inputs like image or audio analysis. The model is part of Perplexity's suite, designed for real-time information retrieval and high-quality responses.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "54": {
      "model": "sonar-pro",
      "description": "Sonar-Pro is a variant of the Sonar model, designed for professional users. It is built on the Llama 3.3 70B architecture and enhances factuality and readability, making it suitable for interactive applications. It does not support multimodal inputs like image or audio analysis. It is optimized for speed and real-time information access.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "55": {
      "model": "sonar",
      "description": "Sonar is a cutting-edge AI model built on the Llama 3.3 70B architecture. It is optimized for enhanced factuality, readability, and speed. Sonar provides real-time access to information and supports text-based interactions but lacks multimodal capabilities like image or audio analysis. It is designed for fast and accurate responses in search environments.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "56": {
      "model": "gpt-4.1-mini",
      "description": "GPT-4.1 Mini is a mid-sized transformer-based large language model developed by OpenAI, released in April 2025. It features a 1 million token context window and delivers performance competitive with GPT-4o but with significantly reduced latency and cost. GPT-4.1 Mini supports both text and image understanding (multimodal input for vision tasks), strong code generation, and advanced instruction following. It is suitable for interactive applications requiring high throughput, but it does not generate images, audio, or video, and lacks audio/video analysis or synthesis capabilities. Fine-tuning is supported for domain-specific tasks.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-04",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "57": {
      "model": "gemini-2.5-pro-preview-tts",
      "description": "Gemini 2.5 Pro Preview TTS is a large multimodal language model from Google, released in June 2025. It is designed primarily for advanced text-to-speech (TTS) generation, supporting both single and multi-speaker audio outputs. The model offers fine-grained control over style, emotion, pace, accent, and pronunciation in generated speech, with a 32k token context window. Gemini 2.5 Pro Preview TTS accepts only text input and outputs audio, making it ideal for podcasts, audiobooks, and dynamic audio content. It does not process images, video, or audio inputs, nor does it generate images or video.",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "58": {
      "model": "gemini-2.5-flash-preview-tts",
      "description": "Gemini 2.5 Flash Preview TTS is a text-to-speech model supporting over 80 languages and enabling podcast creation with multiple speakers. It is designed for efficient performance and offers dynamic control over its thinking budget. The model is part of the Gemini 2.5 family, which excels in reasoning and multimodal capabilities. It is optimized for speed and cost-effectiveness.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 1,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "61": {
      "model": "dall-e-3",
      "description": "DALL-E 3 is a text-to-image generative model developed by OpenAI, released in 2023 as a major improvement over previous versions. It is built on a transformer architecture related to GPT-3, focusing on producing highly detailed and accurate images from complex textual prompts. While its parameter count and exact context window are undisclosed, DALL-E 3 is designed exclusively for image generation and refinement based on detailed natural language instructions. It supports integration with ChatGPT Plus for conversational, iterative image creation; however, it does not natively support text, audio, or video analysis or generation. Its primary use case is creative, designer-level image synthesis, and prompt-based visual art generation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": "2023-10",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "63": {
      "model": "imagen-3.0-generate-002",
      "description": "Imagen 3.0 (model: imagen-3.0-generate-002) is a state-of-the-art text-to-image diffusion model developed by Google DeepMind, released in 2024 and integrated into Google Gemini and other platforms. The model specializes in generating high-resolution (up to 2048x2048px), photorealistic, and visually faithful images from natural language prompts. It supports nuanced control over composition and visual style, as well as multi-subject scene synthesis, but does not process text, audio, or video inputs. The model is not designed for text, audio, or video analysis or generation tasks. Intended uses include creative image generation, storyboarding, design prototyping, and advertising. Technical specifications beyond its generative focus (such as architecture details or parameter size) are not disclosed by Google.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": "2025-07",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "65": {
      "model": "gpt-oss:latest",
      "description": "GPT-OSS is an open-source large language model series released by OpenAI and integrated by Ollama, available in two primary sizes: gpt-oss-20b (21B parameters) and gpt-oss-120b (117B parameters). Both use a Mixture-of-Experts (MoE) architecture with 4-bit (MXFP4) quantization for efficient inference. The models are optimized for instruction following, chain-of-thought reasoning, and agentic capabilities such as function calling and tool use. They support text completion and structured chats, and can run on a range of hardware—from consumer RTX GPUs (20b) to high-end servers (120b). There is support for extended context window sizes, though the exact maximum is not specified. The primary intended use is as a general-purpose text LLM for reasoning, code, and workflow automation. The models do not natively support image, audio, or video analysis or generation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 131072,
      "parameter_count": "21B/117B",
      "knowledge_cutoff": "2025-08",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "89": {
      "model": "gpt-5",
      "description": "GPT-5 (OpenAI, released August 2025) is a unified multimodal architecture available in three variants (regular, mini, nano), designed to support sophisticated text reasoning and complex analytical tasks. It supports both text and image as input, with a maximum input context window of 272,000 tokens and output limit of 128,000 tokens. Capabilities include deep reasoning, multi-turn voice conversations, real-time web search integration, and a creative canvas workspace. Knowledge cutoff is September 30, 2024. Primary use cases include advanced dialogue, research, creative ideation, and multimodal analysis. Notable features: dynamic model selection, context-aware mode switching, support for custom grammars, and a focus on a seamless, holistic AI workbench. Limitations: image output, audio and video generation/analysis are not yet supported.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 272000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-09-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "90": {
      "model": "gpt-5-mini",
      "description": "GPT-5-mini is a scaled-down variant of OpenAI's GPT-5 family, designed for efficient real-time applications with minimal latency. Released in August 2025, its knowledge cutoff is May 30th, 2024. The model offers a context window of 272,000 tokens for input and 128,000 tokens for output. GPT-5-mini supports both text and image inputs, but only generates text as output. It is available for direct API access at various reasoning levels (minimal, low, medium, high), accommodating a wide range of task complexities. Its primary focus is on scalable, high-throughput text and multimodal reasoning tasks. Notable limitations include the absence of image, audio, and video generation or audio/video analysis capabilities.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 272000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-05-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "91": {
      "model": "gpt-5-nano",
      "description": "GPT-5 Nano is a developer-focused, compact variant of OpenAI's GPT-5 family. It is designed for efficiency and high-throughput applications, supporting text and image inputs, with text-only output. The model supports function calling, agentic tool use, and offers developer controls such as adjustable reasoning effort levels and verbosity control. It has a context window of up to 400,000 tokens. Its knowledge cutoff is May 30, 2024. Primary use cases include scalable text generation, reasoning tasks, and agentic workflows, but it does not generate images, audio, or video.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 400000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-05-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "102": {
      "model": "gemini-2.5-flash-image-preview",
      "description": "Gemini 2.5 Flash Image Preview is a multimodal large language model developed by Google, released in August 2025. It supports both text and image inputs and outputs, enabling conversational text generation, image generation, and precise image editing directly via natural language instructions. The model offers a context window of up to 32,768 tokens and incorporates world knowledge for more advanced image rendering and manipulation. It does not support audio or video input/output. Typical use cases include creative writing with rich images, educational applications, and interactive design. All AI-generated images include invisible watermarking for provenance.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32768,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "104": {
      "model": "claude-haiku-4-5-20251001",
      "description": "Claude Haiku 4.5, released by Anthropic in October 2025, is a fast, compact large language model in their small, efficient class. It supports a 200,000-token context window and is engineered for high-throughput, low-latency applications such as chat agents, customer service tools, and coding assistants. Haiku 4.5 is the first in its series to provide advanced features like extended thinking, chain-of-thought reasoning, enhanced computer-use support, and full multimodal processing of both text and image input. It is particularly optimized for reasoning, agentic coding, and scalable business deployments, but does not support image, audio, or video generation.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-10",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "105": {
      "model": "qwen3:14b",
      "description": "Qwen3-14B, developed by Alibaba Cloud, is a dense transformer-based large language model with 14.8 billion parameters and a native context window of 32,768 tokens (expandable to 131,072 with YaRN). It features hybrid 'thinking' (for advanced reasoning, math, logic, and coding) and 'non-thinking' (for fast dialogue and general tasks) modes. The model supports over 100 languages, enhanced reasoning, agentic tool integration, and is released under the Apache 2.0 license (April 2025). It does not natively process images, audio, or video, focusing solely on text-based NLP applications, including multilingual understanding, code generation, and complex problem-solving.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "14.8B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "106": {
      "model": "llava:13b",
      "description": "LLaVA 13B is a multimodal large language and vision model with 13 billion parameters, integrating a CLIP ViT-L/14 vision encoder and the Vicuna large language model via a projection matrix. It processes both text and images, allowing for image captioning, visual question answering, multimodal instruction following, and complex reasoning. Designed for general-purpose multimodal tasks, it supports inputs up to 672x672 pixels or panoramic images, but does not generate new images or handle audio/video modalities. Released with regular updates and optimized for platforms like Ollama and Hugging Face, its main use cases include accessibility, education, and content analysis.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 4096,
      "parameter_count": "13b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "107": {
      "model": "llama4:16x17b",
      "description": "Llama 4:16x17b is a mixture-of-experts (MoE) large language model with 109B parameters and 17B active parameters, optimized for multilingual text and image input. Architecturally, it supports native multimodality, including visual recognition, image reasoning, and captioning. The typical context window is up to 128K tokens. The model is primarily designed for assistant-like chat, visual reasoning, and commercial or research applications across more than 12 languages. It does not support audio or video inputs or outputs nor image or video generation, focusing instead on robust text and image understanding capabilities. Llama 4:16x17b was released circa August 2024.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "109B total (17B active)",
      "knowledge_cutoff": "August 2024",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "108": {
      "model": "qwen2.5vl:32b",
      "description": "Qwen2.5-VL-32B is a 32-billion parameter multimodal large language model from the Qwen2.5-VL series, released March 2025. It supports text, image, and video inputs, excelling in natural language processing, visual content understanding, and advanced logical and mathematical reasoning. The model offers a context window of up to 32,768 tokens (configurable larger for video tasks) and features enhanced OCR, fine-grained image and video comprehension, and detailed structured output formatting. Primary uses include multimodal reasoning, document analysis, and visual question answering, though it does not support audio or image/video generation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "32b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "109": {
      "model": "cogito:32b",
      "description": "Cogito:32b, developed by DeepCogito and available on Ollama, is a 32-billion parameter hybrid reasoning large language model based on Llama/Qwen architectures. It is instruction-tuned, optimized for coding, STEM, tool calling, and multilingual applications, and supports a context window of 128,000 tokens. Released in 2024 as part of the Cogito v1 family, it features both standard and self-reflective reasoning modes and is designed for advanced text-based tasks, but does not support multimodal (image, audio, video) inputs or generation. Its primary focus is logical reasoning, code generation, and agentic function calling for advanced language-based workflow automation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 128000,
      "parameter_count": "32b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "110": {
      "model": "glm4:latest",
      "description": "GLM4 is a multilingual general-purpose large language model developed by Zhipu AI and distributed via Ollama. It features a transformer-based architecture, with competitive performance to Llama 3 and a primary focus on text reasoning, coding, and agentic tasks. While parameter sizes are known for GLM-4.5 (up to 355B total), specifics for GLM4 may be similar but are not publicly listed. The model supports advanced logical reasoning and tool usage and is optimized for versatility across various language tasks. GLM4 does not natively support multimodal inputs such as image or audio, nor image or video generation. Context window size, while not precisely documented, likely matches industry standards for large generative models (8k–32k tokens). Released in mid-2025, GLM4 is primarily intended for conversational AI, multilingual understanding, and code generation, but lacks native support for image, audio, or video modalities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "111": {
      "model": "minicpm-v:8b",
      "description": "MiniCPM-V 8B is an 8 billion parameter multimodal large language model built on the SigLip-400M vision encoder and Qwen2-7B language backbone. Released as version 2.6 and later 4.5, it excels at image and video understanding (including multi-image, video, and strong OCR), visual reasoning, and in-context learning. The model supports multimodal inputs (images and video) with a context window that processes images up to 1.8 million pixels at high token efficiency. Its technical focus is state-of-the-art vision-language modeling under 30B parameters, targeting OCR, VQA, video comprehension, and scientific and document analysis use cases. MiniCPM-V does not generate images, audio, or video and has no speech or audio processing capabilities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "8B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "112": {
      "model": "llava:34b",
      "description": "LLaVA 34B (Large Language and Vision Assistant) is a multimodal model combining a high-capacity vision encoder with the Vicuna large language model, totaling 34 billion parameters. Updated to version 1.6, it supports high-resolution image inputs, improved OCR, and advanced visual reasoning. It enables both text and image-based queries for detailed visual question answering. Primary use cases include general-purpose visual analysis and multimodal assistant applications. LLaVA 34B does not natively support audio or video analysis or generation. The model requires substantial hardware (e.g., NVIDIA RTX 4090) for optimal performance.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "34b",
      "knowledge_cutoff": "2024-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "113": {
      "model": "llama3.3:latest",
      "description": "Llama 3.3, developed by Meta and available via Ollama, is a 70B parameter instruction-tuned large language model based on the Transformer architecture. It is optimized for multilingual conversational tasks, supporting English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama 3.3 is text-only (text in/text out) with a large context window (up to 128K tokens), features such as Generalized Query Attention (GQA), and is intended for commercial and research use in dialogue, reasoning, code generation, and text generation. The model does not support image, audio, or video modalities and was released in 2025.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "70B",
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "114": {
      "model": "qwen3:30b",
      "description": "Qwen3-30B-A3B is a 30.5 billion parameter Mixture-of-Experts (MoE) causal language model, primarily designed for advanced text generation, logical reasoning, math, coding, and multilingual instruction following (over 100 languages/dialects). It achieves strong performance in benchmark evaluations for agentic tasks and supports tool-calling via agent protocols, including Model Context Protocol (MCP). The model natively handles up to 32,768 tokens of context, with a validated extension to 131,072 tokens using YaRN techniques. Qwen3 does not offer multimodal (image, audio, video) functionalities—it is optimized for language and reasoning tasks in various conversational and agentic use cases. The model was released in 2025.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "30.5B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "115": {
      "model": "gemma3:27b",
      "description": "Gemma 3 27B, from Google and available on Ollama, is a 27-billion parameter large language model in the Gemma 3 family, introduced in March 2025. It is a transformer-based architecture designed for both text and vision-language tasks (multimodal), with support for text and image input analysis. It features a 128,000 token context window and supports function calling and structured outputs. The model supports multilingual inputs (over 140 languages), is quantizable (QAT supported for efficient inference), and is suited for advanced reasoning, code understanding, and document generation. While it supports text and image analysis, it does not natively generate images, handle audio, or process video content.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "27b",
      "knowledge_cutoff": "2025-03",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "116": {
      "model": "cogito:14b",
      "description": "Cogito:14b is a 14 billion parameter open-source large language model developed by DeepCogito, available through Ollama. It is architected as a hybrid reasoning model trained using Iterated Distillation and Amplification (IDA), which enhances its performance on complex reasoning, coding, STEM tasks, and instruction following. Cogito:14b is optimized for multilingual, coding, and general problem-solving applications. The model is purely text-based and does not natively support multimodal (image, audio, video) inputs or outputs. Typical usage is on local machines via Ollama for efficiency and privacy. Its context window and precise release date are not explicitly stated in official model cards, but it is positioned as a 2024-generation model focused on high-quality general-purpose and deep reasoning tasks.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 8192,
      "parameter_count": "14b",
      "knowledge_cutoff": "2024-03",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "118": {
      "model": "llama3.3:70b",
      "description": "Llama 3.3:70B is a 70-billion-parameter, decoder-only transformer language model developed by Meta and available via Ollama. It is a text-only, multilingual model optimized for instruction-following, complex reasoning, coding, and assistant-style dialogue in eight languages. It features a 128k token context window, Grouped-Query Attention (GQA) for scalable inference, and supports function calling and tool integration. Released in 2024, it is designed for research and commercial natural language generation and understanding tasks, but does not include multimodal (image, audio, video) input or output capabilities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 128000,
      "parameter_count": "70b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "120": {
      "model": "phi4:14b",
      "description": "Phi-4 (14B) is a dense, decoder-only transformer language model with 14 billion parameters, developed by Microsoft Research and widely available in the open-source ecosystem, including via Ollama. The architecture is optimized for advanced text-based reasoning, logical problem-solving, instruction following, and code generation. It supports a 16,000-token context window and is trained primarily on high-quality English data. Released in early 2024, this model is strictly unimodal—accepting only text inputs—with no native support for image, audio, or video processing. It is designed for research, general-purpose AI assistants, and reasoning-intensive applications, but lacks multimodal, speech, or image generation functionality. Notable limitations include the absence of multimodal or function-calling capabilities and reduced efficacy for non-English tasks or domains outside its training focus.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 16000,
      "parameter_count": "14B",
      "knowledge_cutoff": "2024-03",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "121": {
      "model": "llama3.2-vision:11b",
      "description": "Llama 3.2 Vision 11B is a multimodal transformer model built by Meta, featuring approximately 11 billion parameters. Based on the Llama 3.1 architecture with a specialized vision adapter, it processes both text and images to produce text outputs, supporting tasks such as image captioning, Visual Question Answering (VQA), and document element identification. The model supports a context window of up to 128,000 tokens and was trained on approximately 6 billion image-text pairs, with a knowledge cutoff of December 2023. Image support includes common formats (GIF, JPEG, PNG, WEBP) up to 1120x1120 pixels. Notable limitations include the lack of image generation and audio/video processing, and English-only multimodal support. Primary use cases are multimodal reasoning, document analysis, and visual recognition.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "11b",
      "knowledge_cutoff": "December 2023",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "122": {
      "model": "llava:7b",
      "description": "LLaVA 7B is a multimodal large language model combining a vision encoder with the Vicuna language model, featuring approximately 7 billion parameters. It supports both text and image input for general-purpose visual question answering and image analysis. The model was updated to version 1.6, adding support for higher-resolution images and improved optical character recognition (OCR) and visual reasoning. Typical context window is limited by the underlying Vicuna base, generally in the 4k token range. It does not natively support audio or video analysis/generation. Release was in 2024, and it is primarily intended for multimodal conversational applications where understanding both text and images is required.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 4096,
      "parameter_count": "7b",
      "knowledge_cutoff": "2024-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "123": {
      "model": "llama3.2:latest",
      "description": "Llama 3.2, by Meta and available via Ollama, is a multilingual large language model collection spanning 1B, 3B, 11B, and 90B parameter sizes. The 1B and 3B models are text-only, optimized for dialogue, summarization, agentic retrieval, and function calling/tool use. The 11B and 90B 'Vision' models introduce multimodal text and image processing, supporting image understanding, captioning, and basic document analysis. Context window is up to 128,000 tokens for Vision models. Released in 2024, Llama 3.2 is designed for assistant-like chat, local deployment, and, for Vision models, visual understanding tasks. Text-only versions do not support image inputs; Vision models do not support audio or video modalities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "1B, 3B, 11B, 90B (model-dependent)",
      "knowledge_cutoff": "2023-12",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "124": {
      "model": "mistral:latest",
      "description": "Mistral (latest) on Ollama is a transformer-based large language model, most commonly using the Mistral 7B architecture (7.3 billion parameters)[1][6]. Newer releases, such as Mistral Small 3.1 and Medium 3, expand this line with up to 24 billion parameters and context windows reaching up to 128,000 tokens, supporting complex reasoning and large document interactions[1][2][4][7]. Mistral models are designed primarily for advanced text generation, code synthesis, and logical reasoning tasks, with strong multilingual capabilities. Some recent variants—like Pixtral Large—offer multimodal text and image understanding, but standard Mistral 7B and its instruct/text-completion model as distributed by Ollama are text-only[1][2][6]. Mistral models do not natively support image generation, audio, or video modalities. Intended for local, secure, and high-performance deployments, use cases focus on chatbots, code assistants, document processing, and retrieval-augmented generation. Notable features include efficient Grouped-Query Attention (GQA), Sliding Window Attention (SWA), and fast inference speeds. Core limitations are the lack of native support for image, audio, or video analysis/generation in the standard Mistral releases.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32000,
      "parameter_count": "7.3B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "125": {
      "model": "llama4:latest",
      "description": "Llama 4 is a natively multimodal large language model developed by Meta, featuring a mixture-of-experts (MoE) architecture with models such as Llama 4 Scout boasting 109B parameters (17B active). It supports both text and image inputs, excels in visual recognition, image reasoning, captioning, and answering image-based queries. Its context window reaches up to 10 million tokens, leading the industry in sequence handling. Released in 2025, it is optimized for assistant-like chat, natural language generation, and visual reasoning. Llama 4's output capabilities focus on text and code; direct speech and video modalities are not supported.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 10000000,
      "parameter_count": "109B total (17B active)",
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "127": {
      "model": "gpt-5.1-2025-11-13",
      "description": "GPT-5.1, released in November 2025 by OpenAI, introduces a novel Mixture-of-Agents (MoA) architecture, enabling dynamic collaboration among specialized agents for complex queries. The model offers two variants—in 'Instant' mode for fast responses and 'Thinking' mode for advanced reasoning and thorough analysis. It natively supports multimodal inputs including text, images, audio, and real-time video streams, and can generate as well as analyze images, audio, and 3D object files. Context window and parameter size details remain undisclosed, but it improves instruction following, agentic workflows, and tool integration, with primary use cases spanning advanced decision support, code, technical explanations, and real-world multimodal data processing. Notable features include adaptive reasoning, robust safety audit trails, and low-latency responses.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-11",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": true,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": true,
      "video_gen": true,
      "reasoning": true
    },
    "131": {
      "model": "granite4:1b",
      "description": "Granite4:1b is a 1-billion parameter large language model developed by IBM and distributed via Ollama, notable for its hybrid Mamba/transformer architecture. This design improves memory and computational efficiency over pure transformer models while supporting extended context windows, with training on samples up to 512K tokens. The model features improved instruction following and tool-calling capabilities, making it suitable for code generation, chat, and enterprise applications. It is a text-only model (monomodal) with no image, audio, or video capabilities, and was released as part of the Granite 4 family in 2025.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "1b",
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "132": {
      "model": "granite4:32b-a9b-h",
      "description": "IBM Granite 4.0 H-Small (granite4:32b-a9b-h) is a hybrid Mamba-2/transformer LLM with a Mixture-of-Experts (MoE) architecture, totaling 32 billion parameters (with 9 billion active per inference). Released in 2024, it supports up to a 128K token context window, emphasizing efficiency in agentic tasks including instruction following, function calling, and retrieval-augmented generation (RAG). It is designed for enterprise and agent use cases, with a focus on multi-session, long-context scenarios. The model is text-only and does not natively support image, audio, or video processing.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "32B (9B active)",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "133": {
      "model": "hf.co/unsloth/gpt-oss-20b-GGUF:Q8_0",
      "description": "The GPT-OSS 20B (Q8_0 quantization) is a large language model with approximately 20 billion parameters, designed primarily for text generation and understanding. It is based on the transformer architecture and optimized for efficient local inference using the GGUF format. The model supports text completion, reasoning, and analytical tasks but is not multimodal—thus, it does not natively process images, audio, or video. Its context window size typically ranges from 8k to 16k tokens, depending on configuration and build. Released in 2024, its primary use cases include text-based applications such as chatbots, code generation, and general-purpose language understanding. It lacks function calling and multimodal (vision, audio) capabilities by default.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "20b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "135": {
      "model": "gpt-4o-mini-tts",
      "description": "GPT-4o-mini-tts is a text-to-speech model built on the GPT-4o mini architecture, a transformer-based small language model from OpenAI. It converts text inputs into natural-sounding speech with steerable control over tone, emotion, pacing, and accent via prompts. Supports over 50 languages at 48 kHz sampling rate, with a 2,000-token context window. Released around March 2025, it is accessed via OpenAI's Text-to-Speech API for voice applications. Does not support text generation, image inputs, audio analysis, or video processing.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 2000,
      "parameter_count": "unknown",
      "knowledge_cutoff": "October 2023",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "136": {
      "model": "gemini-3-flash-preview",
      "description": "Gemini 3 Flash Preview is a high-speed model from Google optimized for agentic workflows, multi-turn chat, and coding assistance. It supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, producing text outputs. Key features include configurable reasoning levels, structured output, tool use, and automatic context caching. It excels in complex reasoning, video analysis, and near real-time multimodal processing. Parameter count is undisclosed. Available in preview via Gemini API, Vertex AI, and Gemini app.",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "137": {
      "model": "gemini-2.5-flash",
      "description": "Gemini 2.5 Flash is a multimodal model from Google supporting text, images, video, and audio inputs. It features a 1,048,576 token context window, thinking capabilities for reasoning, and function calling. Launched with latest update in June 2025 and knowledge cutoff January 2025, it excels in low-latency, high-volume tasks and large-scale processing.[3][5]",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1048576,
      "parameter_count": null,
      "knowledge_cutoff": "January 2025",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "138": {
      "model": "gpt-4o-vision-preview",
      "description": "GPT-4o-vision-preview is a multimodal variant of OpenAI's GPT-4o model, released in preview around late 2024. It processes text and image inputs, enabling analysis of visual content such as object detection, image captioning, visual question answering, OCR, and data from charts or diagrams. Supports function calling and a context window of 128,000 tokens. Does not generate images, audio, or video. Designed for vision-language tasks with reasoning capabilities. Parameter count undisclosed.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2023-12",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "139": {
      "model": "whisper-1",
      "description": "Whisper-1 is an encoder-decoder Transformer architecture for automatic speech recognition, trained on 680,000 hours of multilingual supervised data. It processes audio input converted to log-Mel spectrograms in 30-second chunks, supporting multilingual speech transcription, translation to English, and language identification. Accessible via OpenAI API endpoints v1/audio/transcriptions and v1/audio/translations. Audio input only, text output. No specified context window or parameter count; general-purpose ASR without text generation, vision, or reasoning capabilities.[1][2][3]",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "140": {
      "model": "imagen-4.0-generate-001",
      "description": "Google Imagen 4.0-generate-001 is a diffusion-based text-to-image generation model from Google, optimized for high-fidelity image creation with up to 2K resolution output. It supports advanced text rendering, diverse styles including photorealism and artistic renders, multiple aspect ratios, and SynthID watermarking for AI content identification. Key features include fast generation modes up to 10x quicker than prior versions and integration with Google AI tools. Primary use cases encompass marketing assets, editorial visuals, product mockups, and creative content production. Limitations involve challenges with precise object counts and spatial arrangements in complex scenes.[1][2][3][4][5]",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "unknown",
      "knowledge_cutoff": "2025-10",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "141": {
      "model": "gpt-5.2",
      "description": "GPT-5.2 is a large language model from OpenAI, released with variants including Thinking, Pro, and Instant, featuring a 400,000-token context window and 128,000-token max output. Knowledge cutoff is 2025-08-31. It supports advanced reasoning, long-context retrieval, agentic tool-calling, visual understanding for charts and interfaces, and complex workflows like financial modeling and coding. Parameter size is undisclosed. Canvas and image generation are unavailable in Pro variant.[1][2][3]",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 400000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-08-31",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "142": {
      "model": "gpt-4o-mini-transcribe",
      "description": "GPT-4o-mini-transcribe is a specialized speech-to-text model derived from GPT-4o mini, optimized for accurate audio transcription with improvements in word error rate and language recognition over Whisper models. It supports a 16,000-token context window and 2,000 max output tokens, pretrained on diverse audio datasets using supervised fine-tuning and reinforcement learning. Designed for real-time and batch transcription via APIs like Azure OpenAI Realtime and OpenAI Audio, it excels in noisy environments, accents, and varying speech speeds but lacks multimodal inputs beyond audio or text generation capabilities.[1][2][3][6]",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06-01",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "143": {
      "model": "gpt-4o-transcribe",
      "description": "GPT-4o-transcribe is a closed-source speech-to-text model leveraging GPT-4o technology for audio transcription via OpenAI's Audio API. It supports a 16,000-token context window and 2,000 max output tokens, with a knowledge cutoff of June 1, 2024. Key features include improved word error rates, enhanced language recognition over Whisper models, speaker diarization, and real-time transcription via WebSockets. Primary use cases are accurate audio-to-text conversion, including multi-speaker scenarios and non-latency-sensitive workloads. File uploads limited to 25MB.[1][2][5]",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06-01",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "144": {
      "model": "claude-opus-4-6",
      "description": "Claude Opus 4.6 is a frontier large language model from Anthropic, released in early February 2026 as an upgrade emphasizing coding, agentic workflows, and enterprise tasks. It features a 1M token context window (beta), up to 128k output tokens, adaptive thinking for dynamic reasoning depth, and context compaction for extended tasks. Strong in software engineering, multi-step planning, tool calling, and visual understanding for computer use. No native support for image, audio, or video generation or analysis.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2026-02",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "148": {
      "model": "claude-sonnet-4-6",
      "description": "Claude Sonnet 4.6 is Anthropic's most capable Sonnet-class model, featuring a 1M token context window (with 200K standard) and 64K max output tokens. It supports extended thinking, adaptive thinking, and context compaction for long-running tasks. Designed for coding, agentic workflows, and professional knowledge work with improved computer use and multi-step reasoning capabilities. Released in February 2026.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "149": {
      "model": "gemini-3.1-pro-preview",
      "description": "Gemini 3.1 Pro Preview is Google's frontier reasoning model optimized for advanced development and agentic systems. It supports multimodal inputs including text, images, video, audio, and PDF text with a 1M-token context window. Released in February 2026, it features enhanced reasoning capabilities, improved token efficiency, and superior performance on software engineering and autonomous task execution benchmarks. The model includes a medium thinking level for balanced cost and performance.",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1048576,
      "parameter_count": null,
      "knowledge_cutoff": "January 2025",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "150": {
      "model": "mistral-large-latest",
      "description": "Mistral Large latest is a dense large language model with approximately 123B parameters, supporting a 128,000-token context window. It excels in text generation, code generation, mathematics, reasoning, multilingual tasks, instruction-following, and advanced function calling. The model handles long multi-turn conversations and acknowledges knowledge limitations. It lacks native support for image, audio, or video modalities, focusing on text-based processing. Released in iterations like 24.11, it prioritizes cost-efficient performance on benchmarks such as MMLU and MT-Bench.",
      "provider": "mistral",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "123b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "151": {
      "model": "glm-4.7-flash:latest",
      "description": "GLM-4.7-Flash is a 30B-parameter mixture-of-experts model that activates ~3.6B parameters per token. It supports a 200K context window and is optimized for lightweight local deployment on consumer hardware (16GB VRAM minimum). The model excels at text completion and tool calling for agent workflows, with inference speeds of 60-220 tokens per second depending on GPU and quantization. It is designed primarily for coding assistance, local chatbots, and agentic applications requiring offline capability and data privacy.",
      "provider": "ollama",
      "temperature_range": [
        0.7,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": "30B (3.6B active)",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "152": {
      "model": "hf.co/Triangle104/Cydonia-v1.3-Magnum-v4-22B-Q6_K-GGUF:latest",
      "description": "A 22.2-billion parameter language model created through SLERP merging of Cydonia-22B-v1.3 and Magnum-v4-22b, optimized for conversational AI and instruction-following tasks. The Q6_K quantization variant provides an 8-bit GGUF format for efficient deployment. Supports English text processing with an 8,192 token context window. Designed for chat applications and dialogue systems with reduced memory requirements while maintaining model quality.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "22.2b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "153": {
      "model": "qwen3:latest",
      "description": "Qwen3:latest is the latest tag for Qwen3, Alibaba's series of dense and mixture-of-experts (MoE) large language models ranging from 0.6B to 32B dense and up to 235B-A22B MoE parameters. It supports text completion with hybrid thinking and non-thinking modes for reasoning, mathematics, coding, and general tasks. Features include 256K context window extendable to 1M tokens, tool usage, agent capabilities, and multilingual support for 100+ languages. Open-source Ollama variants handle text inputs only; no native image, audio, or video processing.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 262144,
      "parameter_count": "Multiple (0.6B to 235B-A22B)",
      "knowledge_cutoff": "2025-07",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "154": {
      "model": "mistral-small-latest",
      "description": "Mistral Small latest is a 24 billion parameter dense language model from Mistral AI, featuring multimodal capabilities with vision understanding for image analysis, instruction following, function calling, and text generation. It supports a 128k token context window, excels in multilingual tasks, and is optimized for low-latency enterprise applications like conversational assistance, document verification, and on-device processing. Released under Apache 2.0, it runs on single GPUs such as RTX 4090.",
      "provider": "mistral",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "24b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "155": {
      "model": "pixtral-large-latest",
      "description": "Pixtral Large is a 124B parameter multimodal model with a 123B decoder and 1B vision encoder, built on Mistral Large 2. It supports text and image inputs with a 128K context window, enabling analysis of up to 30 high-resolution images alongside text. Key features include advanced image understanding for documents, charts, and natural images; multilingual support; native function calling; and strong performance on benchmarks like MathVista, DocVQA, and MM-MT-Bench. Intended for visual reasoning, document processing, and complex analytical tasks. No audio or video generation capabilities.[1][2][3][6]",
      "provider": "mistral",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "124b",
      "knowledge_cutoff": "2025-04",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "156": {
      "model": "codestral-latest",
      "description": "Codestral-latest (Codestral 25.01 or 2508) is a 32B parameter code generation model from Mistral AI, released around January 2025. It features a 256k context window, supports over 80 programming languages, and excels in code completion, fill-in-the-middle (FIM), and low-latency tasks with a re-engineered tokenizer for 2x faster generation. It offers text-based chat completions and function calling but lacks multimodal capabilities like image, audio, or video processing.[1][2][7][8]",
      "provider": "mistral",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 256000,
      "parameter_count": "32b",
      "knowledge_cutoff": "2025-01",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "157": {
      "model": "mistral-medium-2508",
      "description": "Mistral Medium 3.1 (mistral-medium-2508) is a frontier-class multimodal model from Mistral AI, released in August 2025. It supports text and image inputs with a 128k-131k token context window. Key features include chat completions, function calling, agents & conversations, structured outputs, OCR, annotations, structured bounding box extraction, and document Q&A. It handles reasoning tasks and is designed for enterprise applications with hybrid/on-prem deployment options.[1][3][5]",
      "provider": "mistral",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": "2025-08",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    }
  }
}
