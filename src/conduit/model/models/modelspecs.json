{
  "_default": {
    "18": {
      "model": "llama3.1:latest",
      "description": "Llama3.1:latest is an earlier release in Meta's Llama 3 series, available in multiple parameter sizes (e.g., 8B, 70B). It is a text-only model for tasks like chat, code generation, summarization, and general language understanding. The model does not process images, audio, or video. Context window is typically around 8,000 to 32,000 tokens. Strong in reasoning and instruction following.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32000,
      "parameter_count": "varies",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "24": {
      "model": "gpt-4.1",
      "description": "GPT-4.1 is OpenAI's flagship large language model, designed for complex tasks, reasoning, and high-fidelity text generation. It is based on a transformer architecture and supports both text and image inputs (multimodal). The model features an expanded context window — typically 128k tokens — enabling it to manage lengthy interactions and context-rich documents. GPT-4.1 supports advanced reasoning and function calling. Known for robust performance on diverse benchmarks, it was released in early 2025 and is intended for applications requiring nuanced understanding and generation, including content creation, education, and consultation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "25": {
      "model": "gpt-4o",
      "description": "GPT-4o (\"omni\") is OpenAI's most advanced flagship model as of April 2025. It is fully multimodal, accepting text, image, and audio inputs, and can generate text and audio outputs. It features real-time, low-latency capabilities for conversation and supports a large context window, reportedly up to 128k tokens. GPT-4o is suitable for complex reasoning, function calling, and integration into conversational and assistant applications. Designed for seamless multimodal interaction, it excels in accessibility and real-world user engagement.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "26": {
      "model": "gpt-4-turbo",
      "description": "GPT-4 Turbo is an optimized variant of GPT-4 offering faster inference and lower costs, while maintaining the core GPT-4 architecture's capabilities. It is primarily focused on text and code generation with an extended context window (up to 128k tokens). Turbo does not natively support image, audio, or video processing. Released in late 2023, it is intended for scalable applications such as chatbots, code assistance, and generative tools where throughput and performance are important.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "27": {
      "model": "gpt-3.5-turbo-0125",
      "description": "GPT-3.5 Turbo (0125) is a cost-effective, high-performance text generation model based on the GPT-3.5 architecture. It supports text generation and completion tasks with a context window of up to 16,385 tokens. While capable in conversation, coding, and basic reasoning, GPT-3.5 Turbo does not have multimodal capabilities (image, audio, or video). It was released in early 2024 and is most commonly used for chatbots, drafting, summarization, and general text-based automation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16385,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "28": {
      "model": "gpt-4o-mini",
      "description": "GPT-4o Mini is a lighter, more efficient variant of the GPT-4o model. It is designed to balance speed and capability, featuring support for text and image input while maintaining strong reasoning abilities. Its context window is smaller than GPT-4o, optimized for lower latency and cost. Intended use cases include lightweight conversational agents and tasks where full GPT-4o performance is not required. Unlike GPT-4o, it does not natively process audio or video.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "29": {
      "model": "gpt-4o-audio-preview",
      "description": "GPT-4o Audio Preview is a specialized variant of GPT-4o, focused on low-latency speech recognition (speech-to-text), speech translation, and text-to-speech (audio generation). It is designed for \"speech in, speech out\" conversational systems and supports multimodal input. With real-time audio capabilities and strong reasoning, it is well-suited for accessibility tools, voice assistants, and rapid dialogue applications. Its context window size and parameter count are not publicly detailed; released in 2025.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "30": {
      "model": "o1-preview",
      "description": "O1-Preview is part of OpenAI’s reasoning-focused \"o-series\" models, aimed at complex, multi-step analytical and logical reasoning. It is a general-purpose model built for improved problem solving and knowledge tasks, with a context window likely between 8k and 32k tokens. Unlike flagship GPT models, o1-preview does not support image, audio, or video input. The model is primarily intended for high-quality text generation and in-depth reasoning applications.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16384,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "31": {
      "model": "o1-mini",
      "description": "O1-Mini is a deprecated, lightweight variant of the o1 reasoning model. It focuses on efficient text processing and basic reasoning and is designed for lower resource consumption. The model lacks multimodal capabilities, handling only text input and output, with a modest context window (typically 4k–8k tokens). O1-Mini is intended for simple chatbot, summarization, and text classification tasks.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "32": {
      "model": "o3-mini",
      "description": "O3-Mini is a small-scale variant of OpenAI's o3 reasoning model, engineered for efficiency and speed. It is suited for applications demanding rapid response and lower compute costs, supporting advanced text generation and reasoning. O3-Mini has a limited context window (8k tokens) and does not feature multimodal or generative audio/video capabilities. Its design targets chatbots, simple assistants, and text automation workflows.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "33": {
      "model": "o3-mini-high",
      "description": "O3-Mini-High is an enhanced version of O3-Mini, offering improved reasoning, accuracy, and performance at a small model size. It retains the same primary focus: text generation and complex analytical tasks, with an 8k context window. Like other mini models, O3-Mini-High does not support multimodal processing. It is suitable for efficient deployment in assistants, summarization, and logical inference use cases.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "34": {
      "model": "o4-mini",
      "description": "O4-Mini is a compact, high-speed reasoning model in OpenAI’s o-series, optimized for lower latency and deployment cost. It excels at text generation and multi-step reasoning with a context window of 8k–16k tokens. O4-Mini does not support image, audio, or video input/output. Intended uses include chatbots, digital assistants, and lightweight analysis tools where performance-to-cost ratio is key.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 16384,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "35": {
      "model": "claude-3-5-haiku-20241022",
      "description": "Claude 3.5 Haiku (October 2024) is part of Anthropic's third-generation model lineup, designed as the fastest and most cost-effective tier. Built on a large transformer architecture, it excels in rapid text processing, general-purpose text and code tasks, and supports image analysis (multimodal input) but does not generate images or audio. It typically features a substantial context window (up to 200K tokens in advanced versions) and is optimized for applications requiring high throughput and responsiveness. Main use cases include content generation, summarization, and analysis at scale. Notable limitations include a lack of image, audio, and video generation capabilities.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "36": {
      "model": "claude-3-7-sonnet-20250219",
      "description": "Claude 3.7 Sonnet (February 2025) is Anthropic's mid-size transformer-based model in the Claude 3 family, targeting a balance of capability, speed, and operating cost. It supports robust text generation, strong reasoning, and image analysis through multimodal inputs, but does not generate images, audio, or video. With a large context window (likely up to 200K tokens), it is intended for high-volume applications such as data analysis, code synthesis, search, and workflow orchestration. Limitations include no image or audio output and no video understanding or production.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "37": {
      "model": "claude-opus-4-20250514",
      "description": "Claude Opus 4 (May 2025) is Anthropic's flagship large multimodal model, boasting advanced reasoning, coding abilities, and sophisticated tool use. Built on a large transformer architecture, it offers a 200K token context window and is designed for enterprise-grade AI agent applications and autonomous multi-step workflows. Opus 4 delivers top-tier performance on complex tasks, including research synthesis and orchestrating cross-functional activities, with vision (image analysis) support but no image, audio, or video generation features.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "38": {
      "model": "claude-sonnet-4-20250514",
      "description": "Claude Sonnet 4 (May 2025) is a mid-size transformer model in Anthropic’s Claude portfolio, optimized for quality, responsiveness, and cost-efficiency. With a 200K token context window, it tackles high-volume use cases such as workflow automation, data analysis, and code generation. Sonnet 4 supports robust text generation, advanced reasoning, and image understanding (vision input), but does not create images, audio, or video. It is well-suited for scalable task-specific applications within larger AI systems.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "39": {
      "model": "gemini-2.5-flash-preview-05-20",
      "description": "Gemini 2.5 Flash (Preview 05-20) is a multimodal language model released by Google in 2025 as part of the Gemini family. It is optimized for rapid response and efficiency in handling text, image, video, and audio inputs, with a context window sufficient for most practical applications (reportedly up to 1 million tokens in the Gemini 1.5 family; specifics for 2.5 are not fully disclosed but are similar). The Flash variant is designed for price-performance and supports advanced reasoning and step-by-step thinking capabilities. It is suited for conversational agents, document analysis, and multimodal data tasks, but lacks image, audio, and video generation functionalities. Intended for scalable, production-grade deployments.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "40": {
      "model": "gemini-2.5-pro-exp-03-25",
      "description": "Gemini 2.5 Pro (Experimental 03-25) is an advanced multimodal LLM by Google, focused on high-level reasoning, complex problem-solving, and robust comprehension across text, image, audio, code, and video. It features an input size limit of 500 MB and supports large context windows (similar to earlier Gemini 1.5 models, likely up to 1 million tokens). Designed for challenging applications such as scientific research, advanced analytics, and enterprise-scale tasks. Known for deep multimodal understanding, but does not support content generation in image, audio, or video formats.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "41": {
      "model": "gemini-2.5-pro-preview-05-06",
      "description": "Gemini 2.5 Pro (Preview 05-06) is Google's most advanced reasoning model in the Gemini series, released in 2025. It is capable of handling and integrating information from large, diverse datasets, including text, audio, imagery, video, and code repositories. Its context window is similar to the Gemini 2.5 series (on the order of 1 million tokens). The model is optimized for deep reasoning, multimodal analysis, research, and enterprise use. It can analyze but not generate images, audio, or video.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "42": {
      "model": "gemini-2.0-flash-001",
      "description": "Gemini 2.0 Flash 001 is an earlier fast-response variant in Google’s Gemini line, designed for both multimodal data ingestion (text, image, audio, video) and rapid inference, but with smaller context windows and fewer advanced features than subsequent Gemini 2.5 models. It is intended for scenarios requiring quick, scalable responses with solid reasoning and multimodal analysis, but it does not support generation of images, audio, or video. Used primarily for chatbots, Q&A, and basic document/image analysis.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2023-12",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "43": {
      "model": "gemini-1.5-pro",
      "description": "Gemini 1.5 Pro is a leading multimodal model from Google, offering advanced reasoning and comprehension across text, documents, code, images, audio, and video. It features a context window of up to 1 million tokens, enabling analysis of large inputs and complex cross-modal interactions. Designed for tasks involving deep research, analytics, or enterprise-grade analysis, it can process but not generate image/audio/video. Released in 2024, it supports conversational use, document/image/video understanding, and complex problem solving.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-02",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "44": {
      "model": "gemini-1.5-flash",
      "description": "Gemini 1.5 Flash is a fast, efficient multimodal model released by Google in early 2024. It is designed for rapid inference, handling text, images, audio, and video inputs. This model trades off some depth of reasoning for latency, making it suitable for chat assistants and high-throughput applications. It supports large context windows (up to 1 million tokens), with strengths in multimodal content analysis and quick response. The model does not support generating new images, audio, or video.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-02",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "45": {
      "model": "gemini-1.5-flash-8b",
      "description": "Gemini 1.5 Flash-8B is a smaller, efficient multimodal variant of Gemini Flash, likely featuring 8 billion parameters (exact size not official). Released in 2024, it is engineered for speed and moderate resource usage, supporting text, image, audio, and video analysis but not media generation. It is especially suited for latency-sensitive applications, chatbots, and batch document/image/video processing with a context window similar to the Gemini 1.5 series. Unlike the Pro versions, it sacrifices some analytical depth for efficiency.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 1000000,
      "parameter_count": "8b",
      "knowledge_cutoff": "2024-02",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "46": {
      "model": "llama3-8b-8192",
      "description": "Llama3-8b-8192 is an 8 billion parameter transformer-based language model, featuring an 8,192 token context window. Designed primarily for text-based tasks, it excels in text completion, dialogue, and reasoning. The model supports function calling and is optimized for rapid inference. It is primarily intended for chatbot, code generation, and conversational AI use cases, leveraging Groq’s fast inference hardware for low-latency outputs.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "8B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "47": {
      "model": "llama3-70b-8192",
      "description": "Llama3-70b-8192 is a 70 billion parameter transformer-based language model with an 8,192 token context window. It is highly optimized for dialogue, content generation, and complex reasoning tasks. The model maintains a strong MMLU score (79.5%) and supports function calling and tool use. Designed for production-ready, fast, and consistent outputs, it is widely used for advanced chatbots, research, and large-scale automation.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "70B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "48": {
      "model": "mixtral-8x7b-32768",
      "description": "Mixtral-8x7b-32768 is a mixture-of-experts model with 8 experts of 7B parameters each (total active parameters per forward pass: ~12.9B), featuring a 32,768 token context window. It is designed for advanced text generation, reasoning, and cost-effective inference at scale. The model focuses on open-ended dialogue, code generation, and large-document processing, leveraging Groq’s speed for real-time interaction.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 32768,
      "parameter_count": "8x7B (active ~12.9B per pass)",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "49": {
      "model": "gemma2-9b-it",
      "description": "Gemma2-9b-it is a 9 billion parameter language model from Google’s Gemma 2 series, likely deployed on Groq for fast inference. It features a transformer architecture and is optimized for instruction following and text-based tasks. The model supports text completion and reasoning, focusing on instruction-based dialogue, summarization, and knowledge extraction. Multimodal or image/audio/video capabilities are not available.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "9B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "50": {
      "model": "llama-3.3-70b-versatile",
      "description": "Llama-3.3-70b-versatile is a 70 billion parameter transformer-based model, likely a variant of Meta’s Llama 3, optimized for versatile use cases with a large context window. It supports advanced reasoning, function calling, and tool use. The model is designed for complex language understanding, automation, and research applications, but it does not support image, audio, or video analysis or generation.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 8192,
      "parameter_count": "70B",
      "knowledge_cutoff": "Recent",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "53": {
      "model": "sonar-reasoning",
      "description": "Sonar-reasoning is likely a variant of the Sonar model, optimized for logical reasoning tasks. It is built on top of the Llama 3.3 70B architecture and focuses on providing accurate and factual responses. It does not support multimodal inputs like image or audio analysis. The model is part of Perplexity's suite, designed for real-time information retrieval and high-quality responses.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "54": {
      "model": "sonar-pro",
      "description": "Sonar-Pro is a variant of the Sonar model, designed for professional users. It is built on the Llama 3.3 70B architecture and enhances factuality and readability, making it suitable for interactive applications. It does not support multimodal inputs like image or audio analysis. It is optimized for speed and real-time information access.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "55": {
      "model": "sonar",
      "description": "Sonar is a cutting-edge AI model built on the Llama 3.3 70B architecture. It is optimized for enhanced factuality, readability, and speed. Sonar provides real-time access to information and supports text-based interactions but lacks multimodal capabilities like image or audio analysis. It is designed for fast and accurate responses in search environments.",
      "provider": "perplexity",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 131072,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "56": {
      "model": "gpt-4.1-mini",
      "description": "GPT-4.1 Mini is a mid-sized transformer-based large language model developed by OpenAI, released in April 2025. It features a 1 million token context window and delivers performance competitive with GPT-4o but with significantly reduced latency and cost. GPT-4.1 Mini supports both text and image understanding (multimodal input for vision tasks), strong code generation, and advanced instruction following. It is suitable for interactive applications requiring high throughput, but it does not generate images, audio, or video, and lacks audio/video analysis or synthesis capabilities. Fine-tuning is supported for domain-specific tasks.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 1000000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-04",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "57": {
      "model": "gemini-2.5-pro-preview-tts",
      "description": "Gemini 2.5 Pro Preview TTS is a large multimodal language model from Google, released in June 2025. It is designed primarily for advanced text-to-speech (TTS) generation, supporting both single and multi-speaker audio outputs. The model offers fine-grained control over style, emotion, pace, accent, and pronunciation in generated speech, with a 32k token context window. Gemini 2.5 Pro Preview TTS accepts only text input and outputs audio, making it ideal for podcasts, audiobooks, and dynamic audio content. It does not process images, video, or audio inputs, nor does it generate images or video.",
      "provider": "google",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "58": {
      "model": "gemini-2.5-flash-preview-tts",
      "description": "Gemini 2.5 Flash Preview TTS is a text-to-speech model supporting over 80 languages and enabling podcast creation with multiple speakers. It is designed for efficient performance and offers dynamic control over its thinking budget. The model is part of the Gemini 2.5 family, which excels in reasoning and multimodal capabilities. It is optimized for speed and cost-effectiveness.",
      "provider": "google",
      "temperature_range": [],
      "context_window": 1,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": true,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "61": {
      "model": "dall-e-3",
      "description": "DALL-E 3 is a text-to-image generative model developed by OpenAI, released in 2023 as a major improvement over previous versions. It is built on a transformer architecture related to GPT-3, focusing on producing highly detailed and accurate images from complex textual prompts. While its parameter count and exact context window are undisclosed, DALL-E 3 is designed exclusively for image generation and refinement based on detailed natural language instructions. It supports integration with ChatGPT Plus for conversational, iterative image creation; however, it does not natively support text, audio, or video analysis or generation. Its primary use case is creative, designer-level image synthesis, and prompt-based visual art generation.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": "2023-10",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "63": {
      "model": "imagen-3.0-generate-002",
      "description": "Imagen 3.0 (model: imagen-3.0-generate-002) is a state-of-the-art text-to-image diffusion model developed by Google DeepMind, released in 2024 and integrated into Google Gemini and other platforms. The model specializes in generating high-resolution (up to 2048x2048px), photorealistic, and visually faithful images from natural language prompts. It supports nuanced control over composition and visual style, as well as multi-subject scene synthesis, but does not process text, audio, or video inputs. The model is not designed for text, audio, or video analysis or generation tasks. Intended uses include creative image generation, storyboarding, design prototyping, and advertising. Technical specifications beyond its generative focus (such as architecture details or parameter size) are not disclosed by Google.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": "2025-07",
      "text_completion": false,
      "image_analysis": false,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    },
    "65": {
      "model": "gpt-oss:latest",
      "description": "GPT-OSS is an open-source large language model series released by OpenAI and integrated by Ollama, available in two primary sizes: gpt-oss-20b (21B parameters) and gpt-oss-120b (117B parameters). Both use a Mixture-of-Experts (MoE) architecture with 4-bit (MXFP4) quantization for efficient inference. The models are optimized for instruction following, chain-of-thought reasoning, and agentic capabilities such as function calling and tool use. They support text completion and structured chats, and can run on a range of hardware—from consumer RTX GPUs (20b) to high-end servers (120b). There is support for extended context window sizes, though the exact maximum is not specified. The primary intended use is as a general-purpose text LLM for reasoning, code, and workflow automation. The models do not natively support image, audio, or video analysis or generation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "21B/117B",
      "knowledge_cutoff": "2025-08",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "89": {
      "model": "gpt-5",
      "description": "GPT-5 (OpenAI, released August 2025) is a unified multimodal architecture available in three variants (regular, mini, nano), designed to support sophisticated text reasoning and complex analytical tasks. It supports both text and image as input, with a maximum input context window of 272,000 tokens and output limit of 128,000 tokens. Capabilities include deep reasoning, multi-turn voice conversations, real-time web search integration, and a creative canvas workspace. Knowledge cutoff is September 30, 2024. Primary use cases include advanced dialogue, research, creative ideation, and multimodal analysis. Notable features: dynamic model selection, context-aware mode switching, support for custom grammars, and a focus on a seamless, holistic AI workbench. Limitations: image output, audio and video generation/analysis are not yet supported.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 272000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-09-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": true,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "90": {
      "model": "gpt-5-mini",
      "description": "GPT-5-mini is a scaled-down variant of OpenAI's GPT-5 family, designed for efficient real-time applications with minimal latency. Released in August 2025, its knowledge cutoff is May 30th, 2024. The model offers a context window of 272,000 tokens for input and 128,000 tokens for output. GPT-5-mini supports both text and image inputs, but only generates text as output. It is available for direct API access at various reasoning levels (minimal, low, medium, high), accommodating a wide range of task complexities. Its primary focus is on scalable, high-throughput text and multimodal reasoning tasks. Notable limitations include the absence of image, audio, and video generation or audio/video analysis capabilities.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 272000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-05-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "91": {
      "model": "gpt-5-nano",
      "description": "GPT-5 Nano is a developer-focused, compact variant of OpenAI's GPT-5 family. It is designed for efficiency and high-throughput applications, supporting text and image inputs, with text-only output. The model supports function calling, agentic tool use, and offers developer controls such as adjustable reasoning effort levels and verbosity control. It has a context window of up to 400,000 tokens. Its knowledge cutoff is May 30, 2024. Primary use cases include scalable text generation, reasoning tasks, and agentic workflows, but it does not generate images, audio, or video.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 400000,
      "parameter_count": null,
      "knowledge_cutoff": "2024-05-30",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "98": {
      "model": "claude-opus-4-1-20250805",
      "description": "Claude Opus 4.1, released by Anthropic on August 5, 2025, is a large language model designed for advanced reasoning, coding, agentic tasks, and research-grade content synthesis. It operates via a transformer architecture (specific parameter count undisclosed) and supports a large context window and long outputs (up to 32K tokens). The model offers hybrid reasoning capabilities, excelling at multi-step logical analysis, code generation across large files, and complex business or research workflows. It is accessible via API and major cloud platforms. Claude Opus 4.1 does not natively support image, audio, or video processing or generation—it is strictly text and code oriented, with primary use cases in software engineering, intensive research, and agentic workflows.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-05",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "102": {
      "model": "gemini-2.5-flash-image-preview",
      "description": "Gemini 2.5 Flash Image Preview is a multimodal large language model developed by Google, released in August 2025. It supports both text and image inputs and outputs, enabling conversational text generation, image generation, and precise image editing directly via natural language instructions. The model offers a context window of up to 32,768 tokens and incorporates world knowledge for more advanced image rendering and manipulation. It does not support audio or video input/output. Typical use cases include creative writing with rich images, educational applications, and interactive design. All AI-generated images include invisible watermarking for provenance.",
      "provider": "google",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32768,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": true,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "103": {
      "model": "claude-sonnet-4-5-20250929",
      "description": "Claude Sonnet 4 is a large language model developed by Anthropic, officially released on May 22, 2025, as part of the Claude 4 family. It features a 200,000-token context window, supports hybrid reasoning modes (instant or extended step-by-step thinking), and is optimized for applications such as coding, advanced reasoning, code review, agentic workflows, and customer-facing AI agents. Sonnet 4 enables multimodal inputs—specifically, analysis of image inputs as well as text—but it does not support generative image, audio, or video outputs. The model can integrate with external tools and APIs (tool use), supports agentic code execution, and offers improved memory management. Key technical strengths include strong code analysis, advanced planning, and flexible integration with developer tools and APIs. Sonnet 4 is designed primarily for text and image understanding, stepwise reasoning, and workflow automation, rather than creation of novel images, audio, or video.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-05",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "104": {
      "model": "claude-haiku-4-5-20251001",
      "description": "Claude Haiku 4.5, released by Anthropic in October 2025, is a fast, compact large language model in their small, efficient class. It supports a 200,000-token context window and is engineered for high-throughput, low-latency applications such as chat agents, customer service tools, and coding assistants. Haiku 4.5 is the first in its series to provide advanced features like extended thinking, chain-of-thought reasoning, enhanced computer-use support, and full multimodal processing of both text and image input. It is particularly optimized for reasoning, agentic coding, and scalable business deployments, but does not support image, audio, or video generation.",
      "provider": "anthropic",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 200000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-10",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "105": {
      "model": "qwen3:14b",
      "description": "Qwen3-14B, developed by Alibaba Cloud, is a dense transformer-based large language model with 14.8 billion parameters and a native context window of 32,768 tokens (expandable to 131,072 with YaRN). It features hybrid 'thinking' (for advanced reasoning, math, logic, and coding) and 'non-thinking' (for fast dialogue and general tasks) modes. The model supports over 100 languages, enhanced reasoning, agentic tool integration, and is released under the Apache 2.0 license (April 2025). It does not natively process images, audio, or video, focusing solely on text-based NLP applications, including multilingual understanding, code generation, and complex problem-solving.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "14.8B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "106": {
      "model": "llava:13b",
      "description": "LLaVA 13B is a multimodal large language and vision model with 13 billion parameters, integrating a CLIP ViT-L/14 vision encoder and the Vicuna large language model via a projection matrix. It processes both text and images, allowing for image captioning, visual question answering, multimodal instruction following, and complex reasoning. Designed for general-purpose multimodal tasks, it supports inputs up to 672x672 pixels or panoramic images, but does not generate new images or handle audio/video modalities. Released with regular updates and optimized for platforms like Ollama and Hugging Face, its main use cases include accessibility, education, and content analysis.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 4096,
      "parameter_count": "13b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "107": {
      "model": "llama4:16x17b",
      "description": "Llama 4:16x17b is a mixture-of-experts (MoE) large language model with 109B parameters and 17B active parameters, optimized for multilingual text and image input. Architecturally, it supports native multimodality, including visual recognition, image reasoning, and captioning. The typical context window is up to 128K tokens. The model is primarily designed for assistant-like chat, visual reasoning, and commercial or research applications across more than 12 languages. It does not support audio or video inputs or outputs nor image or video generation, focusing instead on robust text and image understanding capabilities. Llama 4:16x17b was released circa August 2024.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "109B total (17B active)",
      "knowledge_cutoff": "August 2024",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "108": {
      "model": "qwen2.5vl:32b",
      "description": "Qwen2.5-VL-32B is a 32-billion parameter multimodal large language model from the Qwen2.5-VL series, released March 2025. It supports text, image, and video inputs, excelling in natural language processing, visual content understanding, and advanced logical and mathematical reasoning. The model offers a context window of up to 32,768 tokens (configurable larger for video tasks) and features enhanced OCR, fine-grained image and video comprehension, and detailed structured output formatting. Primary uses include multimodal reasoning, document analysis, and visual question answering, though it does not support audio or image/video generation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "32b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "109": {
      "model": "cogito:32b",
      "description": "Cogito:32b, developed by DeepCogito and available on Ollama, is a 32-billion parameter hybrid reasoning large language model based on Llama/Qwen architectures. It is instruction-tuned, optimized for coding, STEM, tool calling, and multilingual applications, and supports a context window of 128,000 tokens. Released in 2024 as part of the Cogito v1 family, it features both standard and self-reflective reasoning modes and is designed for advanced text-based tasks, but does not support multimodal (image, audio, video) inputs or generation. Its primary focus is logical reasoning, code generation, and agentic function calling for advanced language-based workflow automation.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 128000,
      "parameter_count": "32b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "110": {
      "model": "glm4:latest",
      "description": "GLM4 is a multilingual general-purpose large language model developed by Zhipu AI and distributed via Ollama. It features a transformer-based architecture, with competitive performance to Llama 3 and a primary focus on text reasoning, coding, and agentic tasks. While parameter sizes are known for GLM-4.5 (up to 355B total), specifics for GLM4 may be similar but are not publicly listed. The model supports advanced logical reasoning and tool usage and is optimized for versatility across various language tasks. GLM4 does not natively support multimodal inputs such as image or audio, nor image or video generation. Context window size, while not precisely documented, likely matches industry standards for large generative models (8k–32k tokens). Released in mid-2025, GLM4 is primarily intended for conversational AI, multilingual understanding, and code generation, but lacks native support for image, audio, or video modalities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": null,
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "111": {
      "model": "minicpm-v:8b",
      "description": "MiniCPM-V 8B is an 8 billion parameter multimodal large language model built on the SigLip-400M vision encoder and Qwen2-7B language backbone. Released as version 2.6 and later 4.5, it excels at image and video understanding (including multi-image, video, and strong OCR), visual reasoning, and in-context learning. The model supports multimodal inputs (images and video) with a context window that processes images up to 1.8 million pixels at high token efficiency. Its technical focus is state-of-the-art vision-language modeling under 30B parameters, targeting OCR, VQA, video comprehension, and scientific and document analysis use cases. MiniCPM-V does not generate images, audio, or video and has no speech or audio processing capabilities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "8B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": true,
      "video_gen": false,
      "reasoning": true
    },
    "112": {
      "model": "llava:34b",
      "description": "LLaVA 34B (Large Language and Vision Assistant) is a multimodal model combining a high-capacity vision encoder with the Vicuna large language model, totaling 34 billion parameters. Updated to version 1.6, it supports high-resolution image inputs, improved OCR, and advanced visual reasoning. It enables both text and image-based queries for detailed visual question answering. Primary use cases include general-purpose visual analysis and multimodal assistant applications. LLaVA 34B does not natively support audio or video analysis or generation. The model requires substantial hardware (e.g., NVIDIA RTX 4090) for optimal performance.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 8192,
      "parameter_count": "34b",
      "knowledge_cutoff": "2024-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "113": {
      "model": "llama3.3:latest",
      "description": "Llama 3.3, developed by Meta and available via Ollama, is a 70B parameter instruction-tuned large language model based on the Transformer architecture. It is optimized for multilingual conversational tasks, supporting English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama 3.3 is text-only (text in/text out) with a large context window (up to 128K tokens), features such as Generalized Query Attention (GQA), and is intended for commercial and research use in dialogue, reasoning, code generation, and text generation. The model does not support image, audio, or video modalities and was released in 2025.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "70B",
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "114": {
      "model": "qwen3:30b",
      "description": "Qwen3-30B-A3B is a 30.5 billion parameter Mixture-of-Experts (MoE) causal language model, primarily designed for advanced text generation, logical reasoning, math, coding, and multilingual instruction following (over 100 languages/dialects). It achieves strong performance in benchmark evaluations for agentic tasks and supports tool-calling via agent protocols, including Model Context Protocol (MCP). The model natively handles up to 32,768 tokens of context, with a validated extension to 131,072 tokens using YaRN techniques. Qwen3 does not offer multimodal (image, audio, video) functionalities—it is optimized for language and reasoning tasks in various conversational and agentic use cases. The model was released in 2025.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 32768,
      "parameter_count": "30.5B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "115": {
      "model": "gemma3:27b",
      "description": "Gemma 3 27B, from Google and available on Ollama, is a 27-billion parameter large language model in the Gemma 3 family, introduced in March 2025. It is a transformer-based architecture designed for both text and vision-language tasks (multimodal), with support for text and image input analysis. It features a 128,000 token context window and supports function calling and structured outputs. The model supports multilingual inputs (over 140 languages), is quantizable (QAT supported for efficient inference), and is suited for advanced reasoning, code understanding, and document generation. While it supports text and image analysis, it does not natively generate images, handle audio, or process video content.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "27b",
      "knowledge_cutoff": "2025-03",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "116": {
      "model": "cogito:14b",
      "description": "Cogito:14b is a 14 billion parameter open-source large language model developed by DeepCogito, available through Ollama. It is architected as a hybrid reasoning model trained using Iterated Distillation and Amplification (IDA), which enhances its performance on complex reasoning, coding, STEM tasks, and instruction following. Cogito:14b is optimized for multilingual, coding, and general problem-solving applications. The model is purely text-based and does not natively support multimodal (image, audio, video) inputs or outputs. Typical usage is on local machines via Ollama for efficiency and privacy. Its context window and precise release date are not explicitly stated in official model cards, but it is positioned as a 2024-generation model focused on high-quality general-purpose and deep reasoning tasks.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 8192,
      "parameter_count": "14b",
      "knowledge_cutoff": "2024-03",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "118": {
      "model": "llama3.3:70b",
      "description": "Llama 3.3:70B is a 70-billion-parameter, decoder-only transformer language model developed by Meta and available via Ollama. It is a text-only, multilingual model optimized for instruction-following, complex reasoning, coding, and assistant-style dialogue in eight languages. It features a 128k token context window, Grouped-Query Attention (GQA) for scalable inference, and supports function calling and tool integration. Released in 2024, it is designed for research and commercial natural language generation and understanding tasks, but does not include multimodal (image, audio, video) input or output capabilities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 128000,
      "parameter_count": "70b",
      "knowledge_cutoff": "2024-06",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "120": {
      "model": "phi4:14b",
      "description": "Phi-4 (14B) is a dense, decoder-only transformer language model with 14 billion parameters, developed by Microsoft Research and widely available in the open-source ecosystem, including via Ollama. The architecture is optimized for advanced text-based reasoning, logical problem-solving, instruction following, and code generation. It supports a 16,000-token context window and is trained primarily on high-quality English data. Released in early 2024, this model is strictly unimodal—accepting only text inputs—with no native support for image, audio, or video processing. It is designed for research, general-purpose AI assistants, and reasoning-intensive applications, but lacks multimodal, speech, or image generation functionality. Notable limitations include the absence of multimodal or function-calling capabilities and reduced efficacy for non-English tasks or domains outside its training focus.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 16000,
      "parameter_count": "14B",
      "knowledge_cutoff": "2024-03",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "121": {
      "model": "llama3.2-vision:11b",
      "description": "Llama 3.2 Vision 11B is a multimodal transformer model built by Meta, featuring approximately 11 billion parameters. Based on the Llama 3.1 architecture with a specialized vision adapter, it processes both text and images to produce text outputs, supporting tasks such as image captioning, Visual Question Answering (VQA), and document element identification. The model supports a context window of up to 128,000 tokens and was trained on approximately 6 billion image-text pairs, with a knowledge cutoff of December 2023. Image support includes common formats (GIF, JPEG, PNG, WEBP) up to 1120x1120 pixels. Notable limitations include the lack of image generation and audio/video processing, and English-only multimodal support. Primary use cases are multimodal reasoning, document analysis, and visual recognition.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "11b",
      "knowledge_cutoff": "December 2023",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "122": {
      "model": "llava:7b",
      "description": "LLaVA 7B is a multimodal large language model combining a vision encoder with the Vicuna language model, featuring approximately 7 billion parameters. It supports both text and image input for general-purpose visual question answering and image analysis. The model was updated to version 1.6, adding support for higher-resolution images and improved optical character recognition (OCR) and visual reasoning. Typical context window is limited by the underlying Vicuna base, generally in the 4k token range. It does not natively support audio or video analysis/generation. Release was in 2024, and it is primarily intended for multimodal conversational applications where understanding both text and images is required.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 4096,
      "parameter_count": "7b",
      "knowledge_cutoff": "2024-01",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "123": {
      "model": "llama3.2:latest",
      "description": "Llama 3.2, by Meta and available via Ollama, is a multilingual large language model collection spanning 1B, 3B, 11B, and 90B parameter sizes. The 1B and 3B models are text-only, optimized for dialogue, summarization, agentic retrieval, and function calling/tool use. The 11B and 90B 'Vision' models introduce multimodal text and image processing, supporting image understanding, captioning, and basic document analysis. Context window is up to 128,000 tokens for Vision models. Released in 2024, Llama 3.2 is designed for assistant-like chat, local deployment, and, for Vision models, visual understanding tasks. Text-only versions do not support image inputs; Vision models do not support audio or video modalities.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": "1B, 3B, 11B, 90B (model-dependent)",
      "knowledge_cutoff": "2023-12",
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "124": {
      "model": "mistral:latest",
      "description": "Mistral (latest) on Ollama is a transformer-based large language model, most commonly using the Mistral 7B architecture (7.3 billion parameters)[1][6]. Newer releases, such as Mistral Small 3.1 and Medium 3, expand this line with up to 24 billion parameters and context windows reaching up to 128,000 tokens, supporting complex reasoning and large document interactions[1][2][4][7]. Mistral models are designed primarily for advanced text generation, code synthesis, and logical reasoning tasks, with strong multilingual capabilities. Some recent variants—like Pixtral Large—offer multimodal text and image understanding, but standard Mistral 7B and its instruct/text-completion model as distributed by Ollama are text-only[1][2][6]. Mistral models do not natively support image generation, audio, or video modalities. Intended for local, secure, and high-performance deployments, use cases focus on chatbots, code assistants, document processing, and retrieval-augmented generation. Notable features include efficient Grouped-Query Attention (GQA), Sliding Window Attention (SWA), and fast inference speeds. Core limitations are the lack of native support for image, audio, or video analysis/generation in the standard Mistral releases.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 32000,
      "parameter_count": "7.3B",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "125": {
      "model": "llama4:latest",
      "description": "Llama 4 is a natively multimodal large language model developed by Meta, featuring a mixture-of-experts (MoE) architecture with models such as Llama 4 Scout boasting 109B parameters (17B active). It supports both text and image inputs, excels in visual recognition, image reasoning, captioning, and answering image-based queries. Its context window reaches up to 10 million tokens, leading the industry in sequence handling. Released in 2025, it is optimized for assistant-like chat, natural language generation, and visual reasoning. Llama 4's output capabilities focus on text and code; direct speech and video modalities are not supported.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        1.0
      ],
      "context_window": 10000000,
      "parameter_count": "109B total (17B active)",
      "knowledge_cutoff": "2025-06",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "126": {
      "model": "qwen2.5:32b-instruct",
      "description": "Qwen2.5-Coder-32B-Instruct is an open-source, instruction-tuned large language model released by Alibaba and available via Ollama. It uses a transformer architecture with rotary positional embeddings (RoPE), SwiGLU activations, and RMSNorm. The model consists of 32.5 billion parameters and 64 layers, with a generous 131,072-token context window. Its primary focus is high-performance text completion, particularly for code generation, reasoning, and code repair tasks across over 40 programming languages. This model does not natively support multimodal (image, audio, or video) analysis or generation; those capabilities are found in the separate Qwen2.5-VL series. Qwen2.5-Coder-32B-Instruct is best suited as an advanced coding assistant or text-based reasoning engine.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 131072,
      "parameter_count": "32.5b",
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "127": {
      "model": "gpt-5.1-2025-11-13",
      "description": "GPT-5.1, released in November 2025 by OpenAI, introduces a novel Mixture-of-Agents (MoA) architecture, enabling dynamic collaboration among specialized agents for complex queries. The model offers two variants—in 'Instant' mode for fast responses and 'Thinking' mode for advanced reasoning and thorough analysis. It natively supports multimodal inputs including text, images, audio, and real-time video streams, and can generate as well as analyze images, audio, and 3D object files. Context window and parameter size details remain undisclosed, but it improves instruction following, agentic workflows, and tool integration, with primary use cases spanning advanced decision support, code, technical explanations, and real-world multimodal data processing. Notable features include adaptive reasoning, robust safety audit trails, and low-latency responses.",
      "provider": "openai",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": "2025-11",
      "text_completion": true,
      "image_analysis": true,
      "image_gen": true,
      "audio_analysis": true,
      "audio_gen": true,
      "video_analysis": true,
      "video_gen": true,
      "reasoning": true
    },
    "128": {
      "model": "gpt-oss:20b",
      "description": "gpt-oss:20b is a 20.91 billion parameter Transformer-based language model with a Mixture-of-Experts (MoE) architecture, activating 3.6 billion parameters per token. It supports text completion and advanced reasoning, including chain-of-thought and configurable reasoning effort. The model is optimized for local deployment on consumer hardware with at least 16GB RAM, features a 128K context window, and uses MXFP4 quantization for efficiency. It is designed for local inference, privacy-sensitive tasks, and rapid iteration, with no support for multimodal inputs or outputs.",
      "provider": "ollama",
      "temperature_range": [
        0.0,
        2.0
      ],
      "context_window": 128000,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": true,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": true
    },
    "129": {
      "model": "all-minilm:latest",
      "description": "all-minilm:latest is a lightweight, fast sentence embedding model available on Ollama, based on the MiniLM architecture. It typically provides 384-dimensional dense vector representations for input text, optimized for semantic similarity, clustering, and retrieval tasks. The model is trained using self-supervised contrastive learning objectives on large sentence datasets. It is designed for text-to-vector embedding only, does not generate or analyze text, and does not support multimodal or generative functionalities. There is no public support for image, audio, or video inputs, nor advanced reasoning tasks. The model is primarily used for semantic search, document clustering, and information retrieval. The latest version focuses on efficiency and local offline deployment, but does not have a specified context window or parameter count publicly stated.",
      "provider": "ollama",
      "temperature_range": [],
      "context_window": 0,
      "parameter_count": null,
      "knowledge_cutoff": null,
      "text_completion": false,
      "image_analysis": false,
      "image_gen": false,
      "audio_analysis": false,
      "audio_gen": false,
      "video_analysis": false,
      "video_gen": false,
      "reasoning": false
    }
  }
}
