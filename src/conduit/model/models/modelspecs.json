{"_default": {"18": {"model": "llama3.1:latest", "description": "Llama3.1:latest is an earlier release in Meta's Llama 3 series, available in multiple parameter sizes (e.g., 8B, 70B). It is a text-only model for tasks like chat, code generation, summarization, and general language understanding. The model does not process images, audio, or video. Context window is typically around 8,000 to 32,000 tokens. Strong in reasoning and instruction following.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "varies", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "24": {"model": "gpt-4.1", "description": "GPT-4.1 is OpenAI's flagship large language model, designed for complex tasks, reasoning, and high-fidelity text generation. It is based on a transformer architecture and supports both text and image inputs (multimodal). The model features an expanded context window \u2014 typically 128k tokens \u2014 enabling it to manage lengthy interactions and context-rich documents. GPT-4.1 supports advanced reasoning and function calling. Known for robust performance on diverse benchmarks, it was released in early 2025 and is intended for applications requiring nuanced understanding and generation, including content creation, education, and consultation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "25": {"model": "gpt-4o", "description": "GPT-4o (\"omni\") is OpenAI's most advanced flagship model as of April 2025. It is fully multimodal, accepting text, image, and audio inputs, and can generate text and audio outputs. It features real-time, low-latency capabilities for conversation and supports a large context window, reportedly up to 128k tokens. GPT-4o is suitable for complex reasoning, function calling, and integration into conversational and assistant applications. Designed for seamless multimodal interaction, it excels in accessibility and real-world user engagement.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "26": {"model": "gpt-4-turbo", "description": "GPT-4 Turbo is an optimized variant of GPT-4 offering faster inference and lower costs, while maintaining the core GPT-4 architecture's capabilities. It is primarily focused on text and code generation with an extended context window (up to 128k tokens). Turbo does not natively support image, audio, or video processing. Released in late 2023, it is intended for scalable applications such as chatbots, code assistance, and generative tools where throughput and performance are important.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "27": {"model": "gpt-3.5-turbo-0125", "description": "GPT-3.5 Turbo (0125) is a cost-effective, high-performance text generation model based on the GPT-3.5 architecture. It supports text generation and completion tasks with a context window of up to 16,385 tokens. While capable in conversation, coding, and basic reasoning, GPT-3.5 Turbo does not have multimodal capabilities (image, audio, or video). It was released in early 2024 and is most commonly used for chatbots, drafting, summarization, and general text-based automation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16385, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "28": {"model": "gpt-4o-mini", "description": "GPT-4o Mini is a lighter, more efficient variant of the GPT-4o model. It is designed to balance speed and capability, featuring support for text and image input while maintaining strong reasoning abilities. Its context window is smaller than GPT-4o, optimized for lower latency and cost. Intended use cases include lightweight conversational agents and tasks where full GPT-4o performance is not required. Unlike GPT-4o, it does not natively process audio or video.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "29": {"model": "gpt-4o-audio-preview", "description": "GPT-4o Audio Preview is a specialized variant of GPT-4o, focused on low-latency speech recognition (speech-to-text), speech translation, and text-to-speech (audio generation). It is designed for \"speech in, speech out\" conversational systems and supports multimodal input. With real-time audio capabilities and strong reasoning, it is well-suited for accessibility tools, voice assistants, and rapid dialogue applications. Its context window size and parameter count are not publicly detailed; released in 2025.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "30": {"model": "o1-preview", "description": "O1-Preview is part of OpenAI\u2019s reasoning-focused \"o-series\" models, aimed at complex, multi-step analytical and logical reasoning. It is a general-purpose model built for improved problem solving and knowledge tasks, with a context window likely between 8k and 32k tokens. Unlike flagship GPT models, o1-preview does not support image, audio, or video input. The model is primarily intended for high-quality text generation and in-depth reasoning applications.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "31": {"model": "o1-mini", "description": "O1-Mini is a deprecated, lightweight variant of the o1 reasoning model. It focuses on efficient text processing and basic reasoning and is designed for lower resource consumption. The model lacks multimodal capabilities, handling only text input and output, with a modest context window (typically 4k\u20138k tokens). O1-Mini is intended for simple chatbot, summarization, and text classification tasks.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "32": {"model": "o3-mini", "description": "O3-Mini is a small-scale variant of OpenAI's o3 reasoning model, engineered for efficiency and speed. It is suited for applications demanding rapid response and lower compute costs, supporting advanced text generation and reasoning. O3-Mini has a limited context window (8k tokens) and does not feature multimodal or generative audio/video capabilities. Its design targets chatbots, simple assistants, and text automation workflows.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "33": {"model": "o3-mini-high", "description": "O3-Mini-High is an enhanced version of O3-Mini, offering improved reasoning, accuracy, and performance at a small model size. It retains the same primary focus: text generation and complex analytical tasks, with an 8k context window. Like other mini models, O3-Mini-High does not support multimodal processing. It is suitable for efficient deployment in assistants, summarization, and logical inference use cases.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "34": {"model": "o4-mini", "description": "O4-Mini is a compact, high-speed reasoning model in OpenAI\u2019s o-series, optimized for lower latency and deployment cost. It excels at text generation and multi-step reasoning with a context window of 8k\u201316k tokens. O4-Mini does not support image, audio, or video input/output. Intended uses include chatbots, digital assistants, and lightweight analysis tools where performance-to-cost ratio is key.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "35": {"model": "claude-3-5-haiku-20241022", "description": "Claude 3.5 Haiku (October 2024) is part of Anthropic's third-generation model lineup, designed as the fastest and most cost-effective tier. Built on a large transformer architecture, it excels in rapid text processing, general-purpose text and code tasks, and supports image analysis (multimodal input) but does not generate images or audio. It typically features a substantial context window (up to 200K tokens in advanced versions) and is optimized for applications requiring high throughput and responsiveness. Main use cases include content generation, summarization, and analysis at scale. Notable limitations include a lack of image, audio, and video generation capabilities.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "36": {"model": "claude-3-7-sonnet-20250219", "description": "Claude 3.7 Sonnet (February 2025) is Anthropic's mid-size transformer-based model in the Claude 3 family, targeting a balance of capability, speed, and operating cost. It supports robust text generation, strong reasoning, and image analysis through multimodal inputs, but does not generate images, audio, or video. With a large context window (likely up to 200K tokens), it is intended for high-volume applications such as data analysis, code synthesis, search, and workflow orchestration. Limitations include no image or audio output and no video understanding or production.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "37": {"model": "claude-opus-4-20250514", "description": "Claude Opus 4 (May 2025) is Anthropic's flagship large multimodal model, boasting advanced reasoning, coding abilities, and sophisticated tool use. Built on a large transformer architecture, it offers a 200K token context window and is designed for enterprise-grade AI agent applications and autonomous multi-step workflows. Opus 4 delivers top-tier performance on complex tasks, including research synthesis and orchestrating cross-functional activities, with vision (image analysis) support but no image, audio, or video generation features.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "38": {"model": "claude-sonnet-4-20250514", "description": "Claude Sonnet 4 (May 2025) is a mid-size transformer model in Anthropic\u2019s Claude portfolio, optimized for quality, responsiveness, and cost-efficiency. With a 200K token context window, it tackles high-volume use cases such as workflow automation, data analysis, and code generation. Sonnet 4 supports robust text generation, advanced reasoning, and image understanding (vision input), but does not create images, audio, or video. It is well-suited for scalable task-specific applications within larger AI systems.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "39": {"model": "gemini-2.5-flash-preview-05-20", "description": "Gemini 2.5 Flash (Preview 05-20) is a multimodal language model released by Google in 2025 as part of the Gemini family. It is optimized for rapid response and efficiency in handling text, image, video, and audio inputs, with a context window sufficient for most practical applications (reportedly up to 1 million tokens in the Gemini 1.5 family; specifics for 2.5 are not fully disclosed but are similar). The Flash variant is designed for price-performance and supports advanced reasoning and step-by-step thinking capabilities. It is suited for conversational agents, document analysis, and multimodal data tasks, but lacks image, audio, and video generation functionalities. Intended for scalable, production-grade deployments.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "40": {"model": "gemini-2.5-pro-exp-03-25", "description": "Gemini 2.5 Pro (Experimental 03-25) is an advanced multimodal LLM by Google, focused on high-level reasoning, complex problem-solving, and robust comprehension across text, image, audio, code, and video. It features an input size limit of 500 MB and supports large context windows (similar to earlier Gemini 1.5 models, likely up to 1 million tokens). Designed for challenging applications such as scientific research, advanced analytics, and enterprise-scale tasks. Known for deep multimodal understanding, but does not support content generation in image, audio, or video formats.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "41": {"model": "gemini-2.5-pro-preview-05-06", "description": "Gemini 2.5 Pro (Preview 05-06) is Google's most advanced reasoning model in the Gemini series, released in 2025. It is capable of handling and integrating information from large, diverse datasets, including text, audio, imagery, video, and code repositories. Its context window is similar to the Gemini 2.5 series (on the order of 1 million tokens). The model is optimized for deep reasoning, multimodal analysis, research, and enterprise use. It can analyze but not generate images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "42": {"model": "gemini-2.0-flash-001", "description": "Gemini 2.0 Flash 001 is an earlier fast-response variant in Google\u2019s Gemini line, designed for both multimodal data ingestion (text, image, audio, video) and rapid inference, but with smaller context windows and fewer advanced features than subsequent Gemini 2.5 models. It is intended for scenarios requiring quick, scalable responses with solid reasoning and multimodal analysis, but it does not support generation of images, audio, or video. Used primarily for chatbots, Q&A, and basic document/image analysis.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2023-12", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "43": {"model": "gemini-1.5-pro", "description": "Gemini 1.5 Pro is a leading multimodal model from Google, offering advanced reasoning and comprehension across text, documents, code, images, audio, and video. It features a context window of up to 1 million tokens, enabling analysis of large inputs and complex cross-modal interactions. Designed for tasks involving deep research, analytics, or enterprise-grade analysis, it can process but not generate image/audio/video. Released in 2024, it supports conversational use, document/image/video understanding, and complex problem solving.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "44": {"model": "gemini-1.5-flash", "description": "Gemini 1.5 Flash is a fast, efficient multimodal model released by Google in early 2024. It is designed for rapid inference, handling text, images, audio, and video inputs. This model trades off some depth of reasoning for latency, making it suitable for chat assistants and high-throughput applications. It supports large context windows (up to 1 million tokens), with strengths in multimodal content analysis and quick response. The model does not support generating new images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "45": {"model": "gemini-1.5-flash-8b", "description": "Gemini 1.5 Flash-8B is a smaller, efficient multimodal variant of Gemini Flash, likely featuring 8 billion parameters (exact size not official). Released in 2024, it is engineered for speed and moderate resource usage, supporting text, image, audio, and video analysis but not media generation. It is especially suited for latency-sensitive applications, chatbots, and batch document/image/video processing with a context window similar to the Gemini 1.5 series. Unlike the Pro versions, it sacrifices some analytical depth for efficiency.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": "8b", "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "46": {"model": "llama3-8b-8192", "description": "Llama3-8b-8192 is an 8 billion parameter transformer-based language model, featuring an 8,192 token context window. Designed primarily for text-based tasks, it excels in text completion, dialogue, and reasoning. The model supports function calling and is optimized for rapid inference. It is primarily intended for chatbot, code generation, and conversational AI use cases, leveraging Groq\u2019s fast inference hardware for low-latency outputs.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "8B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "47": {"model": "llama3-70b-8192", "description": "Llama3-70b-8192 is a 70 billion parameter transformer-based language model with an 8,192 token context window. It is highly optimized for dialogue, content generation, and complex reasoning tasks. The model maintains a strong MMLU score (79.5%) and supports function calling and tool use. Designed for production-ready, fast, and consistent outputs, it is widely used for advanced chatbots, research, and large-scale automation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "48": {"model": "mixtral-8x7b-32768", "description": "Mixtral-8x7b-32768 is a mixture-of-experts model with 8 experts of 7B parameters each (total active parameters per forward pass: ~12.9B), featuring a 32,768 token context window. It is designed for advanced text generation, reasoning, and cost-effective inference at scale. The model focuses on open-ended dialogue, code generation, and large-document processing, leveraging Groq\u2019s speed for real-time interaction.", "provider": "google", "temperature_range": [], "context_window": 32768, "parameter_count": "8x7B (active ~12.9B per pass)", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "49": {"model": "gemma2-9b-it", "description": "Gemma2-9b-it is a 9 billion parameter language model from Google\u2019s Gemma 2 series, likely deployed on Groq for fast inference. It features a transformer architecture and is optimized for instruction following and text-based tasks. The model supports text completion and reasoning, focusing on instruction-based dialogue, summarization, and knowledge extraction. Multimodal or image/audio/video capabilities are not available.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "9B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "50": {"model": "llama-3.3-70b-versatile", "description": "Llama-3.3-70b-versatile is a 70 billion parameter transformer-based model, likely a variant of Meta\u2019s Llama 3, optimized for versatile use cases with a large context window. It supports advanced reasoning, function calling, and tool use. The model is designed for complex language understanding, automation, and research applications, but it does not support image, audio, or video analysis or generation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "53": {"model": "sonar-reasoning", "description": "Sonar-reasoning is likely a variant of the Sonar model, optimized for logical reasoning tasks. It is built on top of the Llama 3.3 70B architecture and focuses on providing accurate and factual responses. It does not support multimodal inputs like image or audio analysis. The model is part of Perplexity's suite, designed for real-time information retrieval and high-quality responses.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "54": {"model": "sonar-pro", "description": "Sonar-Pro is a variant of the Sonar model, designed for professional users. It is built on the Llama 3.3 70B architecture and enhances factuality and readability, making it suitable for interactive applications. It does not support multimodal inputs like image or audio analysis. It is optimized for speed and real-time information access.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "55": {"model": "sonar", "description": "Sonar is a cutting-edge AI model built on the Llama 3.3 70B architecture. It is optimized for enhanced factuality, readability, and speed. Sonar provides real-time access to information and supports text-based interactions but lacks multimodal capabilities like image or audio analysis. It is designed for fast and accurate responses in search environments.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "56": {"model": "gpt-4.1-mini", "description": "GPT-4.1 Mini is a mid-sized transformer-based large language model developed by OpenAI, released in April 2025. It features a 1 million token context window and delivers performance competitive with GPT-4o but with significantly reduced latency and cost. GPT-4.1 Mini supports both text and image understanding (multimodal input for vision tasks), strong code generation, and advanced instruction following. It is suitable for interactive applications requiring high throughput, but it does not generate images, audio, or video, and lacks audio/video analysis or synthesis capabilities. Fine-tuning is supported for domain-specific tasks.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-04", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "57": {"model": "gemini-2.5-pro-preview-tts", "description": "Gemini 2.5 Pro Preview TTS is a large multimodal language model from Google, released in June 2025. It is designed primarily for advanced text-to-speech (TTS) generation, supporting both single and multi-speaker audio outputs. The model offers fine-grained control over style, emotion, pace, accent, and pronunciation in generated speech, with a 32k token context window. Gemini 2.5 Pro Preview TTS accepts only text input and outputs audio, making it ideal for podcasts, audiobooks, and dynamic audio content. It does not process images, video, or audio inputs, nor does it generate images or video.", "provider": "google", "temperature_range": [0.0, 2.0], "context_window": 32000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": false}, "58": {"model": "gemini-2.5-flash-preview-tts", "description": "Gemini 2.5 Flash Preview TTS is a text-to-speech model supporting over 80 languages and enabling podcast creation with multiple speakers. It is designed for efficient performance and offers dynamic control over its thinking budget. The model is part of the Gemini 2.5 family, which excels in reasoning and multimodal capabilities. It is optimized for speed and cost-effectiveness.", "provider": "google", "temperature_range": [], "context_window": 1, "parameter_count": null, "knowledge_cutoff": null, "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "61": {"model": "dall-e-3", "description": "DALL-E 3 is a text-to-image generative model developed by OpenAI, released in 2023 as a major improvement over previous versions. It is built on a transformer architecture related to GPT-3, focusing on producing highly detailed and accurate images from complex textual prompts. While its parameter count and exact context window are undisclosed, DALL-E 3 is designed exclusively for image generation and refinement based on detailed natural language instructions. It supports integration with ChatGPT Plus for conversational, iterative image creation; however, it does not natively support text, audio, or video analysis or generation. Its primary use case is creative, designer-level image synthesis, and prompt-based visual art generation.", "provider": "openai", "temperature_range": [0.0, 1.0], "context_window": 0, "parameter_count": null, "knowledge_cutoff": "2023-10", "text_completion": false, "image_analysis": false, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "63": {"model": "imagen-3.0-generate-002", "description": "Imagen 3.0 (model: imagen-3.0-generate-002) is a state-of-the-art text-to-image diffusion model developed by Google DeepMind, released in 2024 and integrated into Google Gemini and other platforms. The model specializes in generating high-resolution (up to 2048x2048px), photorealistic, and visually faithful images from natural language prompts. It supports nuanced control over composition and visual style, as well as multi-subject scene synthesis, but does not process text, audio, or video inputs. The model is not designed for text, audio, or video analysis or generation tasks. Intended uses include creative image generation, storyboarding, design prototyping, and advertising. Technical specifications beyond its generative focus (such as architecture details or parameter size) are not disclosed by Google.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 0, "parameter_count": null, "knowledge_cutoff": "2025-07", "text_completion": false, "image_analysis": false, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "65": {"model": "gpt-oss:latest", "description": "GPT-OSS is an open-source large language model series released by OpenAI and integrated by Ollama, available in two primary sizes: gpt-oss-20b (21B parameters) and gpt-oss-120b (117B parameters). Both use a Mixture-of-Experts (MoE) architecture with 4-bit (MXFP4) quantization for efficient inference. The models are optimized for instruction following, chain-of-thought reasoning, and agentic capabilities such as function calling and tool use. They support text completion and structured chats, and can run on a range of hardware\u2014from consumer RTX GPUs (20b) to high-end servers (120b). There is support for extended context window sizes, though the exact maximum is not specified. The primary intended use is as a general-purpose text LLM for reasoning, code, and workflow automation. The models do not natively support image, audio, or video analysis or generation.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32768, "parameter_count": "21B/117B", "knowledge_cutoff": "2025-08", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "89": {"model": "gpt-5", "description": "GPT-5 (OpenAI, released August 2025) is a unified multimodal architecture available in three variants (regular, mini, nano), designed to support sophisticated text reasoning and complex analytical tasks. It supports both text and image as input, with a maximum input context window of 272,000 tokens and output limit of 128,000 tokens. Capabilities include deep reasoning, multi-turn voice conversations, real-time web search integration, and a creative canvas workspace. Knowledge cutoff is September 30, 2024. Primary use cases include advanced dialogue, research, creative ideation, and multimodal analysis. Notable features: dynamic model selection, context-aware mode switching, support for custom grammars, and a focus on a seamless, holistic AI workbench. Limitations: image output, audio and video generation/analysis are not yet supported.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 272000, "parameter_count": null, "knowledge_cutoff": "2024-09-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "90": {"model": "gpt-5-mini", "description": "GPT-5-mini is a scaled-down variant of OpenAI's GPT-5 family, designed for efficient real-time applications with minimal latency. Released in August 2025, its knowledge cutoff is May 30th, 2024. The model offers a context window of 272,000 tokens for input and 128,000 tokens for output. GPT-5-mini supports both text and image inputs, but only generates text as output. It is available for direct API access at various reasoning levels (minimal, low, medium, high), accommodating a wide range of task complexities. Its primary focus is on scalable, high-throughput text and multimodal reasoning tasks. Notable limitations include the absence of image, audio, and video generation or audio/video analysis capabilities.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 272000, "parameter_count": null, "knowledge_cutoff": "2024-05-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "91": {"model": "gpt-5-nano", "description": "GPT-5 Nano is a developer-focused, compact variant of OpenAI's GPT-5 family. It is designed for efficiency and high-throughput applications, supporting text and image inputs, with text-only output. The model supports function calling, agentic tool use, and offers developer controls such as adjustable reasoning effort levels and verbosity control. It has a context window of up to 400,000 tokens. Its knowledge cutoff is May 30, 2024. Primary use cases include scalable text generation, reasoning tasks, and agentic workflows, but it does not generate images, audio, or video.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 400000, "parameter_count": null, "knowledge_cutoff": "2024-05-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "98": {"model": "claude-opus-4-1-20250805", "description": "Claude Opus 4.1, released by Anthropic on August 5, 2025, is a large language model designed for advanced reasoning, coding, agentic tasks, and research-grade content synthesis. It operates via a transformer architecture (specific parameter count undisclosed) and supports a large context window and long outputs (up to 32K tokens). The model offers hybrid reasoning capabilities, excelling at multi-step logical analysis, code generation across large files, and complex business or research workflows. It is accessible via API and major cloud platforms. Claude Opus 4.1 does not natively support image, audio, or video processing or generation\u2014it is strictly text and code oriented, with primary use cases in software engineering, intensive research, and agentic workflows.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": null, "knowledge_cutoff": "2025-05", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "99": {"model": "all-minilm:latest", "description": "The all-minilm:latest model, available in the Ollama platform as of 2025, is a lightweight, transformer-based text embedding model designed for fast, local computation of high-dimensional vector representations of text. It operates with 384-dimensional output vectors and is well suited for semantic search, clustering, and information retrieval tasks. This model supports only text embedding (not text generation), does not process images, audio, or video, and is optimized for environments requiring efficient, privacy-preserving NLP workflows. It is not multimodal, does not support reasoning as found in large language models, and is primarily used for converting text into embeddings for downstream tasks. Typical hardware requirements are modest (2-4GB RAM; optionally GPU) and the model is ideal for on-premise or offline deployment.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 512, "parameter_count": "unknown", "knowledge_cutoff": null, "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "100": {"model": "gpt-oss:20b", "description": "gpt-oss:20b is a 21B parameter (3.6B active) mixture-of-experts large language model collaboratively developed by OpenAI and distributed through Ollama and the open-weight ecosystem. It uses the MXFP4 quantization format, enabling local deployment on consumer devices with at least 16GB RAM. The architecture is optimized for efficient chat, agentic tasks (including tool/function calling and light code execution), chain-of-thought reasoning, and customizable reasoning effort. It features a large context window of 128,000 tokens, supports fine-tuning, and has an Apache 2.0 license. Released in mid-2024, its primary focus is high-quality text reasoning and developer-centric customization; it does not include native support for multimodal input (image/audio/video).", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "21B (3.6B active)", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "101": {"model": "qwen2.5:32b-instruct", "description": "Qwen2.5-Coder-32B-Instruct is an instruction-tuned, 32 billion-parameter causal language model from Alibaba\u2019s Qwen team, released in late 2024. The model is specialized for code generation, code reasoning, and code repair tasks, achieving state-of-the-art performance across popular code benchmarks. It supports text completion and advanced reasoning, with a context window large enough for multi-file code editing. It is designed primarily as a code assistant and does not offer multimodal capabilities such as image or audio input/output.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 4000, "parameter_count": "32b", "knowledge_cutoff": "2024-11", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "102": {"model": "gemini-2.5-flash-image-preview", "description": "Gemini 2.5 Flash Image Preview is a multimodal large language model developed by Google, released in August 2025. It supports both text and image inputs and outputs, enabling conversational text generation, image generation, and precise image editing directly via natural language instructions. The model offers a context window of up to 32,768 tokens and incorporates world knowledge for more advanced image rendering and manipulation. It does not support audio or video input/output. Typical use cases include creative writing with rich images, educational applications, and interactive design. All AI-generated images include invisible watermarking for provenance.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 32768, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "103": {"model": "claude-sonnet-4-5-20250929", "description": "Claude Sonnet 4 is a large language model developed by Anthropic, officially released on May 22, 2025, as part of the Claude 4 family. It features a 200,000-token context window, supports hybrid reasoning modes (instant or extended step-by-step thinking), and is optimized for applications such as coding, advanced reasoning, code review, agentic workflows, and customer-facing AI agents. Sonnet 4 enables multimodal inputs\u2014specifically, analysis of image inputs as well as text\u2014but it does not support generative image, audio, or video outputs. The model can integrate with external tools and APIs (tool use), supports agentic code execution, and offers improved memory management. Key technical strengths include strong code analysis, advanced planning, and flexible integration with developer tools and APIs. Sonnet 4 is designed primarily for text and image understanding, stepwise reasoning, and workflow automation, rather than creation of novel images, audio, or video.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-05", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "104": {"model": "claude-haiku-4-5-20251001", "description": "Claude Haiku 4.5, released by Anthropic in October 2025, is a fast, compact large language model in their small, efficient class. It supports a 200,000-token context window and is engineered for high-throughput, low-latency applications such as chat agents, customer service tools, and coding assistants. Haiku 4.5 is the first in its series to provide advanced features like extended thinking, chain-of-thought reasoning, enhanced computer-use support, and full multimodal processing of both text and image input. It is particularly optimized for reasoning, agentic coding, and scalable business deployments, but does not support image, audio, or video generation.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-10", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}}}