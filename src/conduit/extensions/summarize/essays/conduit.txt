CONTEXT: 174 files found in conduit-project.

## FILE: pyproject.toml
```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "conduit"
version = "2.0.0"
description = "A lightweight, unified framework for building LLM applications"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    # Core dependencies
    "pydantic",
    "jinja2",
    "rich",
    "tiktoken",
    "instructor[perplexity]",
    "tinydb",
    "python-dotenv",
    # LLM Provider SDKs
    "openai",
    "anthropic",
    "google-generativeai",
    "ollama",
    # Audio/Image Processing
    "pydub",
    "pillow",
    "soundfile",
    # Web/HTTP
    "headwater_api",
    "headwater_client",
    # Dev tools
    "ruff>=0.12.4",
    "ty>=0.0.1a15",
    # database
    "dbclients",
    "xdg-base-dirs>=6.0.2",
    "prompt-toolkit>=3.0.52",
    "transformers>=4.57.1",
    "sentencepiece>=0.2.1",
    "rapidfuzz>=3.14.3",
    "pyperclip>=1.11.0",
    "pytest-asyncio>=1.2.0",
    "markdownify>=1.2.2",
    "readabilipy>=0.3.0",
    "pathspec>=0.12.1",
    "pyyaml>=6.0.3",
    "asyncpg>=0.31.0",
]

[project.optional-dependencies]
dev = [
    "pytest",
    "pytest-asyncio",
    "ruff>=0.12.4",
]

# ── Hatchling (build) ──────────────────────────────────────────────────────────
# Point Hatchling at the src/ package; this is the critical bit for src-layout.
[tool.hatch.build.targets.wheel]
packages = ["src/conduit"]
sources = ["src"]
include = ["src/conduit/py.typed"]

# If you ship non-.py files inside the package (configs, prompts, assets),
# list them so they’re included in wheels and sdists.
[tool.hatch.build]
include = [
  "src/twig/**/*.txt",
  "src/twig/**/*.md",
  "src/twig/**/*.json",
  "src/twig/**/*.yaml",
  "src/twig/**/*.yml",
  "src/twig/**/*.ini",
  "src/twig/**/*.jinja*",
  "src/twig/assets/**",
]

[tool.hatch.build.targets.sdist]
include = ["src/**", "README.*", "LICENSE*", "*.json"]

# ── Pytest ─────────────────────────────────────────────────────────────────────
[tool.pytest.ini_options]
addopts = "-v -s --tb=short --no-header --showlocals --pdb -x"
log_cli = true
log_cli_level = "INFO"

# ── Ruff ───────────────────────────────────────────────────────────────────────
[tool.ruff]
line-length = 88
target-version = "py312"  # align with requires-python

[tool.ruff.lint]
select = [
    "F", "E",
    "UP",
    "SIM",
    "ARG",
    "B",
    "RUF",
    "S102", "S103", "S108",
    "PERF",
    "FBT003",
]
ignore = [
    "SIM118",
    "ARG001", "ARG002",
    "E501",
]

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = ["S101", "ARG", "FBT"]
"scripts/**/*.py" = ["ARG", "SIM"]

[tool.ruff.lint.isort]
known-first-party = ["twig"]
force-sort-within-sections = true
split-on-trailing-comma = true

[tool.uv.workspace]
members = [
    ".",
]

# ── uv sources (local/editable deps) ───────────────────────────────────────────
[tool.uv.sources]
dbclients = { path = "../dbclients-project", editable = true }
headwater_api = { path = "../headwater/headwater-api", editable = true }
headwater_client = { path = "../headwater/headwater-client", editable = true }

[project.scripts]
bchat = "conduit.apps.scripts.chat_cli:main"
chat = "conduit.apps.scripts.fchat_cli:main"
imagegen = "conduit.apps.scripts.imagegen_cli:main"
models = "conduit.apps.scripts.models_cli:main"
tokens = "conduit.apps.scripts.tokens_cli:main"
update = "conduit.apps.scripts.update_modelstore:main"
update_ollama = "conduit.apps.scripts.update_ollama_list:main"
tokenize="conduit.apps.scripts.tokenize_cli:main"
conduit = "conduit.apps.scripts.conduit_cli:main"
ask = "conduit.apps.scripts.conduit_cli:query_entrypoint"

```

## FILE: src/conduit/__init__.py
```py
from conduit.utils.logs import logging_config

_ = logging_config

```

## FILE: src/conduit/apps/chat/app.py
```py
from __future__ import annotations
from conduit.apps.chat.engine.async_engine import ChatEngine
from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.utils.progress.verbosity import Verbosity
import logging

logger = logging.getLogger(__name__)


class ChatApp:
    """
    The main chat application.
    """

    def __init__(
        self,
        engine: ChatEngine,
        input_interface: InputInterface,
        welcome_message: str = "",
        verbosity: Verbosity = Verbosity.SILENT,
    ):
        # Init variables
        self.engine: ChatEngine = engine
        self.input_interface: InputInterface = input_interface
        self.welcome_message: str = welcome_message
        self.verbosity: Verbosity = verbosity
        # Control variable for the main loop
        self.is_running: bool = True

    async def run(self) -> None:
        """
        Start the main event loop for the chat application.

            Initializes the session by clearing the screen and displaying the welcome
            message, then enters a continuous loop to process user input and generate
            responses until the application is terminated via the `is_running` flag.
        """
        # Clear the screen at the start of the chat session
        self.input_interface.clear_screen()

        # IMPORTANT: Route all startup output through the input interface so EnhancedInput
        # can print safely without corrupting the prompt UI.
        from conduit.apps.chat.ui.logo import get_logo

        self.input_interface.show_message(get_logo())

        if self.welcome_message:
            self.input_interface.show_message(self.welcome_message)

        while self.is_running:
            await self.run_once()

    async def run_once(self) -> None:
        """
        Execute a single cycle of the chat loop: get input, process it, and display output.

            Retrieves user input asynchronously. If the input starts with '/', it is routed
            to command execution; otherwise, it is processed as a conversation query via the
            engine. Handles input cancellation (e.g., Ctrl+C) by updating the running state
            and catches execution errors to prevent app crashes.
        """
        user_input = await self.input_interface.get_input()
        # Handle case where input can be cancelled (e.g., Ctrl+C)
        if user_input is None:
            self.is_running = False
            return

        try:
            if user_input.startswith("/"):
                output = await self.engine.execute_command(user_input, self)
                if output:
                    self.input_interface.show_message(output)
                # Commands may have side effects without returning output (e.g., /exit, /wipe)
            else:
                # If user only pressed enter, skip processing
                if not user_input.strip():
                    return
                output = await self.engine.handle_query(user_input)
                if output:
                    self.input_interface.show_message(output)
                else:
                    logger.warning(f"Query returned no output for: {user_input}")
        except Exception as e:
            logger.exception(f"Error processing input: {user_input}")
            self.input_interface.show_message(f"[red]Error: {e}[/red]")

```

## FILE: src/conduit/apps/chat/create_app.py
```py
"""
This module serves as the factory layer for the Conduit chat application, abstracting the dependency injection logic required to instantiate a fully configured `ChatApp`. It orchestrates the initialization of the `ChatEngine` for conversation state management and selects the appropriate `InputInterface` strategy (either standard asynchronous input or the enhanced `prompt_toolkit` UI) based on the provided mode.

The primary function connects generation parameters and runtime options to the engine while ensuring correct wiring between the input layer and the engine. This centralization allows distinct CLI entry points to share identical setup logic while supporting features like command auto-completion, which require circular dependency resolution between the UI and the engine.
"""

from conduit.apps.chat.app import ChatApp
from conduit.apps.chat.engine.async_engine import ChatEngine
from conduit.apps.chat.ui.async_input import AsyncInput
from conduit.apps.chat.ui.enhanced_input import EnhancedInput
from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from rich.console import Console


def create_chat_app(
    preferred_model: str,
    welcome_message: str,
    system_message: str,
    input_mode: str,
    options: ConduitOptions,
) -> ChatApp:
    """
    Factory function to create a fully configured async ChatApp.
    """
    # Create dependencies
    params = GenerationParams(model=preferred_model, system=system_message)
    engine = ChatEngine(params=params, options=options)

    # Select and create input interface
    if input_mode == "enhanced":
        console = Console()
        input_interface: InputInterface = EnhancedInput(console)
    else:
        input_interface = AsyncInput()

    # Create app with all dependencies
    app = ChatApp(
        engine=engine,
        input_interface=input_interface,
        welcome_message=welcome_message,
        verbosity=options.verbosity,  # Verbosity comes from options now
    )

    if isinstance(input_interface, EnhancedInput):
        input_interface.set_engine(engine)

    return app

```

## FILE: src/conduit/apps/chat/demo_app.py
```py
"""
Factory module for the Conduit chat application.
Now supports 'demo' mode for the split-screen TUI.
"""

from conduit.apps.chat.app import ChatApp
from conduit.apps.chat.engine.async_engine import ChatEngine
from conduit.apps.chat.ui.async_input import AsyncInput
from conduit.apps.chat.ui.enhanced_input import EnhancedInput
from conduit.apps.chat.ui.demo_tui import DemoInput
from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from rich.console import Console


def create_chat_app(
    preferred_model: str,
    welcome_message: str,
    system_message: str,
    input_mode: str,
    options: ConduitOptions,
) -> ChatApp:
    """
    Factory function to create a fully configured async ChatApp.
    """
    # Create dependencies
    params = GenerationParams(model=preferred_model, system=system_message)
    engine = ChatEngine(params=params, options=options)

    # Select and create input interface
    if input_mode == "demo":
        console = Console()
        input_interface: InputInterface = DemoInput(console)
    elif input_mode == "enhanced":
        console = Console()
        input_interface = EnhancedInput(console)
    else:
        input_interface = AsyncInput()

    # Create app with all dependencies
    app = ChatApp(
        engine=engine,
        input_interface=input_interface,
        welcome_message=welcome_message,
        verbosity=options.verbosity,
    )

    # Dependency Injection: Wire the engine back into the UI for history access
    if isinstance(input_interface, (EnhancedInput, DemoInput)):
        input_interface.set_engine(engine)

    return app

```

## FILE: src/conduit/apps/chat/demo_entry.py
```py
import asyncio
import logging
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.sync import Verbosity

# Assuming you placed demo_app.py in conduit/apps/chat/ alongside create_app.py
from conduit.apps.chat.demo_app import create_chat_app

# Configure logging to write to a file, NOT stdout,
# because stdout is owned by the TUI now.
logging.basicConfig(
    filename="conduit_debug.log",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)


async def main():
    # 1. Configuration
    options = ConduitOptions(project_name="demo-tui", verbosity=Verbosity.SILENT)

    # 2. Factory Creation (Force "demo" mode)
    app = create_chat_app(
        preferred_model="haiku",
        welcome_message="[bold cyan]Welcome to the TUI Demo![/bold cyan]\nType your message below and press [bold green]Enter[/bold green].",
        system_message="You are a helpful, concise assistant running in a TUI demo.",
        input_mode="demo",
        options=options,
    )

    # 3. Run the Event Loop
    try:
        await app.run()
    except KeyboardInterrupt:
        # Handle Ctrl+C gracefully if it bubbles up
        pass


if __name__ == "__main__":
    asyncio.run(main())

```

## FILE: src/conduit/apps/chat/engine/async_engine.py
```py
from __future__ import annotations
import re
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.message.message import UserMessage
from conduit.domain.message.role import Role
from conduit.core.engine import Engine
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.apps.chat.app import ChatApp


class ChatEngine:
    """
    The asynchronous chat engine.
    """

    # Command Handlers
    async def help(self, app: ChatApp) -> str:
        """
        Displays the help message.
        """
        return "Available commands:\n/help\n/wipe\n/exit\n/history\n/models\n/model\n/set model <model_name>"

    async def history(self, app: ChatApp) -> None:
        """
        Displays the conversation history.
        """
        self.conversation.print_history()

    async def models(self, app: ChatApp) -> str:
        """
        Displays available models.
        """
        from conduit.core.model.models.modelstore import ModelStore

        model_store = ModelStore()
        return model_store._generate_renderable_model_list()

    async def model(self, app: ChatApp) -> str:
        """
        Displays the current model.
        """
        return f"Current model: {self.params.model}"

    async def set_model(self, app: ChatApp, model_name: str | None) -> str:
        """
        Sets the current model.
        """
        if model_name:
            self.params.model = model_name
            return f"Set model to {model_name}"
        return "Error: Please provide a model name (e.g., /set model gpt-4)"

    async def wipe(self, app: ChatApp) -> None:
        """
        Wipes the conversation.
        """
        self.conversation.wipe()

    async def exit(self, app: ChatApp) -> None:
        """
        Exits the application.
        """
        app.is_running = False

    def __init__(
        self,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
    ):
        self.conversation = Conversation()
        self.params = params or GenerationParams.defaults(
            "claude-3-sonnet"
        )  # Default if not provided
        # Options should always be provided at app creation, but provide a default for robustness
        self.options = options or ConduitOptions(project_name="default-chat-app")
        self.commands = {
            "/help": self.help,
            "/wipe": self.wipe,
            "/exit": self.exit,
            "/history": self.history,
            "/models": self.models,
            "/model": self.model,
            "/set model": self.set_model,
        }

    def _parse_command_args(self, command_string: str) -> tuple[str, list[str]]:
        """
        Splits a command string into the command name and its arguments,
        prioritizing multi-word commands defined in self.commands.
        Supports quoted strings for multi-word arguments.
        """
        # Remove leading '/'
        command_string = command_string[1:]

        # Find the longest matching command name
        cmd_name = ""
        remaining_args_string = command_string
        for registered_cmd in sorted(self.commands.keys(), key=len, reverse=True):
            if command_string.startswith(
                registered_cmd[1:]
            ):  # Compare without leading slash
                cmd_name = registered_cmd
                remaining_args_string = command_string[
                    len(registered_cmd[1:]) :
                ].strip()
                break

        if not cmd_name:
            # If no registered command prefix matches, try parsing the first word as command
            parts = re.findall(r'"([^"]*)"|(\S+)', command_string)
            parsed_parts = [quoted or unquoted for quoted, unquoted in parts]
            if parsed_parts:
                cmd_name = "/" + parsed_parts[0]
                remaining_args_string = " ".join(parsed_parts[1:])
            else:
                return "", []  # No command found

        # Parse remaining arguments
        args_parts = re.findall(r'"([^"]*)"|(\S+)', remaining_args_string)
        args = [quoted or unquoted for quoted, unquoted in args_parts]

        return cmd_name, args

    async def handle_query(self, user_input: str) -> str | None:
        """
        Handles a user's query.
        """
        if not user_input.strip():
            return None
        self.conversation.add(UserMessage(content=user_input))
        self.conversation = await Engine.run(
            self.conversation, params=self.params, options=self.options
        )

        if (
            self.conversation.messages
            and self.conversation.messages[-1].role == Role.ASSISTANT
        ):
            return self.conversation.content
        return None

    async def execute_command(self, command_string: str, app: ChatApp) -> str | None:
        """
        Executes a command string.
        """
        cmd_name, args = self._parse_command_args(command_string)

        handler = self.commands.get(cmd_name)
        if handler:
            # Special handling for commands that take arguments
            if cmd_name == "/set model":
                if args:
                    return await handler(app, args[0])
                else:
                    return await handler(
                        app, None
                    )  # Or raise an error, or return a usage string
            else:
                return await handler(app)
        return None

```

## FILE: src/conduit/apps/chat/ui/async_input.py
```py
from prompt_toolkit.shortcuts import PromptSession


class AsyncInput:
    """
    Asynchronous input handler using prompt_toolkit.
    """

    def __init__(self):
        self.session = PromptSession()

    async def get_input(self) -> str:
        """
        Get input from the user asynchronously.
        """
        return await self.session.prompt_async("> ")

```

## FILE: src/conduit/apps/chat/ui/basic_input.py
```py
from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.apps.chat.ui.ui_command import UICommand
from rich.console import Console, RenderableType
from rich.markdown import Markdown
from typing import override
import re

# Precompile regex pattern to detect Rich style tags
style_pattern = re.compile(r"\[/?[a-zA-Z0-9_ ]+\]")


class BasicInput(InputInterface):
    """
    Minimal input using Rich Console (current implementation)
    """

    def __init__(self, console: Console):
        self.console: Console = console

    @override
    def get_input(self, prompt: str = ">> ") -> str:
        """
        Get user input from console with formatted prompt styling.
        """
        return self.console.input(f"[bold gold3]{prompt}[/bold gold3]")

    @override
    def show_message(self, message: RenderableType, style: str = "") -> None:
        """
        Display a message to the user via Rich console.
        Automatically convert string messages to Markdown for formatted output.
        """
        if isinstance(message, str):
            # If a style is explicitly provided, use it directly
            if style:
                self.console.print(message, style=style)
                return
            # If style tags are detected, print as-is
            if style_pattern.search(message):
                self.console.print(message)
                return
            # This is a plain string, convert to Markdown
            else:
                message = Markdown(message)
                self.console.print(message)
        # This is already a RenderableType (e.g., Markdown, Table, etc.)
        else:
            self.console.print(message)

    @override
    def execute_ui_command(self, command: UICommand) -> None:
        if command == UICommand.CLEAR_SCREEN:
            self.clear_screen()
        elif command == UICommand.CLEAR_HISTORY_FILE:
            self.clear_history_file()
        elif command == UICommand.EXIT:
            self.exit()
        else:
            raise NotImplementedError(
                f"UI command {command} not implemented in BasicInput."
            )

    @override
    def clear_screen(self) -> None:
        self.console.clear()

    @override
    def clear_history_file(self) -> None:
        self.console.print(
            "[yellow]Clearing history file is not supported in BasicInput.[/yellow]"
        )

    @override
    def exit(self) -> None:
        import sys

        sys.exit(0)

```

## FILE: src/conduit/apps/chat/ui/demo_tui.py
```py
from __future__ import annotations
import io
import shutil
import sys
from typing import override, TYPE_CHECKING

from rich.console import Console, RenderableType
from rich.markdown import Markdown

from prompt_toolkit import Application
from prompt_toolkit.buffer import Buffer
from prompt_toolkit.layout.containers import HSplit, Window
from prompt_toolkit.layout.controls import BufferControl, FormattedTextControl
from prompt_toolkit.layout.layout import Layout
from prompt_toolkit.layout.dimension import Dimension
from prompt_toolkit.key_binding import KeyBindings
from prompt_toolkit.formatted_text import ANSI
from prompt_toolkit.widgets import Frame
from prompt_toolkit.styles import Style as PtStyle

from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.apps.chat.ui.ui_command import UICommand

if TYPE_CHECKING:
    from conduit.apps.chat.engine.async_engine import ChatEngine


class DemoInput(InputInterface):
    """
    A TUI-based input interface that splits the screen:
    - Top: Conversation History (Rich rendered)
    - Bottom: Input Buffer
    """

    def __init__(self, console: Console):
        self.console: Console = console
        self.engine: ChatEngine | None = None

        # --- 1. The Output Buffer (Top Pane) ---
        # We use a FormattedTextControl to display raw ANSI strings generated by Rich.
        self.output_control = FormattedTextControl(text="")
        self.output_window = Window(
            content=self.output_control,
            wrap_lines=True,
            always_hide_cursor=True,
            style="class:output-pane",
        )

        # --- 2. The Input Buffer (Bottom Pane) ---
        self.input_buffer = Buffer(multiline=True)  # Enable multiline input (Alt+Enter)
        self.input_control = BufferControl(buffer=self.input_buffer)
        self.input_window = Window(
            content=self.input_control,
            height=Dimension(min=3, max=6),  # Grow slightly if user types a lot
            style="class:input-pane",
            wrap_lines=True,
        )

        # --- 3. Layout ---
        self.root_container = HSplit(
            [
                # Top: Chat History (takes all remaining space)
                Frame(self.output_window, title=" Conduit Chat "),
                # Bottom: Input
                Frame(self.input_window, title=" Input (Alt+Enter for new line) "),
            ]
        )

        # --- 4. Key Bindings ---
        self.kb = KeyBindings()

        @self.kb.add("c-c")
        def _(event):
            "Quit application."
            event.app.exit(result=None)

        @self.kb.add("enter")
        def _(event):
            "Accept input. (If you want multiline by default, swap this with c-enter)"
            # Submit the buffer text and close the app loop temporarily
            text = self.input_buffer.text
            if text.strip():
                event.app.exit(result=text)

        # --- 5. Styles ---
        self.style = PtStyle.from_dict(
            {
                "output-pane": "bg:#000000 #ffffff",
                "input-pane": "",  # inherit default
                "frame.label": "bold #00ff00",
                "frame.border": "#444444",
            }
        )

        # --- 6. The Application Instance ---
        # Note: We don't run it yet. We run it inside get_input().
        self.app = Application(
            layout=Layout(self.root_container, focused_element=self.input_window),
            key_bindings=self.kb,
            style=self.style,
            full_screen=True,
            mouse_support=True,
        )

    def set_engine(self, engine: ChatEngine) -> None:
        """Dependency injection for the engine (to access history)."""
        self.engine = engine

    def _render_conversation_to_ansi(self) -> str:
        """
        Snapshot the current conversation state, render it with Rich to an
        in-memory buffer, and return the ANSI string.
        """
        if not self.engine:
            return ""

        # Create a memory buffer
        buf = io.StringIO()

        # Calculate width (subtracting border padding)
        width = shutil.get_terminal_size().columns - 4

        # Create a ephemeral console for this frame
        render_console = Console(
            file=buf,
            force_terminal=True,  # Crucial: forces ANSI generation
            color_system="truecolor",
            width=width,
            markup=True,
        )

        # Render the whole conversation
        if self.engine.conversation.messages:
            render_console.print(self.engine.conversation)
        else:
            render_console.print("[dim italic]No messages yet...[/dim italic]")

        return buf.getvalue()

    @override
    async def get_input(self, prompt: str = ">> ") -> str:
        """
        1. Refreshes the top pane with latest history.
        2. Clears previous input.
        3. Runs the TUI loop until user hits Enter.
        """

        # Update the view
        ansi_history = self._render_conversation_to_ansi()
        self.output_control.text = ANSI(ansi_history)

        # Clear the input box
        self.input_buffer.reset()

        # Hack to scroll to bottom:
        # Since FormattedTextControl is static, we rely on prompt_toolkit's
        # default layout engine to keep the cursor in view. But since focus
        # is on the Input Window, the Output Window might not scroll.
        # Ideally, we would use a specialized scrolling container here.
        # For this demo, let's see if the natural layout handles it.

        # Run the app!
        # This blocks (asynchronously) until app.exit() is called in keybindings.
        result = await self.app.run_async()

        return result

    @override
    def show_message(self, message: RenderableType, style: str = "info") -> None:
        """
        Fallback for startup messages before the loop starts.
        Once the loop runs, messages are shown via _render_conversation_to_ansi.
        """
        if not self.app.is_running:
            self.console.print(message)

    @override
    def execute_ui_command(self, command: UICommand) -> None:
        # TUI handles clearing and history internally
        pass

    @override
    def clear_screen(self) -> None:
        pass

    @override
    def clear_history_file(self) -> None:
        pass

    @override
    def exit(self) -> None:
        sys.exit(0)

```

## FILE: src/conduit/apps/chat/ui/display.py
```py
from rich.console import Console

console = Console()


def display(message: str) -> None:
    """
    Displays a message to the console.
    """
    console.print(message)

```

## FILE: src/conduit/apps/chat/ui/enhanced_input.py
```py
"""
Our existing REPL application uses a basic input prompt. We want to enhance it with Prompt Toolkit features.
- [ ] 1.0 Spinner
- [x] 1.1 Command History with Persistence
- [x] 1.2 Tab Completion for Commands
- [x] 1.3 Multi-line Input
- [x] 1.4 Basic Key Bindings
- [x] 2.1 Bottom Toolbar
- [x] 2.2 Dynamic Toolbar
    - [x] refreshing data
    - [ ] app state (i.e. model, tokens, etc.)
- [ ] 3.1 Nested Completers
- [ ] 3.2 Fuzzy Command Matching
- [ ] 4.2 History View Keybinding
- [ ] 5.1 Clipboard Integration
- [ ] 5.2 Quick Save
- [ ] 6.1 Simple Expansion
- [ ] 8.1 Split Layout
"""

from __future__ import annotations
from conduit.apps.chat.ui.input_interface import InputInterface
from conduit.apps.chat.ui.ui_command import UICommand
from conduit.apps.chat.ui.keybindings import KeyBindingsRepo
from rich.console import Console, RenderableType
from rich.markdown import Markdown
from typing import override, TYPE_CHECKING
from collections.abc import Iterable
import re
from pathlib import Path
import io
import shutil

from prompt_toolkit import PromptSession
from prompt_toolkit.history import FileHistory
from prompt_toolkit.styles import Style
from prompt_toolkit.completion import Completer, Completion
from prompt_toolkit.document import Document
from prompt_toolkit.filters import Condition
from prompt_toolkit.application import run_in_terminal
from prompt_toolkit.formatted_text import ANSI
from prompt_toolkit.shortcuts import print_formatted_text
from prompt_toolkit.patch_stdout import patch_stdout

from conduit.core.model.models.modelstore import ModelStore

if TYPE_CHECKING:
    from conduit.apps.chat.engine.async_engine import ChatEngine

# Precompile regex pattern (copied from BasicInput)
style_pattern = re.compile(r"\[/?[a-zA-Z0-9_ ]+\]")

# 1.1: Define a path for the persistent history file
HISTORY_FILE = Path.home() / ".conduit_chat_history"


class CommandCompleter(Completer):
    """
    A prompt_toolkit completer that suggests commands from the ChatEngine.
    """

    def __init__(self, engine: ChatEngine):
        self.engine: ChatEngine = engine
        self._all_commands: dict[str, str] | None = None

    def _get_commands(self):
        """Cache command and alias names."""
        if self._all_commands is None:
            self._all_commands = {}
            registered_commands = self.engine.commands
            for name, handler in registered_commands.items():
                # name is like "/help" or "/set model"
                command_name = name[1:]  # remove leading '/'
                description = ""
                if handler.__doc__:
                    # Get the first line of the docstring
                    description = handler.__doc__.strip().split("\n")[0]
                self._all_commands[command_name] = description

    def get_completions(
        self, document: Document, complete_event
    ) -> Iterable[Completion]:
        """
        Yield completion suggestions.
        """
        self._get_commands()
        text = document.text_before_cursor

        if not text.startswith("/"):
            return

        # If user has typed a full command and a space, do not complete.
        if text[1:] in self._all_commands and text.endswith(" "):
            return

        command_text = text[1:]

        # Suggest primary commands
        for name, description in self._all_commands.items():
            if name.startswith(command_text):
                yield Completion(
                    text=f"/{name}",
                    start_position=-len(text),  # Replace the whole thing user typed
                    display=f"/{name}",
                    display_meta=description,
                )


class EnhancedInput(InputInterface, KeyBindingsRepo):
    """
    Enhanced input using Prompt Toolkit for features like persistent history.

    Output strategy ("boring and stable"):
    - While the prompt is active, prompt_toolkit owns terminal output.
    - Rich renderables are rendered to ANSI strings in-memory (no stdout side effects).
    - ANSI strings are printed via prompt_toolkit's safe printing mechanism (run_in_terminal).
    """

    def __init__(self, console: Console):
        self.console: Console = console
        self._model_spec_cache = {}
        self._model_store = ModelStore()

        # We need extra context about commands for tab completion, as well as the model name.
        self.engine: ChatEngine | None = None

        # Track multiline mode state
        self.multiline_mode = False

        # Create key bindings
        self.kb = self._create_key_bindings()

        self.session: PromptSession = PromptSession(
            # Create a session that uses a persistent file history
            history=FileHistory(str(HISTORY_FILE)),
            vi_mode=True,
            # Tab Completion Setup
            completer=None,
            complete_while_typing=True,  # Show suggestions as you type
            # Multi-line Input Setup
            multiline=Condition(lambda: self.multiline_mode),
            prompt_continuation=".. ",  # Optional: Prompt for 2nd+ lines
            # Toolbar
            bottom_toolbar=self.get_toolbar_text,
            # Key bindings
            key_bindings=self.kb,
        )

        # Basic styling for the prompt (approximates BasicInput's gold3)
        self.style = Style.from_dict(
            {
                "prompt": "bold #ffaf00",
                # --- completion menu ---
                "completion-menu.completion": "bg:#005f5f #ffffff",
                "completion-menu.completion.current": "bg:#008787 #ffffff",
                "scrollbar.background": "bg:#005f5f",
                "scrollbar.button": "bg:#008787",
            }
        )

    def _get_model_spec(self, model_name: str):
        if model_name not in self._model_spec_cache:
            self._model_spec_cache[model_name] = self._model_store.get_model(model_name)
        return self._model_spec_cache[model_name]

    def get_toolbar_text(self):
        if not self.engine:
            return [("class:bottom-toolbar", " <Esc>h for keybindings")]

        model_name = self.engine.params.model or "unknown"
        context_length = len(self.engine.conversation.messages)

        try:
            model_spec = self._get_model_spec(model_name)
            context_window = model_spec.context_window if model_spec else 0
        except Exception:
            context_window = 0

        multiline_indicator = " [MULTILINE]" if self.multiline_mode else ""

        return [
            (
                "class:bottom-toolbar",
                f" <Esc>h for keybindings | {model_name} | Context: {context_length}/{context_window}{multiline_indicator} ",
            )
        ]

    def set_engine(self, engine: ChatEngine) -> None:
        """
        Set the engine to enable tab completion.
        This is called by the factory after the engine is created.
        """
        self.engine = engine
        self.session.completer = CommandCompleter(engine)

    async def get_input(self, prompt: str = ">> ") -> str:
        """
        Get user input using Prompt Toolkit.
        Use patch_stdout to prevent stray prints/logging from corrupting the prompt.
        """
        styled_prompt_message = [("class:prompt", prompt)]
        with patch_stdout():
            return await self.session.prompt_async(
                styled_prompt_message, style=self.style
            )

    def _get_render_width(self) -> int:
        """
        Determine a stable width for Rich rendering.

        Prefer prompt_toolkit's notion of terminal width when the app exists,
        otherwise fall back to the current terminal size / rich console width.
        """
        try:
            if self.session.app:
                size = self.session.app.output.get_size()
                if size and getattr(size, "columns", None):
                    return max(20, int(size.columns))
        except Exception:
            pass

        try:
            cols = shutil.get_terminal_size().columns
            return max(20, int(cols))
        except Exception:
            return max(20, int(getattr(self.console, "width", 80)))

    def _render_to_ansi(self, renderable: RenderableType) -> str:
        """
        Render a Rich renderable to an ANSI-escaped string in memory.
        No stdout side effects.
        """
        buf = io.StringIO()
        width = self._get_render_width()

        render_console = Console(
            file=buf,
            force_terminal=True,
            color_system="truecolor",
            width=width,
            markup=True,
            emoji=True,
            highlight=False,
        )
        render_console.print(renderable)
        return buf.getvalue()

    def _print_ansi_safely(self, ansi_text: str) -> None:
        """
        Print ANSI text above the current prompt without corrupting the UI.
        """

        def _do_print() -> None:
            # ANSI() parses escape sequences into prompt_toolkit formatted text.
            print_formatted_text(ANSI(ansi_text), end="")

        run_in_terminal(_do_print)

    @override
    def show_message(self, message: RenderableType, style: str = "info") -> None:
        """
        Display a message without breaking the prompt.

        Behavior:
        - If prompt_toolkit app is running: render (Rich) -> ANSI string -> print safely.
        - Otherwise: fallback to direct Rich printing for startup / non-interactive output.
        """
        if self.session.app:
            # Normalize strings into Rich renderables so we keep styling consistent.
            if isinstance(message, str):
                if style_pattern.search(message):
                    renderable: RenderableType = message  # rich markup string
                else:
                    renderable = Markdown(message)
            else:
                renderable = message

            ansi_text = self._render_to_ansi(renderable)
            self._print_ansi_safely(ansi_text)
            return

        # Fallback for initial messages before prompt_toolkit app is fully running.
        if isinstance(message, str):
            if style_pattern.search(message):
                self.console.print(message)
            else:
                self.console.print(Markdown(message))
        else:
            self.console.print(message)

    # UI commands
    @override
    def execute_ui_command(self, command: UICommand) -> None:
        if command == UICommand.CLEAR_SCREEN:
            self.clear_screen()
        elif command == UICommand.CLEAR_HISTORY_FILE:
            self.clear_history_file()
        elif command == UICommand.EXIT:
            self.exit()
        else:
            raise NotImplementedError(
                f"UI command {command} not implemented in EnhancedInput."
            )

    @override
    def clear_screen(self) -> None:
        """
        Clear the screen.
        """
        self.console.clear()

    @override
    def clear_history_file(self) -> None:
        """
        Clear the persistent history file
        """
        try:
            HISTORY_FILE.unlink()
            self.show_message(f"[green]Cleared history file at {HISTORY_FILE}.[/green]")
        except FileNotFoundError:
            self.show_message(
                f"[yellow]No history file found at {HISTORY_FILE}.[/yellow]"
            )

        # Recreate empty history file
        HISTORY_FILE.touch()

    @override
    def exit(self) -> None:
        """
        Exit the application
        """
        import sys

        sys.exit(0)

```

## FILE: src/conduit/apps/chat/ui/input_interface.py
```py
from abc import ABC, abstractmethod
from conduit.apps.chat.ui.ui_command import UICommand
from rich.console import RenderableType


class InputInterface(ABC):
    """
    Abstract interface for user input
    """

    @abstractmethod
    async def get_input(self, prompt: str = ">> ") -> str:
        """
        Get user input. May be single or multi-line.
        """
        pass

    @abstractmethod
    def show_message(self, message: RenderableType, style: str = "info") -> None:
        """
        Display message to user
        """
        pass

    # UI commands
    @abstractmethod
    def execute_ui_command(self, command: UICommand) -> None:
        """
        Execute a UI command
        """
        pass

    @abstractmethod
    def clear_screen(self) -> None:
        """
        Clear the screen
        """
        pass

    @abstractmethod
    def clear_history_file(self) -> None:
        """
        Clear the persistent history file
        """
        pass

    @abstractmethod
    def exit(self) -> None:
        """
        Exit the application
        """
        pass

```

## FILE: src/conduit/apps/chat/ui/keybindings.py
```py
"""
Mixin class to manage keybindings for EnhancedInput class.
"""

from prompt_toolkit.key_binding import KeyBindings


class KeyBindingsRepo:
    """
    Mixin providing Escape-key keybindings for EnhancedInput terminal UI.

    IMPORTANT OUTPUT RULE:
    In EnhancedInput mode, do not call self.console.print() directly from keybindings.
    Route all output through self.show_message() so prompt_toolkit can print safely.
    """

    def _create_key_bindings(self) -> KeyBindings:
        kb = KeyBindings()

        # <Esc>d → exit
        @kb.add("escape", "d")
        def exit_app(event):
            """
            Exit the application
            """
            self.exit()

        # <Esc>n → wipe message history (new chat)
        @kb.add("escape", "n")
        def new_chat(event):
            """
            Start a new chat (wipe history)
            """
            if self.engine:
                self.engine.conversation.wipe()
                self.show_message("[green]Message history cleared.[/green]")

        # <Esc>h → show keybindings help
        @kb.add("escape", "h")
        def show_keybindings(event):
            """
            Show available keybindings
            """
            from rich.table import Table

            table = Table(
                show_header=True, header_style="bold cyan", title="Keybindings"
            )
            table.add_column("Key", style="green")
            table.add_column("Action", style="yellow")

            table.add_row("<Esc>d", "Exit application")
            table.add_row("<Esc>n", "New chat (wipe history)")
            table.add_row("<Esc>h", "Show this help")
            table.add_row("<Esc>m", "Show model card")
            table.add_row("<Esc><Enter>", "Toggle multiline mode")

            self.show_message(table)

        # <Esc>m → show model card
        @kb.add("escape", "m")
        def show_model_card(event):
            """
            Display current model information
            """
            if not self.engine:
                self.show_message("[yellow]Engine not ready yet.[/yellow]")
                return

            from conduit.core.model.models.modelstore import ModelStore

            ms = ModelStore()
            model_spec = ms.get_model(self.engine.params.model)

            # Best effort: show something useful.
            # If model_spec.card is a rich renderable or string, show_message can handle it.
            try:
                card = getattr(model_spec, "card", None)
                if card:
                    self.show_message(card)
                else:
                    self.show_message(f"[cyan]Model:[/cyan] {self.engine.params.model}")
            except Exception as e:
                self.show_message(f"[red]Error showing model card: {e}[/red]")

        # <Esc><Enter> → toggle multiline mode
        @kb.add("escape", "enter")
        def toggle_multiline(event):
            """
            Toggle multiline input mode
            """
            self.multiline_mode = not self.multiline_mode
            mode_status = "enabled" if self.multiline_mode else "disabled"
            self.show_message(f"[cyan]Multiline mode {mode_status}[/cyan]")

        return kb

```

## FILE: src/conduit/apps/chat/ui/logo.py
```py
def get_logo() -> str:
    """
    Return the app logo as Rich markup so it can be printed safely via EnhancedInput.show_message.
    """
    return (
        "[blue]\n"
        " ██████╗ ██████╗ ███╗   ██╗██████╗ ██╗   ██╗██╗████████╗\n"
        "██╔════╝██╔═══██╗████╗  ██║██╔══██╗██║   ██║██║╚══██╔══╝\n"
        "██║     ██║   ██║██╔██╗ ██║██║  ██║██║   ██║██║   ██║   \n"
        "██║     ██║   ██║██║╚██╗██║██║  ██║██║   ██║██║   ██║   \n"
        "╚██████╗╚██████╔╝██║ ╚████║██████╔╝╚██████╔╝██║   ██║   \n"
        " ╚═════╝ ╚═════╝ ╚═╝  ╚═══╝╚═════╝  ╚═════╝ ╚═╝   ╚═╝   \n"
        "[/blue]"
    )


def print_logo() -> None:
    """
    Backwards-compatible: print logo directly.
    Prefer get_logo() + input_interface.show_message(get_logo()) in enhanced mode.
    """
    print(get_logo().replace("[blue]", "\033[94m").replace("[/blue]", "\033[0m"))

```

## FILE: src/conduit/apps/chat/ui/ui_command.py
```py
from enum import Enum, auto


class UICommand(Enum):
    """
    Enumeration of UI commands for the chat interface.

    Defines high-level UI operations that can be triggered by commands, distinct from
    the command execution layer. These represent semantic actions that modify UI state
    or behavior rather than direct command implementations.
    """

    CLEAR_SCREEN = auto()
    CLEAR_HISTORY_FILE = auto()
    EXIT = auto()

```

## FILE: src/conduit/apps/cli/cli_class.py
```py
"""
ConduitCLI is our conduit library as a CLI application.

Customize the query_function to specialize for various prompts / workflows while retaining archival and other functionalities.

To customize:
1. Define your own query function matching the QueryFunctionProtocol signature.
2. Pass your custom function to the ConduitCLI class upon instantiation. NOTE: all Conduit configs are namespaced in the query function inputs. (and any other handlers you define)
3. You can also pass other click options or commands to further customize the CLI behavior.

This allows you to tailor the behavior of ConduitCLI while leveraging its existing features.
"""

from __future__ import annotations
from conduit.config import settings
from conduit.apps.cli.query.query_function import (
    CLIQueryFunctionProtocol,
    default_query_function,
)
from conduit.storage.repository.protocol import ConversationRepository
from conduit.apps.cli.commands.commands import CommandCollection
from conduit.apps.cli.utils.printer import Printer
from functools import cached_property
import sys
import click
import logging
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.domain.conversation.conversation import Conversation


logger = logging.getLogger(__name__)

# Defaults
DEFAULT_PROJECT_NAME = "conduit_cli"
DEFAULT_DESCRIPTION = "Conduit: The LLM CLI"
DEFAULT_QUERY_FUNCTION = default_query_function
PREFERRED_MODEL = settings.preferred_model
DEFAULT_SYSTEM_MESSAGE = settings.system_prompt


class ConduitCLI:
    """
    Main class for the Conduit CLI application.
    Combines argument parsing, configuration loading, and command handling.
    Attributes:
        project_name (str): Name of the CLI application.
        description (str): Description of the CLI application.
        query_function (CLIQueryFunctionProtocol): Function to handle queries.
        verbosity (Verbosity): Verbosity level for LLM responses.
        cache (bool): Whether to use caching for LLM responses.
        persistent (bool): Whether to persist history and settings.
        system_message (str | None): System message for LLM context.
        preferred_model (str): Preferred LLM model to use.
    """

    def __init__(
        self,
        project_name: str = DEFAULT_PROJECT_NAME,
        description: str = DEFAULT_DESCRIPTION,
        query_function: CLIQueryFunctionProtocol = DEFAULT_QUERY_FUNCTION,
        model: str = PREFERRED_MODEL,
        system_message: str = DEFAULT_SYSTEM_MESSAGE,
        version: str = settings.version,
    ):
        # Parameters
        self.project_name: str = project_name
        self.description: str = description
        self.query_function: CLIQueryFunctionProtocol = query_function
        self.version: str = version
        self.preferred_model: str = model
        self.system_message: str = system_message
        # Components
        self.printer: Printer = Printer()
        self.cli: click.Group = self._build_cli()

    @cached_property
    def repository(self) -> ConversationRepository:
        """
        Load the conversation repository.
        Returns:
            ConversationRepository: The loaded repository.
        """
        from dbclients.clients.postgres import get_postgres_client
        from conduit.storage.repository.postgres_repository import (
            PostgresConversationRepository,
        )

        conn_factory = get_postgres_client("context_db", dbname="conduit")

        repository = PostgresConversationRepository(
            conn_factory=conn_factory, project_name=self.project_name
        )
        return repository

    @cached_property
    def conversation(self) -> Conversation:
        """
        Load the last conversation or create a new one.
        Returns:
            Conversation: The loaded or new conversation.
        """
        last_conversation = self.repository.last
        if last_conversation is not None:
            return last_conversation
        else:
            from conduit.domain.conversation.conversation import Conversation

            return Conversation()

    def _build_cli(self) -> click.Group:
        stdin = self._get_stdin()
        printer = self.printer
        version_string: str = self.version

        # invoke_without_command=True allows us to handle --version
        # without triggering a "Missing command" error
        @click.group(invoke_without_command=True)
        @click.option("--version", "show_version", is_flag=True)
        @click.option("--raw", is_flag=True)
        @click.pass_context
        def cli(ctx, show_version, raw):
            ctx.ensure_object(dict)
            # Dependency Injection
            ctx.obj["project_name"] = self.project_name
            ctx.obj["stdin"] = stdin
            ctx.obj["printer"] = printer
            ctx.obj["repository"] = lambda: self.repository  # Lazy load
            ctx.obj["conversation"] = lambda: self.conversation  # Lazy load
            ctx.obj["query_function"] = self.query_function
            ctx.obj["preferred_model"] = self.preferred_model
            ctx.obj["system_message"] = self.system_message
            ctx.obj["verbosity"] = settings.default_verbosity

            # Global Handler: Raw Mode
            if raw:
                printer.set_raw(True)

            # Global Handler: Version
            if show_version:
                click.echo(version_string)
                ctx.exit()

            # Strict Logic: If no subcommand is provided, just show help.
            # We no longer guess that the user meant to query.
            if ctx.invoked_subcommand is None:
                click.echo(ctx.get_help())

        return cli

    def attach(self, command_collection: CommandCollection) -> None:
        """
        Attach a set of commands to the CLI.
        Args:
            command_set (click.Group): The set of commands to attach.
        """
        _ = command_collection.attach(self.cli)

    def run(self) -> None:
        self.cli()

    def _get_stdin(self) -> str:
        """
        Get implicit context from clipboard or other sources.
        """
        context = sys.stdin.read() if not sys.stdin.isatty() else ""
        return context

```

## FILE: src/conduit/apps/cli/commands/base_commands.py
```py
"""
The Command layer defines Click commands and routes to handlers.
Click context is passed to handlers via ctx. Click context should not leak outside of this layer.
If context needs to be edited (like with conversation state management), it should be done here.
"""

from __future__ import annotations
import click
from conduit.apps.cli.commands.commands import CommandCollection
from conduit.apps.cli.handlers.base_handlers import BaseHandlers
from typing import override, TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.utils.progress.verbosity import Verbosity
    from conduit.domain.conversation.conversation import Conversation
    from conduit.storage.repository.protocol import ConversationRepository
    from conduit.apps.cli.utils.printer import Printer

handlers = BaseHandlers()


class BaseCommands(CommandCollection):
    """
    Attachable command collection for Conduit CLI.
    Inject onto any Click group via `attach(group)`.
    """

    def __init__(self):
        self._commands: list[click.Command] = []
        self._register_commands()

    @override
    def _register_commands(self):
        """Define all the base commands as bound methods."""

        @click.command(no_args_is_help=True)
        @click.option(
            "-m", "--model", type=str, help="Specify the model to use.", default=None
        )
        @click.option("-L", "--local", is_flag=True, help="Use local HeadwaterServer.")
        @click.option("-r", "--raw", is_flag=True, help="Print raw output.")
        @click.option("-t", "--temperature", type=float, help="Temperature (0.0-1.0).")
        @click.option(
            "-c", "--chat", is_flag=True, help="Enable chat mode with history."
        )
        @click.option(
            "-a",
            "--append",
            type=str,
            help="Append to query after stdin.",
            default=None,
        )
        @click.argument("query_input", nargs=-1)
        @click.pass_context
        def query(
            ctx: click.Context,
            model: str | None,
            local: bool,
            raw: bool,
            temperature: float | None,
            chat: bool,
            append: str | None,
            query_input: tuple[str, ...],
        ):
            """
            Execute a query against the LLM.

            Input can be passed as arguments or piped via stdin.

            Examples:
                conduit query "Why is the sky blue?"
                conduit query Explain quantum computing --model gpt-4
                cat file.py | conduit query "Refactor this code"
            """
            # 1. Unpack Dependencies from Context
            # The command layer is responsible for knowing WHERE things live (ctx.obj)
            printer = ctx.obj["printer"]
            query_function = ctx.obj["query_function"]
            verbosity = ctx.obj["verbosity"]

            # 2. Extract Config / State
            stdin = ctx.obj.get("stdin")
            system_message = ctx.obj.get("system_message")
            project_name = ctx.obj.get("project_name")
            preferred_model = ctx.obj.get("preferred_model")

            # 3. Resolve Logic (Boundary Responsibility)
            # Determine the final model string here, so the handler is deterministic
            resolved_model = model or preferred_model or "gpt-4o"

            # Smudge query arguments
            query_input_str = " ".join(query_input).strip()

            # 4. Delegate to Handler
            handlers.handle_query(
                query_input=query_input_str,
                model=resolved_model,
                local=local,
                raw=raw,
                temperature=temperature,
                chat=chat,
                append=append,
                # Injected Dependencies
                printer=printer,
                query_function=query_function,
                stdin=stdin,
                system_message=system_message,
                verbosity=verbosity,
                project_name=project_name,
            )

        @click.command()
        @click.pass_context
        def history(ctx: click.Context):
            """View message history."""
            repository: ConversationRepository = ctx.obj["repository"]()  # Lazy load
            conversation: Conversation = ctx.obj["conversation"]()  # Lazy load
            conversation_id: str = conversation.conversation_id
            printer: Printer = ctx.obj["printer"]

            handlers.handle_history(repository, conversation_id, printer)

        @click.command()
        @click.pass_context
        def wipe(ctx: click.Context):
            """Wipe message history."""
            repository: ConversationRepository = ctx.obj["repository"]()  # Lazy load
            conversation = ctx.obj["conversation"]()  # Lazy load
            conversation_id: str = conversation.conversation_id
            printer: Printer = ctx.obj["printer"]

            handlers.handle_wipe(printer, repository, conversation_id)
            # Reset conversation in context
            from conduit.domain.conversation.conversation import Conversation

            ctx.obj["conversation"] = Conversation()
            repository.save(ctx.obj["conversation"], name="Untitled")

        @click.command()
        @click.pass_context
        def ping(ctx: click.Context):
            """Ping the Headwater server."""
            printer: Printer = ctx.obj["printer"]

            handlers.handle_ping(printer)

        @click.command()
        @click.pass_context
        def status(ctx: click.Context):
            """Get Headwater server status."""
            printer: Printer = ctx.obj["printer"]

            handlers.handle_status(printer)

        @click.command()
        @click.pass_context
        def shell(ctx: click.Context):
            """Enter interactive shell mode."""
            raise NotImplementedError

        @click.command()
        @click.pass_context
        def last(ctx: click.Context):
            """Get the last message."""
            conversation: Conversation = ctx.obj["conversation"]()
            printer: Printer = ctx.obj["printer"]

            handlers.handle_last(printer, conversation)

        @click.command()
        @click.pass_context
        @click.argument("index", type=int)
        def get(ctx: click.Context, index: int):
            """Get a specific message from history."""
            conversation: Conversation = ctx.obj["conversation"]
            printer: Printer = ctx.obj["printer"]

            handlers.handle_get(index, conversation, printer)

        @click.command()
        @click.pass_context
        def config(ctx: click.Context):
            """View current configuration."""
            printer: Printer = ctx.obj["printer"]
            preferred_model: str = ctx.obj["preferred_model"]
            system_message: str = ctx.obj["system_message"]
            chat: bool = ctx.obj["chat"]
            verbosity: Verbosity = ctx.obj["verbosity"]

            handlers.handle_config(
                printer,
                preferred_model,
                system_message,
                chat,
                verbosity,
            )

        self._commands = [
            query,
            history,
            wipe,
            ping,
            status,
            shell,
            last,
            get,
            config,
        ]

    @override
    def attach(self, group: click.Group) -> click.Group:
        """Attach all commands to a Click group."""
        for cmd in self._commands:
            group.add_command(cmd)
        return group

```

## FILE: src/conduit/apps/cli/commands/commands.py
```py
from __future__ import annotations
from typing import Protocol, TYPE_CHECKING

if TYPE_CHECKING:
    import click


class CommandCollection(Protocol):
    def _register_commands(self) -> None:
        """Register CLI commands."""
        ...

    def attach(self, group: click.Group) -> click.Group:
        """Attach commands to a Click group."""
        ...

```

## FILE: src/conduit/apps/cli/handlers/base_handlers.py
```py
"""
This is a static class containing CLI command handlers for the Conduit application.
- all methods must have the signature
- if the config file specifies a type for a command or flag, it must be one of: str, int, float, bool, and the method must handle that type accordingly
- the method names must match the handler names in the config file exactly, a la "handle_history", "handle_wipe", etc.
"""

from __future__ import annotations
from conduit.config import settings
from typing import TYPE_CHECKING
import logging
import sys

if TYPE_CHECKING:
    from conduit.utils.progress.verbosity import Verbosity
    from conduit.domain.conversation.conversation import Conversation
    from conduit.apps.cli.utils.printer import Printer
    from conduit.domain.message.message import UserMessage, Message
    from conduit.storage.repository.protocol import ConversationRepository
    from uuid import UUID

logger = logging.getLogger(__name__)

# Constants
DEFAULT_VERBOSITY = settings.default_verbosity


class BaseHandlers:
    @staticmethod
    def grab_image_from_clipboard(printer: Printer) -> tuple[str, str] | None:
        """
        Attempt to grab image from clipboard; return tuple of mime_type and base64.
        """
        logger.info("Attempting to grab image from clipboard...")
        import os

        if "SSH_CLIENT" in os.environ or "SSH_TTY" in os.environ:
            printer.print_pretty("Image paste not available over SSH.", style="red")
            return

        import warnings
        from PIL import ImageGrab
        import base64
        import io

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")  # Suppress PIL warnings
            image = ImageGrab.grabclipboard()

        if image:
            buffer = io.BytesIO()
            image.save(buffer, format="PNG")
            img_base64 = base64.b64encode(buffer.getvalue()).decode()
            # Save for next query
            printer.print_pretty("Image captured!", style="green")
            # Build our ImageMessage
            image_content = img_base64
            mime_type = "image/png"
            return mime_type, image_content
        else:
            printer.print_pretty("No image detected.", style="red")

            sys.exit()

    @staticmethod
    def create_image_message(
        combined_query: str, mime_type: str, image_content: str
    ) -> UserMessage | None:
        logger.info("Creating image message...")
        if not image_content or not mime_type:
            return

        from conduit.domain.message.message import (
            UserMessage,
            ImageContent,
            TextContent,
        )

        text_content_obj = TextContent(text=combined_query)

        image_content_obj = ImageContent(
            url=f"data:{mime_type};base64,{image_content}",
            detail="auto",
        )

        content = [text_content_obj, image_content_obj]

        imagemessage = UserMessage(
            content=content,
        )
        return imagemessage

    # Handlers
    @staticmethod
    def handle_history(
        repository: ConversationRepository,
        conversation_id: str | UUID,
        printer: Printer,
    ) -> None:
        """
        View message history and exit.
        """
        logger.info("Viewing message history...")
        conversation = repository.load_by_conversation_id(conversation_id)
        if not conversation:
            raise ValueError("Conversation not found.")
        conversation.print_history()
        sys.exit()

    @staticmethod
    def handle_wipe(
        printer: Printer,
        repository: ConversationRepository,
        conversation_id: str,
    ):
        """
        Clear the message history after user confirmation.
        """
        logger.info("Wiping message history...")
        from rich.prompt import Confirm

        confirm = Confirm.ask(
            "[red]Are you sure you want to wipe the message history? This action cannot be undone.[/red]",
            default=False,
        )
        if confirm:
            repository.remove_by_conversation_id(conversation_id)
            printer.print_pretty("[green]Message history wiped.[/green]")
        else:
            printer.print_pretty("[yellow]Wipe cancelled.[/yellow]")

    @staticmethod
    def handle_shell():
        pass

    @staticmethod
    def handle_ping(printer: Printer):
        from headwater_client.client.headwater_client import HeadwaterClient

        hc = HeadwaterClient()
        response = hc.ping()
        if response == True:
            response = "Pong!"
            printer.print_pretty(f"[green]{response}[/green]")
        else:
            response = "No response."
            printer.print_pretty(f"[red]{response}[/red]")

    @staticmethod
    def handle_status(printer: Printer):
        from headwater_client.client.headwater_client import HeadwaterClient

        hc = HeadwaterClient()
        status = hc.get_status()
        printer.print_pretty(status)

    @staticmethod
    def handle_last(
        printer: Printer,
        conversation: Conversation,
    ):
        """
        Print the last message in the message store and exit.
        """
        logger.info("Viewing last message...")
        import sys

        # Get last message
        last_message: Message = conversation.last
        # If no messages, inform user
        if not last_message:
            printer.print_pretty("[red]No messages in history.[/red]")
            sys.exit()
        # Print last message
        printer.print_markdown(str(last_message))
        sys.exit()

    @staticmethod
    def handle_get(index: int, conversation: Conversation, printer: Printer):
        """
        Print a specific message from history and exit.
        """
        logger.info(f"Viewing message at index {index}...")
        import sys

        messages = conversation.messages
        # Validate index
        if index < 0 or index >= len(messages):
            printer.print_pretty("[red]Invalid message index.[/red]")
            sys.exit()
        # Get message
        message: Message = messages[index]
        # Print message
        printer.print_markdown(str(message))
        sys.exit()

    @staticmethod
    def handle_config(
        printer: Printer,
        preferred_model: str,
        system_message: str,
        chat: bool,
        verbosity: Verbosity,
    ):
        """
        Print the current configuration and exit.
        """
        logger.info("Viewing configuration...")
        config_md = f"""
# Current Configuration
| Setting | Value |
|---------|-------|
| Preferred Model | {preferred_model} |
| System Message | {system_message[:50]}... |
| Message History | {"Enabled" if chat else "Disabled"} |
| Verbosity | {verbosity} |
"""
        printer.print_markdown(config_md)

    @staticmethod
    def handle_query(
        query_input: str,
        model: str,
        local: bool,
        raw: bool,
        temperature: float | None,
        chat: bool,
        append: str | None,
        verbosity: Verbosity,
        # Injected Dependencies
        printer: Printer,
        query_function: CLIQueryFunctionProtocol,
        stdin: str | None,
        system_message: str = "",
        project_name: str = "",
    ) -> None:
        """
        Here we resolve all inputs for flat input to the query function.
        Dependencies are injected explicitly, removing the Click context coupling.
        """
        from conduit.apps.cli.query.query_function import CLIQueryFunctionInputs

        # 1. Normalize Context (stdin)
        # Handle cases where stdin might be an empty string or whitespace
        context_text = stdin if isinstance(stdin, str) and stdin.strip() else ""

        # 2. Build Inputs
        inputs = CLIQueryFunctionInputs(
            query_input=query_input,
            printer=printer,
            context=context_text,
            append=append or "",
            system_message=system_message,
            project_name=project_name,
            cache=not local,
            local=local,
            preferred_model=model,  # Model is already resolved by the Command layer
            verbose=verbosity,
            include_history=chat,
            temperature=temperature,
        )

        # 3. Execute
        response = query_function(inputs)

        # 4. Display
        if raw:
            printer.print_raw(response.content)
        else:
            printer.print_markdown(response.content)

```

## FILE: src/conduit/apps/cli/hollow_backend.py
```py
import asyncio
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Protocol
from collections.abc import Callable
from uuid import uuid4

# --- 1. Core Domain Models (The "What") ---


class Role(Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"


@dataclass
class Event:
    """An atomic item in the transcript (Spec 1.1)."""

    id: str = field(default_factory=lambda: str(uuid4()))
    role: Role = Role.USER
    content: str = ""

    # Branching metadata (Spec 1.3)
    branch_id: int = 0
    total_branches: int = 1

    # Properties for rendering
    is_expandable: bool = False
    is_collapsed: bool = False


@dataclass
class Conversation:
    """Immutable record of the event stream (Spec 1.2)."""

    id: str
    events: list[Event]


# --- 2. Application State (The "Truth") ---


class AppMode(Enum):
    """(Spec 2.0)"""

    NORMAL = auto()  # Navigation
    INSERT = auto()  # Typing
    COMMAND = auto()  # ':' overlay


@dataclass
class UIState:
    """Transient state for the TUI (Spec 10.12)."""

    mode: AppMode = AppMode.NORMAL

    # Navigation
    selected_event_id: str | None = None

    # Input Buffers
    main_input_buffer: str = ""
    command_buffer: str = ""

    # Visual flags
    show_help: bool = False
    is_streaming: bool = False


# --- 3. The Controller Interface (The "How") ---


class ControllerProtocol(Protocol):
    """
    The strict boundary between UI and Engine.
    The UI calls these methods; these methods mutate State.
    """

    state: UIState
    conversation: Conversation

    def register_redraw_callback(self, cb: Callable[[], None]):
        """So the engine can tell the UI to repaint (Spec 6.2)."""
        ...

    # -- Actions --

    def set_mode(self, mode: AppMode): ...

    def move_selection(self, direction: int):
        """Move up/down (-1/+1). Constraints: bounds check."""
        ...

    def toggle_expand(self):
        """Expand/collapse selected event."""
        ...

    def switch_branch(self, direction: int):
        """Cycle horizontal branches for selected event."""
        ...

    async def submit_input(self, text: str):
        """
        1. Commit user message.
        2. Start async assistant generation (streaming).
        """
        ...


# --- 4. The Mock Implementation (For Vibe Coding) ---


class MockController:
    """
    A functional mock to drive the TUI during development.
    Simulates streaming and branching.
    """

    def __init__(self):
        self.redraw_cb = lambda: None

        # Seed some fake data
        e1 = Event(role=Role.USER, content="Hello, system.")
        e2 = Event(role=Role.ASSISTANT, content="Greetings. Ready.", total_branches=2)

        self.conversation = Conversation(id="conv_1", events=[e1, e2])
        self.state = UIState(selected_event_id=e2.id)

    def register_redraw_callback(self, cb):
        self.redraw_cb = cb

    def _update(self):
        self.redraw_cb()

    def set_mode(self, mode: AppMode):
        self.state.mode = mode
        self._update()

    def move_selection(self, direction: int):
        # (Mock logic to find current index and move up/down)
        ids = [e.id for e in self.conversation.events]
        try:
            curr_idx = ids.index(self.state.selected_event_id)
            new_idx = max(0, min(len(ids) - 1, curr_idx + direction))
            self.state.selected_event_id = ids[new_idx]
            self._update()
        except ValueError:
            pass

    async def submit_input(self, text: str):
        # 1. Add User Message
        user_msg = Event(role=Role.USER, content=text)
        self.conversation.events.append(user_msg)
        self.state.selected_event_id = user_msg.id
        self.state.main_input_buffer = ""  # Clear input
        self.set_mode(AppMode.NORMAL)
        self._update()

        # 2. Simulate Assistant Streaming
        asst_msg = Event(role=Role.ASSISTANT, content="")
        self.conversation.events.append(asst_msg)
        self.state.selected_event_id = asst_msg.id
        self.state.is_streaming = True

        # Fake Stream Loop
        full_response = "This is a simulated streaming response based on your input."
        for word in full_response.split():
            await asyncio.sleep(0.1)  # Fake network lag
            asst_msg.content += word + " "
            self._update()  # Trigger UI redraw

        self.state.is_streaming = False
        self._update()

```

## FILE: src/conduit/apps/cli/query/query_function.py
```py
"""
MAJOR REFACTOR INCOMING:
- should this use Model or Conduit?
- handle persistence?
Our default query function -- passed into ConduitCLI as a dependency.
Define your own for customization.
"""

from conduit.config import settings
from conduit.core.prompt.prompt import Prompt
from conduit.domain.result.response import GenerationResponse
from conduit.utils.progress.verbosity import Verbosity
from conduit.apps.cli.utils.printer import Printer
from dataclasses import dataclass
from typing import Protocol, runtime_checkable
import logging

logger = logging.getLogger(__name__)


@dataclass
class CLIQueryFunctionInputs:
    """
    This has defaults, besides query_input, which is required.
    All query functions need to accept this object.
    Best to handle all possible inputs, so that CLI acts as user expects.
    If not using a particular input, consider logging a warning to inform the user.
    """

    # Project
    project_name: str
    # Prompt inputs
    query_input: str
    printer: Printer
    context: str = ""
    append: str = ""
    system_message: str = ""
    # Configs
    temperature: float | None = None
    cache: bool = True
    local: bool = False
    preferred_model: str = settings.preferred_model
    verbose: Verbosity = Verbosity.PROGRESS
    include_history: bool = True  # whether to include conversation history in messages
    ephemeral: bool = False  # whether to avoid persisting this conversation


# Our protocol
@runtime_checkable
class CLIQueryFunctionProtocol(Protocol):
    """
    Protocol for a query function. Customized query functions should match this signature.
    """

    def __call__(
        self,
        inputs: CLIQueryFunctionInputs,
    ) -> GenerationResponse: ...


# Now, our default implementation -- the beauty of LLMs with POSIX philosophy
def default_query_function(
    inputs: CLIQueryFunctionInputs,
) -> GenerationResponse:
    """
    Default query function.
    """
    logger.debug("Running default_query_function...")
    # Extract inputs from dict
    project_name = inputs.project_name
    query_input: str = inputs.query_input
    context: str = inputs.context
    append: str = inputs.append
    local: bool = inputs.local
    preferred_model: str = inputs.preferred_model
    verbose: Verbosity = inputs.verbose
    include_history: bool = inputs.include_history
    ephemeral: bool = False
    cache = inputs.cache
    system = inputs.system_message

    # ConduitCLI's default POSIX philosophy: embrace pipes and redirection
    combined_query = "\n\n".join([query_input, context, append]).strip()
    logger.info("Combined query prepared.")

    from conduit.core.conduit.conduit_sync import ConduitSync

    prompt = Prompt(combined_query)
    conduit = ConduitSync.create(
        project_name=project_name,
        model=preferred_model,
        prompt=prompt,
        system=system,
        cache=cache,
        persist=not ephemeral,
        verbose=verbose,
        debug_payload=False,  # Change to True to debug payloads
        include_history=include_history,
        use_remote=local,
    )
    logger.info(f"Using model: {preferred_model}")
    response = conduit()
    return response

```

## FILE: src/conduit/apps/cli/utils/printer.py
```py
"""
Module for displaying data and UI elements conditionally based on TTY status.
- If stdout is a TTY: show Rich UI to stderr, suppress data unless --raw
- If stdout is piped/redirected: emit data to stdout, suppress UI
This mirrors best practice for POSIX-friendly CLIs.
"""

from __future__ import annotations
import sys
from contextlib import nullcontext
from signal import signal, SIGPIPE, SIG_DFL
from rich.console import Console
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from rich.console import RenderableType

# Treat broken pipes cleanly (avoid stack traces in pipelines)
_ = signal(SIGPIPE, SIG_DFL)

IS_TTY = sys.stdout.isatty()


class Printer:
    def __init__(self, raw: bool = False):
        """
        Initialize IO policy based on TTY status and raw flag.
        """
        self.emit_data = (not IS_TTY) or raw  # pipe/redirect OR --raw
        self.emit_ui = IS_TTY and (not raw)  # bare terminal AND not --raw
        self.ui = Console(file=sys.stderr) if self.emit_ui else None
        self._write = sys.stdout.write

    def set_raw(self, raw: bool):
        """
        Update IO policy based on raw flag.
        """
        self.emit_data = (not IS_TTY) or raw
        self.emit_ui = IS_TTY and (not raw)
        self.ui = Console(file=sys.stderr) if self.emit_ui else None

    def print_raw(self, s: str = ""):
        """
        Data stream for piping/redirecting (stdout).
        """
        if self.emit_data:
            self._write(s)
            if s and not s.endswith("\n"):
                self._write("\n")

    def print_pretty(self, *args, **kwargs) -> None:
        """
        Human-facing UI (stderr via Rich).
        """
        if self.ui:
            self.ui.print(*args, **kwargs)

    def status(self, *args, **kwargs):
        """
        Context manager for spinners/status messages.
        Disabled when UI is off.
        """
        if self.ui:
            return self.ui.status(*args, **kwargs)
        return nullcontext()

    def print_markdown(
        self, markdown_string: str | RenderableType, add_rule: bool = True
    ):
        """
        Unified Markdown printer:
        - If piping/redirecting (emit_data): write plain Markdown to stdout.
        - If TTY UI (emit_ui): render via Rich Markdown on stderr.
        """
        if self.emit_data:
            self.print_raw(markdown_string)
            return
        if self.ui:
            from rich.markdown import Markdown

            if isinstance(markdown_string, str):
                md = markdown_string
                if add_rule:
                    border = "-" * 100
                    md = f"{border}\n{markdown_string}\n\n{border}"
                self.ui.print(Markdown(md))
            else:
                self.ui.print(markdown_string)  # assume renderable

```

## FILE: src/conduit/apps/scripts/chat_cli.py
```py
import asyncio
import logging
import os

from rich.console import Console

from conduit.config import settings
from conduit.apps.chat.create_app import create_chat_app
from conduit.apps.chat.ui.async_input import AsyncInput
from conduit.domain.config.conduit_options import ConduitOptions

# Set up logging
log_level = int(os.getenv("PYTHON_LOG_LEVEL", "1"))
levels = {1: logging.WARNING, 2: logging.INFO, 3: logging.DEBUG}
logging.basicConfig(
    level=levels.get(log_level, logging.INFO), format="%(levelname)s: %(message)s"
)
logger = logging.getLogger(__name__)

# Constants
CONSOLE = Console()
INPUT_INTERFACE = AsyncInput()
PREFERRED_MODEL = settings.preferred_model
WELCOME_MESSAGE = "[bold cyan]Conduit Chat. Type /exit to exit.[/bold cyan]"
SYSTEM_MESSAGE = settings.system_prompt
VERBOSITY = settings.default_verbosity

OPTIONS = ConduitOptions(project_name="conduit-chat", verbosity=VERBOSITY, console=CONSOLE)


async def async_main():
    """
    Initializes and runs the asynchronous chat application.
    """
    app = create_chat_app(
        input_interface=INPUT_INTERFACE,
        preferred_model=PREFERRED_MODEL,
        welcome_message=WELCOME_MESSAGE,
        system_message=SYSTEM_MESSAGE,
        options=OPTIONS,
    )
    
    await app.run()

def main():
    """
    Synchronous entry point to run the async main function.
    """
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nExiting...")


if __name__ == "__main__":
    main()
```

## FILE: src/conduit/apps/scripts/conduit_cli.py
```py
from conduit.apps.cli.commands.base_commands import BaseCommands
from conduit.apps.cli.cli_class import ConduitCLI
import sys


def query_entrypoint():
    """
    Shortcut entry point for 'ask'.
    Takes us directly to "conduit query ..."
    """

    # Safety check: don't inject if the user (weirdly) typed 'ask query'
    if len(sys.argv) > 1 and sys.argv[1] == "query":
        pass
    else:
        sys.argv.insert(1, "query")

    # Now run the main app exactly as normal
    main()


def main():
    conduit_cli = ConduitCLI()
    commands = BaseCommands()
    conduit_cli.attach(commands)
    conduit_cli.run()


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/fchat_cli.py
```py
"""
fchat = "fancy chat".
This script launches the chat application with the enhanced (prompt_toolkit) input interface.
"""

import asyncio
import logging
import os

from rich.console import Console

from conduit.config import settings
from conduit.apps.chat.create_app import create_chat_app
from conduit.domain.config.conduit_options import ConduitOptions

# Set up logging
log_level = int(os.getenv("PYTHON_LOG_LEVEL", "1"))
levels = {1: logging.WARNING, 2: logging.INFO, 3: logging.DEBUG}
logging.basicConfig(
    level=levels.get(log_level, logging.INFO), format="%(levelname)s: %(message)s"
)
logger = logging.getLogger(__name__)

# Constants
CONSOLE = Console()
PREFERRED_MODEL = settings.preferred_model
WELCOME_MESSAGE = "[bold cyan]Conduit Chat. Type /exit to exit.[/bold cyan]"
SYSTEM_MESSAGE = settings.system_prompt
VERBOSITY = settings.default_verbosity

OPTIONS = ConduitOptions(
    project_name="conduit-fchat",
    verbosity=VERBOSITY,
    console=CONSOLE,
    cache=settings.default_cache("conduit-fchat"),
)


async def async_main():
    """
    Initializes and runs the asynchronous chat application.
    """
    app = create_chat_app(
        input_mode="enhanced",
        preferred_model=PREFERRED_MODEL,
        welcome_message=WELCOME_MESSAGE,
        system_message=SYSTEM_MESSAGE,
        options=OPTIONS,
    )

    await app.run()


def main():
    """
    Synchronous entry point to run the async main function.
    """
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        print("\nExiting...")


if __name__ == "__main__":
    main()


```

## FILE: src/conduit/apps/scripts/imagegen_cli.py
```py
import os
from io import BytesIO
import google.generativeai as genai
from PIL import Image
import argparse
from conduit.domain.message.imagemessage import ImageMessage
import base64


def generate_image(prompt: str) -> ImageMessage:
    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

    # model = genai.GenerativeModel("gemini-2.5-flash-image")
    # image = model.generate_content(prompt)
    # save_image_from_response(image, "generated_image.png")
    # text_response, image_response = image.parts
    # text_response = text_response.text

    model = genai.GenerativeModel("gemini-3-pro-image-preview")

    image = model.generate_content(prompt)
    save_image_from_response(image, "generated_image.png")

    image_response = image.parts
    text_response = ""

    # convert image_response.inline_data.data from bytes to base64 string
    base64_image_data = base64.b64encode(image_response.inline_data.data).decode(
        "utf-8"
    )
    image_message = ImageMessage(
        role="assistant",
        text_content=text_response,
        image_content=base64_image_data,
    )
    return image_message


def save_image_from_response(response, filename):
    for c in getattr(response, "candidates", []):
        for p in getattr(c.content, "parts", []):
            if getattr(p, "inline_data", None):
                img = Image.open(BytesIO(p.inline_data.data))
                img.save(filename)
                return filename
    raise RuntimeError("No image returned")


def main():
    parser = argparse.ArgumentParser(description="Generate an image from a prompt.")
    parser.add_argument(
        "prompt", type=str, help="The prompt to generate the image from."
    )
    args = parser.parse_args()
    image_msg = generate_image(args.prompt)
    image_msg.display()


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/models_cli.py
```py
import argparse
from collections import namedtuple
from conduit.core.model.models.modelstore import ModelStore

models = ModelStore.list_models()
modeltypes = ModelStore.list_model_types()
providers = ModelStore.list_providers()

Match = namedtuple("Match", ["title", "score", "rank"])


def fuzzy_search(query: str, limit: int = 3):
    from rapidfuzz import process, fuzz

    choices = models
    results = process.extract(query, choices, scorer=fuzz.WRatio, limit=limit)
    matches = [
        Match(title=title, score=score, rank=rank + 1)
        for rank, (title, score, _) in enumerate(results)
    ]
    return matches


def main():
    parser = argparse.ArgumentParser(description="CLI for managing models.")
    parser.add_argument(
        "-m", "--model", type=str, help="Name of the model to retrieve details for."
    )
    parser.add_argument(
        "-t", "--type", type=str, help="Type of the model to filter by."
    )
    parser.add_argument(
        "-p", "--provider", type=str, help="Provider of the model to filter by."
    )
    args = parser.parse_args()
    # Validate arguments
    if args.type:
        if args.type not in modeltypes:
            raise ValueError(
                f"Invalid model type: {args.type}. Must be one of: {' | '.join(modeltypes)}."
            )
    if args.provider:
        if args.provider not in providers:
            raise ValueError(
                f"Invalid provider: {args.provider}. Must be one of: {' | '.join(providers)}."
            )
    if args.model:
        model_string = ModelStore().validate_model
        if not model_string:
            raise ValueError(
                f"Model {args.model} not found. Available models: {', '.join(models)}."
            )
    # Run commands
    if args.model:
        try:
            modelspec = ModelStore.get_model(args.model)
            modelspec.card
        except ValueError:
            matches = fuzzy_search(args.model)
            from rich.console import Console

            console = Console()
            console.print(f"[red]Model '{args.model}' not found. Did you mean:[/red]")
            for match in matches:
                console.print(f"  {match.rank}. {match.title}")
    elif args.type:
        modelspecs = ModelStore.by_type(args.type)
        for model in modelspecs:
            print(model.model)
    elif args.provider:
        modelspecs = ModelStore.by_provider(args.provider)
        for model in modelspecs:
            print(model.model)
    else:
        ModelStore.display()


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/snapshot.py
```py
#!/usr/bin/env python3
"""
Odometer Snapshot - Shows usage statistics from the persistent odometer

Usage:
    python snapshot.py
"""

from conduit.storage.odometer.database.pgres.PostgresBackend import PostgresBackend
from datetime import date
from rich.console import Console
from rich.table import Table
import sys


def format_large_number(num):
    """Format large numbers with commas"""
    return f"{num:,}"


def print_usage_stats(console, backend):
    """Print usage statistics as a single line"""
    stats = backend.get_overall_stats()

    if not stats:
        console.print("[red]No usage data found[/red]")
        return

    console.print("[bold gold3]Usage Statistics[/bold gold3]")
    console.print(
        f"[cyan]Requests:[/cyan] {format_large_number(stats['requests'])}    "
        f"[cyan]Tokens:[/cyan] {format_large_number(stats['total_tokens'])}    "
        f"[cyan]Input:[/cyan] [green]{format_large_number(stats['input'])}[/green]    "
        f"[cyan]Output:[/cyan] [yellow]{format_large_number(stats['output'])}[/yellow]    "
        f"[cyan]Providers:[/cyan] {stats['providers']}    "
        f"[cyan]Models:[/cyan] {stats['models']}"
    )


def print_provider_table(console, backend):
    """Print clean provider table"""
    provider_stats = backend.get_aggregates("provider")

    if not provider_stats:
        return

    console.print(f"\n[bold gold3]Usage by Provider[/bold gold3]")

    table = Table(show_header=True, header_style="bold", box=None, padding=(0, 1))
    table.add_column("Provider", style="cyan")
    table.add_column("Requests", justify="right")
    table.add_column("Input", justify="right", style="green")
    table.add_column("Output", justify="right", style="yellow")
    table.add_column("Total", justify="right", style="bold")

    sorted_providers = sorted(
        provider_stats.items(), key=lambda x: x[1]["total"], reverse=True
    )

    for provider, stats in sorted_providers:
        table.add_row(
            provider,
            format_large_number(stats["events"]),
            format_large_number(stats["input"]),
            format_large_number(stats["output"]),
            format_large_number(stats["total"]),
        )

    console.print(table)


def print_models_table(console, model_stats, limit=10, title="Top 10 Models"):
    """Print clean top models table"""
    if not model_stats:
        return

    console.print(f"\n[bold gold3]{title}[/bold gold3]")

    table = Table(show_header=True, header_style="bold", box=None, padding=(0, 1))
    table.add_column("Rank", justify="center", width=4)
    table.add_column("Model", style="cyan")
    table.add_column("Requests", justify="right")
    table.add_column("Total Tokens", justify="right", style="bold")

    sorted_models = sorted(
        model_stats.items(), key=lambda x: x[1]["total"], reverse=True
    )[:limit]

    for i, (model, stats) in enumerate(sorted_models, 1):
        display_model = model[:35] + "..." if len(model) > 38 else model
        rank_text = "🥇" if i == 1 else "🥈" if i == 2 else "🥉" if i == 3 else str(i)

        table.add_row(
            rank_text,
            display_model,
            format_large_number(stats["events"]),
            format_large_number(stats["total"]),
        )

    console.print(table)


def main():
    """Main function to generate the snapshot"""
    console = Console()

    try:
        with console.status("Connecting...", spinner="dots"):
            backend = PostgresBackend()

            if not backend.health_check():
                console.print("❌ [red]Database connection failed[/red]")
                sys.exit(1)

        print_usage_stats(console, backend)
        print_provider_table(console, backend)

        # All-time models
        all_time_stats = backend.get_aggregates("model")
        print_models_table(console, all_time_stats)

        # Today's models
        today = date.today()
        daily_stats = backend.get_aggregates("model", start_date=today, end_date=today)
        if daily_stats:
            print_models_table(console, daily_stats, title="Top 10 Models (Today)")
        else:
            console.print(f"\n[bold gold3]No usage today[/bold gold3]")

    except KeyboardInterrupt:
        console.print("\n[yellow]Cancelled[/yellow]")
        sys.exit(0)
    except Exception as e:
        console.print(f"❌ [red]Error: {e}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/tokenize_cli.py
```py
import argparse


def main():
    parser = argparse.ArgumentParser(description="Tokenize input text.")
    parser.add_argument("text", type=str, help="The text to be tokenized.")
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default="gpt",
        help="The model to use for tokenization.",
    )
    args = parser.parse_args()

    from conduit.sync import Model

    model = Model(args.model)
    tokens: int = model.tokenize(args.text)
    print(f"Number of tokens: {tokens}")


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/tokens_cli.py
```py
#!/usr/bin/env python3
"""
Odometer Snapshot - Shows usage statistics from the persistent odometer

Usage:
    python snapshot.py
"""

import asyncio
import sys
from datetime import date
from rich.console import Console
from rich.table import Table
from conduit.storage.odometer.pgres.postgres_backend_async import AsyncPostgresOdometer


def format_large_number(num):
    """Format large numbers with commas"""
    if num is None:
        return "0"
    return f"{num:,}"


def print_usage_stats(console, stats):
    """Print usage statistics as a single line"""
    if not stats or stats.get("requests", 0) == 0:
        console.print("[red]No usage data found[/red]")
        return

    console.print("[bold gold3]Usage Statistics[/bold gold3]")
    console.print(
        f"[cyan]Requests:[/cyan] {format_large_number(stats['requests'])}    "
        f"[cyan]Tokens:[/cyan] {format_large_number(stats['total_tokens'])}    "
        f"[cyan]Input:[/cyan] [green]{format_large_number(stats['input'])}[/green]    "
        f"[cyan]Output:[/cyan] [yellow]{format_large_number(stats['output'])}[/yellow]    "
        f"[cyan]Providers:[/cyan] {stats['providers']}    "
        f"[cyan]Models:[/cyan] {stats['models']}"
    )


def print_provider_table(console, provider_stats):
    """Print clean provider table"""
    if not provider_stats:
        return

    console.print(f"\n[bold gold3]Usage by Provider[/bold gold3]")

    table = Table(show_header=True, header_style="bold", box=None, padding=(0, 1))
    table.add_column("Provider", style="cyan")
    table.add_column("Requests", justify="right")
    table.add_column("Input", justify="right", style="green")
    table.add_column("Output", justify="right", style="yellow")
    table.add_column("Total", justify="right", style="bold")

    sorted_providers = sorted(
        provider_stats.items(), key=lambda x: x[1]["total"], reverse=True
    )

    for provider, stats in sorted_providers:
        table.add_row(
            provider,
            format_large_number(stats["events"]),
            format_large_number(stats["input"]),
            format_large_number(stats["output"]),
            format_large_number(stats["total"]),
        )

    console.print(table)


def print_models_table(console, model_stats, limit=10, title="Top 10 Models"):
    """Print clean top models table"""
    if not model_stats:
        return

    console.print(f"\n[bold gold3]{title}[/bold gold3]")

    table = Table(show_header=True, header_style="bold", box=None, padding=(0, 1))
    table.add_column("Rank", justify="center", width=4)
    table.add_column("Model", style="cyan")
    table.add_column("Requests", justify="right")
    table.add_column("Total Tokens", justify="right", style="bold")

    sorted_models = sorted(
        model_stats.items(), key=lambda x: x[1]["total"], reverse=True
    )[:limit]

    for i, (model, stats) in enumerate(sorted_models, 1):
        display_model = model[:35] + "..." if len(model) > 38 else model
        rank_text = "🥇" if i == 1 else "🥈" if i == 2 else "🥉" if i == 3 else str(i)

        table.add_row(
            rank_text,
            display_model,
            format_large_number(stats["events"]),
            format_large_number(stats["total"]),
        )

    console.print(table)


async def async_main():
    """Main function to generate the snapshot"""
    console = Console()

    try:
        with console.status("Connecting...", spinner="dots"):
            backend = AsyncPostgresOdometer()

            # Fetch all data concurrently
            overall_task = backend.get_overall_stats()
            provider_task = backend.get_aggregates("provider")
            all_time_task = backend.get_aggregates("model")

            today = date.today()
            daily_task = backend.get_aggregates(
                "model", start_date=today, end_date=today
            )

            # Await all results
            stats = await overall_task
            provider_stats = await provider_task
            all_time_stats = await all_time_task
            daily_stats = await daily_task

        print_usage_stats(console, stats)
        print_provider_table(console, provider_stats)
        print_models_table(console, all_time_stats)

        if daily_stats:
            print_models_table(console, daily_stats, title="Top 10 Models (Today)")
        else:
            console.print(f"\n[bold gold3]No usage today[/bold gold3]")

    except KeyboardInterrupt:
        console.print("\n[yellow]Cancelled[/yellow]")
    except Exception as e:
        console.print(f"❌ [red]Error: {e}[/red]")
        # import traceback
        # traceback.print_exc()
        sys.exit(1)


def main():
    asyncio.run(async_main())


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/update_modelstore.py
```py
from conduit.core.model.models.modelstore import ModelStore


def main():
    modelstore = ModelStore()
    modelstore.update()


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/apps/scripts/update_ollama_list.py
```py
"""
This script updates the list of Ollama models in the Conduit module.
Use this when switching environments, ssh tunnels, or when new models are added.
Need to figure out where to automatically implement this in my Conduit package to avoid manual updates but also preserve lazy loading.
"""

from conduit.core.model.clients.ollama_client import OllamaClientSync
from conduit.core.model.models.modelstore import ModelStore
from rich import console
import logging

logger = logging.getLogger(__name__)
console = console.Console(width=80)


def main():
    console.print("[green]Updating Ollama Models...[/green]")
    client = OllamaClientSync()
    client.update_ollama_models()
    console.print(
        f"[green]Model list updated: [/green][yellow]{ModelStore.models()['ollama']}[/yellow]"
    )


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/async_.py
```py
# Orchestration classes
from conduit.core.conduit.conduit_async import ConduitAsync
from conduit.core.model.model_async import ModelAsync
from conduit.core.prompt.prompt import Prompt

# Primitives: dataclasses / enums
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.request.request import GenerationRequest
from conduit.domain.result.response import GenerationResponse

# Configs
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

__all__ = [
    "ConduitAsync",
    "ConduitOptions",
    "GenerationParams",
    "GenerationRequest",
    "GenerationResponse",
    "ModelAsync",
    "Prompt",
    "Verbosity",
]

```

## FILE: src/conduit/batch.py
```py
# Orchestration classes
from conduit.core.conduit.batch.conduit_batch_async import ConduitBatchAsync
from conduit.core.conduit.batch.conduit_batch_sync import ConduitBatchSync
from conduit.core.prompt.prompt import Prompt

# Primitives: dataclasses / enums
from conduit.domain.result.response import GenerationResponse
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.request.request import GenerationRequest

# Configs
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

__all__ = [
    "ConduitBatchAsync",
    "ConduitBatchSync",
    "ConduitOptions",
    "GenerationParams",
    "GenerationRequest",
    "GenerationResponse",
    "Prompt",
    "Verbosity",
]

```

## FILE: src/conduit/capabilities/skills/format.py
```py
from typing import Literal


def format_skill_content(
    skill_type: Literal["skill", "context", "prompt"], name: str, body: str
) -> str:
    if skill_type == "prompt":
        # Overrides the persona/behavior.
        # Injected as a System Instruction or high-priority User Message.
        return f"""
<procedure_active>
SYSTEM INSTRUCTION: PROCEDURE LOADED ({name})
You must now adopt the following reasoning protocol. 
Abandon previous default behaviors and strictly follow these steps:

{body}
</procedure_active>
"""

    elif skill_type == "context":
        # Grounds the model in facts/constraints.
        # Injected as a User Message or Context Block.
        return f"""
<context_active>
SYSTEM NOTE: CONTEXT LOADED ({name})
Use the following information as the ground truth for your next responses.
Do not treat this as an instruction to act, but as constraints to think within.

{body}
</context_active>
"""

    else:
        # type == "skill" (Action/Tool)
        # Just shows the documentation/manual for the tool.
        return f"""
<skill>
Documentation for {name}:

{body}
</skill>
"""

```

## FILE: src/conduit/capabilities/skills/parse.py
```py
def parse_skill(file_content: str) -> tuple[dict[str, str], str]:
    """
    Parses a string with YAML frontmatter delimited by '---'.
    Raises ValueError if format is invalid or YAML is malformed.
    """
    import yaml

    # Split by '---' max 2 times
    # Expected: [0] empty, [1] yaml, [2] body
    parts = file_content.split("---", 2)

    # 1. Validation: Must start with --- and have a closing ---
    if len(parts) < 3 or parts[0].strip() != "":
        raise ValueError(
            "Invalid Skill file format: Missing '---' frontmatter delimiters."
        )

    yaml_text = parts[1]
    body_text = parts[2].strip()

    # 2. Validation: YAML must be parseable
    try:
        metadata = yaml.safe_load(yaml_text)
    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML in Skill frontmatter: {e}")

    # 3. Validation: YAML must not be empty or non-dict
    if not isinstance(metadata, dict):
        raise ValueError("Skill frontmatter must contain valid key-value pairs.")

    return metadata, body_text

```

## FILE: src/conduit/capabilities/skills/prompts/system_prompt_template.jinja2
```jinja2
<rules>
{{system_prompt}}
</rules>

<skills_protocol>
You have access to a library of specialized skills that grant you additional tools and context.

CRITICAL RULE: When the user asks for a task that matches a skill description below, you MUST use the `enable_skill` tool to activate that skill before proceeding.

<available_skills>
{% for skill in skills %}
  <skill>
      <name>{{ skill.name }}</name>
      <description>{{ skill.description }}</description>
  </skill>
{% endfor %}
</available_skills>

Once a skill is loaded, you will receive confirmation and new tools will become available in your registry.
</skills_protocol>

```

## FILE: src/conduit/capabilities/skills/registry.py
```py
from __future__ import annotations
from pathlib import Path
from conduit.config import settings
from conduit.capabilities.skills.skill import Skill


class SkillRegistry:
    def __init__(self):
        self._skills: dict[str, Skill] = {}

    @property
    def skills_dir(self) -> Path:
        from conduit.config import settings

        return settings.paths["SKILLS_DIR"]

    def register(self, skill: Skill):
        self._skills[skill.name] = skill

    def get_skill(self, name: str) -> Skill:
        return self._skills[name]

    def list_skills(self) -> list[str]:
        return list(self._skills.keys())

    def all_skills(self) -> list[Skill]:
        return list(self._skills.values())

    @classmethod
    def from_skills_dir(
        cls, skills_dir: Path = settings.paths["SKILLS_DIR"]
    ) -> SkillRegistry:
        """
        Creates a SkillRegistry by loading all skill files from the specified directory.
        """
        registry = cls()
        # The skills directory has a directory for each skill; the SKILL.md file inside contains the skill definition.
        for skill_dir in skills_dir.iterdir():
            if skill_dir.is_dir():
                skill_file = skill_dir / "SKILL.md"
                if skill_file.exists():
                    skill = Skill.from_path(skill_file)
                    registry.register(skill)
        return registry

```

## FILE: src/conduit/capabilities/skills/skill.py
```py
from __future__ import annotations
from typing import Literal, TYPE_CHECKING
from pydantic import BaseModel, Field
from conduit.capabilities.skills.parse import parse_skill
from conduit.capabilities.skills.format import format_skill_content

if TYPE_CHECKING:
    from pathlib import Path


class Skill(BaseModel):
    """
    Represents a skill, context, or prompt to be used by an AI model.
    """

    # The YAML header
    type: Literal["skill", "context", "prompt"] = Field(
        ...,
        description="The type of the skill: 'skill' for actions/tools, 'context' for grounding information, 'prompt' for behavior overrides.",
    )
    name: str = Field(..., description="The name of the skill.")
    description: str = Field(..., description="A brief description of the skill.")
    # The actual content -- should be in markdown format
    body: str = Field(..., description="The content/body of the skill.")

    def render(self) -> str:
        """
        Renders the skill content based on its type.
        """
        return format_skill_content(self.type, self.name, self.body)

    @classmethod
    def from_path(cls, path: Path) -> Skill:
        """
        Load a Skill from a markdown file with YAML front matter.
        """

        # Validate path exists and is a file
        if not path.exists() or not path.is_file():
            raise FileNotFoundError(f"Skill file not found: {path}")
        # Read file content
        content = path.read_text(encoding="utf-8")
        # Parse frontmatter and body
        metadata, body = parse_skill(content)
        # Create Skill instance
        return cls(
            type=metadata["type"],
            name=metadata["name"],
            description=metadata["description"],
            body=body,
        )

```

## FILE: src/conduit/capabilities/skills/system.py
```py
from __future__ import annotations
from conduit.config import settings
from pathlib import Path
from jinja2 import Template
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.capabilities.skills.registry import SkillRegistry

SYSTEM_PROMPT_PATH = Path(__file__).parent / "prompts" / "system_prompt_template.jinja2"
PROMPT_STR = SYSTEM_PROMPT_PATH.read_text()


def generate_skills_system_prompt(
    registry: SkillRegistry, system_prompt: str = settings.system_prompt
) -> str:
    """Generate the system prompt with the available skills.

    Args:
        registry (SkillRegistry): The skill registry.
        system_prompt (str, optional): The base system prompt. Defaults to settings.system_prompt.

    Returns:
        str: The generated system prompt.
    """
    template = Template(PROMPT_STR)
    skills = registry.all_skills()
    return template.render(system_prompt=system_prompt, skills=skills)

```

## FILE: src/conduit/capabilities/skills/tool.py
```py
from __future__ import annotations
from typing import Annotated, TYPE_CHECKING
from conduit.capabilities.tools.tool import Tool, ObjectSchema, Property

if TYPE_CHECKING:
    from conduit.capabilities.skills.registry import SkillRegistry
    from conduit.capabilities.tools.registry import ToolRegistry


async def enable_skill(
    skill_name: Annotated[str, "The name of the skill to enable"],
    _skill_registry: SkillRegistry,
    _tool_registry: ToolRegistry,
) -> str:
    """
    Enables a specified skill. This loads the skill body into the conversation, and may update your available tools.
    """
    try:
        skill = _skill_registry.get_skill(skill_name)
        return skill.render()

    except KeyError:
        return f"Skill '{skill_name}' not found."


# We need to roll a special Tool instance
## First our one Property (skill_name, NOT the registry args)
skill_name_property = Property(
    type="string",
    description="The name of the skill to enable",
)
## Now our ObjectSchema
object_schema = ObjectSchema(
    properties={
        "skill_name": skill_name_property,
    },
    required=["skill_name"],
)
## Finally our Tool
enable_skill_tool = Tool(
    name="enable_skill",
    description=enable_skill.__doc__.strip(),
    input_schema=object_schema,
    func=enable_skill,
)

__all__ = ["enable_skill_tool"]

```

## FILE: src/conduit/capabilities/tools/registry.py
```py
from __future__ import annotations
from conduit.capabilities.tools.tool import Tool, ToolCallError
from conduit.capabilities.tools.tool_function import ToolFunction
from conduit.domain.message.message import ToolCall
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.capabilities.skills.registry import SkillRegistry


class ToolRegistry:
    def __init__(self) -> None:
        self._tools: dict[str, Tool] = {}
        self._skill_registry: SkillRegistry = None

    def register(self, tool: Tool) -> None:
        if tool.name in self._tools:
            raise ValueError(f"Tool with name '{tool.name}' is already registered.")
        self._tools[tool.name] = tool

    def get_tool(self, name: str) -> Tool:
        if name not in self._tools:
            raise KeyError(f"Tool with name '{name}' is not registered.")
        return self._tools[name]

    async def call_tool(self, tool_call: ToolCall) -> str:
        tool = self.get_tool(tool_call.function_name)

        # Create a copy of arguments to run the function so we don't pollute
        # the message history with non-serializable objects (like registries).
        func_args = tool_call.arguments.copy()

        # Inject registries if calling the ensure_skill tool
        if tool.name == "enable_skill" and self._skill_registry is not None:
            func_args["_skill_registry"] = self._skill_registry
            func_args["_tool_registry"] = self

        try:
            result = await tool.func(**func_args)
        except Exception as e:
            raise ToolCallError(f"Error calling tool '{tool.name}': {e}") from e
        return result if isinstance(result, str) else str(result)

    def list_tools(self) -> list[str]:
        return list(self._tools.keys())

    @property
    def tools(self) -> list[Tool]:
        return list(self._tools.values())

    def register_function(self, func: ToolFunction) -> None:
        tool = Tool.from_function(func)
        self.register(tool)

    def register_functions(self, funcs: list[ToolFunction]) -> None:
        for func in funcs:
            self.register_function(func)

    def enable_skills(self, skill_registry: SkillRegistry):
        """
        Injects skill-enabling tools into the registry, as well as adding the ensure_skill tool.
        """
        from conduit.capabilities.skills.tool import enable_skill_tool

        self._skill_registry = skill_registry
        self.register(enable_skill_tool)

```

## FILE: src/conduit/capabilities/tools/tool.py
```py
from __future__ import annotations
import inspect
from typing import Annotated, Any, Literal, get_args, get_origin, TYPE_CHECKING
from pydantic import BaseModel, Field, ConfigDict
from conduit.capabilities.tools.tool_function import ToolFunction
from conduit.capabilities.tools.tool_function import validate_tool_function

if TYPE_CHECKING:
    from conduit.capabilities.tools.registry import ToolRegistry


JsonType = Literal["string", "number", "boolean", "integer", "object", "array"]


class ToolCallError(Exception):
    """Custom exception for ToolCall errors."""


class Property(BaseModel):
    type: JsonType
    description: str | None = None
    enum: list[str] | None = None
    items: Property | None = None  # only for arrays


class ObjectSchema(BaseModel):
    type: Literal["object"] = "object"
    properties: dict[str, Property]
    required: list[str] = []
    additional_properties: bool = Field(default=False, alias="additionalProperties")


def _clean_doc(doc: str | None) -> str:
    if not doc:
        return ""
    return inspect.cleandoc(doc).strip()


def _is_optional(tp: Any) -> bool:
    origin = get_origin(tp)
    if origin is None:
        return False
    if origin is type(None):  # noqa: E721
        return True
    if origin is getattr(__import__("typing"), "Union"):
        return any(a is type(None) for a in get_args(tp))  # noqa: E721
    return False


def _strip_optional(tp: Any) -> Any:
    origin = get_origin(tp)
    if origin is getattr(__import__("typing"), "Union"):
        args = tuple(a for a in get_args(tp) if a is not type(None))  # noqa: E721
        if len(args) == 1:
            return args[0]
        return origin[args]  # type: ignore[index]
    return tp


def _extract_annotated(tp: Any) -> tuple[Any, str]:
    if get_origin(tp) is not Annotated:
        raise TypeError("Parameter annotation must be typing.Annotated[...]")
    base, *meta = get_args(tp)
    desc = next((m.strip() for m in meta if isinstance(m, str) and m.strip()), None)
    if desc is None:
        raise TypeError(
            "Annotated parameter must include a non-empty string description."
        )
    return base, desc


def _python_type_to_property(tp: Any) -> Property:
    """
    Minimal mapping from Python type annotations to canonical JSON types.
    Assumes `tp` is the *base type* (i.e., Annotated already unwrapped).
    """
    if tp is inspect._empty:
        return Property(type="string")

    if _is_optional(tp):
        tp = _strip_optional(tp)

    origin = get_origin(tp)

    if origin in (list, tuple):
        args = get_args(tp)
        item_tp = args[0] if args else Any
        return Property(type="array", items=_python_type_to_property(item_tp))

    if origin is dict:
        return Property(type="object")

    if tp is str:
        return Property(type="string")
    if tp is int:
        return Property(type="integer")
    if tp is float:
        return Property(type="number")
    if tp is bool:
        return Property(type="boolean")

    # Fallback for unknown / complex types (including nested models)
    return Property(type="object")


class Tool(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    type: Literal["function"] = "function"
    name: str
    description: str
    input_schema: ObjectSchema

    # Callable attribute
    func: ToolFunction = Field(exclude=True)

    # Factory method
    @classmethod
    def from_function(cls, func: ToolFunction) -> Tool:
        """
        Generate a Tool from a function.

        Strict requirements are enforced by validate_tool_function:
        - stable name required
        - docstring required
        - Annotated required for each parameter (with per-param description)
        """
        # Validate the function first
        errors = validate_tool_function(func)
        if errors:
            lines = []
            for e in errors:
                tail = f" {e.details}" if getattr(e, "details", None) else ""
                lines.append(f"- {e.code}: {e.message}{tail}")
            raise TypeError("Invalid ToolFunction:\n" + "\n".join(lines))

        sig = inspect.signature(func)

        properties: dict[str, Property] = {}
        required: list[str] = []

        for param_name, param in sig.parameters.items():
            if param.kind in (
                inspect.Parameter.VAR_POSITIONAL,
                inspect.Parameter.VAR_KEYWORD,
            ):
                raise TypeError(
                    f"Tool functions cannot use *args/**kwargs: {param_name}"
                )

            base_tp, desc = _extract_annotated(param.annotation)
            prop = _python_type_to_property(base_tp)
            prop.description = desc
            properties[param_name] = prop

            is_required = (param.default is inspect._empty) and (
                not _is_optional(base_tp)
            )
            if is_required:
                required.append(param_name)

        schema = ObjectSchema(
            properties=properties,
            required=required,
            additional_properties=False,
        )

        description = _clean_doc(getattr(func, "__doc__", None))
        if not description:
            raise TypeError("ToolFunction docstring must be non-empty.")

        name = getattr(func, "__name__", "")
        if not name or name == "<lambda>":
            raise TypeError(
                "ToolFunction name must be stable and non-empty (lambdas not allowed)."
            )

        return cls(
            name=name,
            description=description,
            input_schema=schema,
            func=func,
        )

    def register(self, registry: ToolRegistry):
        registry.register(self)

```

## FILE: src/conduit/capabilities/tools/tool_function.py
```py
from __future__ import annotations

import inspect
from dataclasses import dataclass
from collections.abc import Awaitable, Callable
from typing import Any, Annotated, Protocol, get_args, get_origin, runtime_checkable


@runtime_checkable
class ToolFunction(Protocol):
    """
    A function eligible for Tool generation.

    Hard requirements:
    - Async (not sync!)
    - Stable name (not a lambda; has __name__)
    - Non-empty docstring
    - No *args or **kwargs
    - Every parameter MUST be typing.Annotated[T, <non-empty str description>, ...]
      (Type must be present as the first Annotated arg; description must be a str metadata item.)

    Best practices (not enforced):
    - Use standard types (str, int, float, bool, list, dict)
    - Return type should render as meaningful json (so a dict or BaseModel)
    """

    __name__: str
    __doc__: str

    def __call__(self, *args: Any, **kwargs: Any) -> Any | Awaitable[Any]: ...


@dataclass(frozen=True)
class ToolFunctionError:
    code: str
    message: str
    details: dict[str, Any] | None = None


def _clean_doc(doc: str | None) -> str:
    if not doc:
        return ""
    return inspect.cleandoc(doc).strip()


def _is_annotated(tp: Any) -> bool:
    return get_origin(tp) is Annotated


def _extract_annotated(tp: Any) -> tuple[Any, list[Any]]:
    """
    Returns (base_type, metadata_list). If not Annotated, base_type is tp and metadata is [].
    """
    if get_origin(tp) is Annotated:
        base, *meta = get_args(tp)
        return base, list(meta)
    return tp, []


def _extract_description(meta: list[Any]) -> str | None:
    """
    Canonical rule: first non-empty string metadata item is the description.
    """
    for m in meta:
        if isinstance(m, str) and m.strip():
            return m.strip()
    return None


def validate_tool_function(func: Callable[..., Any]) -> list[ToolFunctionError]:
    errors: list[ToolFunctionError] = []

    # Must be introspectable
    try:
        sig = inspect.signature(func)
    except (TypeError, ValueError):
        return [
            ToolFunctionError(
                "not_introspectable", "Object has no inspectable signature."
            )
        ]

    # Stable name required
    name = getattr(func, "__name__", "")
    if not name or name == "<lambda>":
        errors.append(
            ToolFunctionError(
                code="unstable_name",
                message="Function must have a stable __name__ (lambdas are not allowed).",
                details={"name": name},
            )
        )

    # Docstring required
    doc = _clean_doc(getattr(func, "__doc__", None))
    if not doc:
        errors.append(
            ToolFunctionError(
                code="missing_docstring",
                message="Function must have a non-empty docstring.",
            )
        )

    # Parameters: no *args/**kwargs; every param must be Annotated with a description
    for param_name, param in sig.parameters.items():
        if param.kind is inspect.Parameter.VAR_POSITIONAL:
            errors.append(
                ToolFunctionError(
                    code="var_positional_not_supported",
                    message="*args is not supported for tool functions.",
                    details={"param": param_name},
                )
            )
            continue

        if param.kind is inspect.Parameter.VAR_KEYWORD:
            errors.append(
                ToolFunctionError(
                    code="var_keyword_not_supported",
                    message="**kwargs is not supported for tool functions.",
                    details={"param": param_name},
                )
            )
            continue

        ann = param.annotation
        if ann is inspect._empty:
            errors.append(
                ToolFunctionError(
                    code="missing_annotation",
                    message="Every parameter must be annotated using typing.Annotated[...].",
                    details={"param": param_name},
                )
            )
            continue

        if not _is_annotated(ann):
            errors.append(
                ToolFunctionError(
                    code="annotated_required",
                    message="Every parameter must use typing.Annotated[T, 'description', ...].",
                    details={"param": param_name, "annotation": repr(ann)},
                )
            )
            continue

        base_tp, meta = _extract_annotated(ann)

        # Base type sanity (must exist)
        if base_tp is None or base_tp is inspect._empty:
            errors.append(
                ToolFunctionError(
                    code="annotated_missing_base_type",
                    message="Annotated parameter must include a base type as the first argument.",
                    details={"param": param_name},
                )
            )

        # Description required
        desc = _extract_description(meta)
        if not desc:
            errors.append(
                ToolFunctionError(
                    code="missing_param_description",
                    message="Annotated parameter must include a non-empty string description.",
                    details={"param": param_name},
                )
            )

    return errors


def assert_tool_function(func: Callable[..., Any]) -> ToolFunction:
    errs = validate_tool_function(func)
    if errs:
        lines: list[str] = []
        for e in errs:
            tail = f" {e.details}" if e.details else ""
            lines.append(f"- {e.code}: {e.message}{tail}")
        raise TypeError("Invalid ToolFunction:\n" + "\n".join(lines))
    return func  # type: ignore[return-value]

```

## FILE: src/conduit/capabilities/tools/tools/fetch.py
```py
from conduit.domain.exceptions.exceptions import ToolError
from typing import Annotated


def extract_content_from_html(html: str) -> str:
    """Extract and convert HTML content to Markdown format.

    Args:
        html: Raw HTML content to process

    Returns:
        Simplified markdown version of the content
    """
    import markdownify
    import readabilipy

    ret = readabilipy.simple_json.simple_json_from_html_string(
        html, use_readability=True
    )
    if not ret["content"]:
        return "<error>Page failed to be simplified from HTML</error>"
    content = markdownify.markdownify(
        ret["content"],
        heading_style=markdownify.ATX,
    )
    return content


async def fetch_url(url: Annotated[str, "The URL to fetch"]) -> dict[str, str]:
    """
    Fetch the URL and return the content in a form ready for the LLM, as well as a prefix string with status information.
    """
    from httpx import AsyncClient, HTTPError

    async with AsyncClient() as client:
        try:
            response = await client.get(
                url,
                follow_redirects=True,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
                },
                timeout=30,
            )
        except HTTPError as e:
            raise ToolError(f"Failed to fetch {url}: {e}")
        if response.status_code >= 400:
            raise ToolError(
                f"Failed to fetch {url} - status code {response.status_code}"
            )

        page_raw = response.text

    content_type = response.headers.get("content-type", "")
    is_page_html = (
        "<html" in page_raw[:100] or "text/html" in content_type or not content_type
    )

    if is_page_html:
        return {
            "type": "html",
            "url": url,
            "content": extract_content_from_html(page_raw),
        }

    return {
        "type": "text",
        "url": url,
        "content": page_raw,
    }


if __name__ == "__main__":
    # Test the most basic functionality
    example_url = "https://www.example.com"
    import asyncio

    result = asyncio.run(fetch_url(example_url))
    print(result)


async def web_search(
    query: Annotated[str, "The search query to find information online."],
) -> dict[str, str]:
    """
    Performs a web search using the Brave Search API to find documentation, solutions, or current events.
    """
    import os
    import httpx
    import json
    from conduit.domain.exceptions.exceptions import ToolError

    api_key = os.getenv("BRAVE_API_KEY")
    if not api_key:
        raise ToolError("Missing BRAVE_API_KEY environment variable.")

    url = "https://api.search.brave.com/res/v1/web/search"
    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": api_key,
    }
    params = {
        "q": query,
        "count": 5,  # Limit results to save tokens
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                url, headers=headers, params=params, timeout=10.0
            )

            if response.status_code == 401:
                raise ToolError("Invalid Brave Search API key.")
            if response.status_code == 429:
                raise ToolError("Rate limit exceeded for Brave Search.")

            response.raise_for_status()
            data = response.json()

        # Parse and format the results for the LLM
        results = []

        # Brave puts results in ['web']['results']
        web_results = data.get("web", {}).get("results", [])

        if not web_results:
            return {"result": "No results found."}

        for item in web_results:
            title = item.get("title", "No Title")
            link = item.get("url", "")
            description = item.get("description", "")
            # Only include pub date if available, helpful for "latest news" queries
            age = item.get("age", "")

            entry = f"Title: {title}\nURL: {link}\nDescription: {description}"
            if age:
                entry += f"\nPublished: {age}"
            results.append(entry)

        return {"result": "\n---\n".join(results)}

    except httpx.TimeoutException:
        return {"error": "Search request timed out."}
    except Exception as e:
        return {"error": f"Search failed: {str(e)}"}


__all__ = ["fetch_url", "web_search"]

```

## FILE: src/conduit/capabilities/tools/tools/files.py
```py
from typing import Annotated, Any
from conduit.domain.exceptions.exceptions import ToolError


async def glob_files(
    pattern: Annotated[
        str, "The glob pattern to match filenames (e.g., 'src/**/*.py' or '*.md')."
    ],
    root_dir: Annotated[str, "The root directory to start the search from."] = ".",
) -> dict[str, str]:
    """
    Finds file paths matching a glob pattern, respecting .gitignore. Useful for discovering project structure.
    """
    import asyncio
    from pathlib import Path
    import pathspec

    def _run_glob():
        p = Path(root_dir)

        # Load .gitignore if present
        spec = None
        gitignore_path = p / ".gitignore"
        if gitignore_path.exists():
            with open(gitignore_path, "r") as f:
                spec = pathspec.PathSpec.from_lines("gitwildmatch", f)

        # Decide between recursive or flat glob
        files = (
            p.rglob(pattern.replace("**/", "")) if "**" in pattern else p.glob(pattern)
        )

        matches = []
        for file_path in files:
            if not file_path.is_file():
                continue

            # Get relative path
            rel_path = file_path.relative_to(p)

            # Check .gitignore
            if spec and spec.match_file(str(rel_path)):
                continue

            # Skip hidden files
            if file_path.name.startswith("."):
                continue

            matches.append(str(rel_path))

        if not matches:
            return "No files found matching that pattern."
        return "\n".join(matches)

    output = await asyncio.to_thread(_run_glob)
    return {"result": output}


async def grep_files(
    regex: Annotated[str, "The regular expression to search for in file contents."],
    glob_pattern: Annotated[
        str, "A glob pattern to limit the search scope (e.g., '**/*.py')."
    ] = "**/*",
    root_dir: Annotated[str, "The root directory to search within."] = ".",
) -> dict[str, str]:
    """
    Searches for a regex pattern inside file contents, respecting .gitignore.
    """
    import asyncio
    import re
    from pathlib import Path
    import pathspec

    def _run_grep():
        compiled_re = re.compile(regex)
        p = Path(root_dir)

        # Load .gitignore if present
        spec = None
        gitignore_path = p / ".gitignore"
        if gitignore_path.exists():
            with open(gitignore_path, "r") as f:
                spec = pathspec.PathSpec.from_lines("gitwildmatch", f)

        results = []
        candidates = p.rglob(glob_pattern.replace("**/", ""))

        for file_path in candidates:
            if not file_path.is_file():
                continue

            rel_path = file_path.relative_to(p)

            # Check .gitignore
            if spec and spec.match_file(str(rel_path)):
                continue

            # Skip large/binary files
            if file_path.stat().st_size > 1_000_000:
                continue

            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    for i, line in enumerate(f, 1):
                        if compiled_re.search(line):
                            results.append(f"{rel_path}:{i}: {line.strip()}")
            except Exception:
                continue

        if not results:
            return "No matches found."
        return "\n".join(results)

    output = await asyncio.to_thread(_run_grep)
    return {"result": output}


async def file_read(
    path: Annotated[str, "The path to the file to read."],
    start_line: Annotated[
        int, "The line number to start reading from (1-indexed)."
    ] = 1,
    end_line: Annotated[
        int | None, "The line number to stop reading at. If None, reads to end."
    ] = None,
    root_dir: Annotated[str, "The root directory to enforce security sandbox."] = ".",
) -> dict[str, str]:
    """
    Read a file's contents with line numbers, supporting pagination.
    """
    from pathlib import Path

    p = Path(path).expanduser().resolve()
    root = Path(root_dir).expanduser().resolve()

    # Security: Jail Check
    if not p.is_relative_to(root):
        raise ToolError(f"Access denied: {path} is outside the allowed directory.")

    if not p.is_file():
        raise ToolError(f"Path is not a file: {path}")

    if not p.exists():
        raise ToolError(f"The specified file does not exist: {path}")

    try:
        lines = p.read_text(encoding="utf-8").splitlines()
    except UnicodeDecodeError:
        raise ToolError("File is binary or not UTF-8 encoded.")

    total_lines = len(lines)

    # Handle end_line logic
    actual_end = end_line if end_line is not None else total_lines
    start_idx = max(0, start_line - 1)
    end_idx = min(total_lines, actual_end)

    if start_idx >= total_lines:
        return {
            "result": f"File has {total_lines} lines. Start line {start_line} is out of bounds."
        }

    # Add line numbers (1-indexed)
    numbered_lines = [
        f"{i + 1} | {line}"
        for i, line in enumerate(lines[start_idx:end_idx], start=start_idx)
    ]

    content = "\n".join(numbered_lines)

    return {
        "path": str(p),
        "total_lines": str(total_lines),
        "viewing_range": f"{start_idx + 1}-{end_idx}",
        "file_contents": content,
    }


async def ls(
    path: Annotated[str, "The directory path to list."] = ".",
    root_dir: Annotated[str, "The root directory to enforce security sandbox."] = ".",
) -> dict[str, str]:
    """
    Lists files in a directory with useful metadata (size, type).
    """
    from pathlib import Path

    target = Path(path).expanduser().resolve()
    root = Path(root_dir).expanduser().resolve()

    if not target.is_relative_to(root):
        raise ToolError(f"Access denied: {path} is outside the allowed directory.")

    if not target.is_dir():
        raise ToolError(f"Not a directory: {path}")

    results = []
    # Header
    results.append(f"{'Mode':<5} {'Size':<10} {'Name'}")
    results.append("-" * 30)

    # Sort directories first, then files
    try:
        entries = sorted(
            target.iterdir(), key=lambda e: (not e.is_dir(), e.name.lower())
        )
    except PermissionError:
        raise ToolError(f"Permission denied: Cannot list directory {path}")

    for item in entries:
        # Skip hidden files usually, or keep them if relevant to your dev work
        if item.name.startswith("."):
            continue

        try:
            stats = item.stat()
            size_str = f"{stats.st_size:,} B" if item.is_file() else "-"
            type_marker = "DIR" if item.is_dir() else "FILE"
            results.append(f"{type_marker:<5} {size_str:<10} {item.name}")
        except PermissionError:
            results.append(f"????  {'?':<10} {item.name} (Access Denied)")

    return {"result": "\n".join(results)}


__all__ = ["file_read", "glob_files", "grep_files", "ls"]

```

## FILE: src/conduit/config.py
```py
"""
Configuration hierarchy:
1. Explicit parameters to functions / methods
2. Environment variables (highest priority)
3. System prompt file / settings TOML file
4. Defaults (lowest priority)
"""

from __future__ import annotations
from pathlib import Path
import tomllib
import json
from dataclasses import dataclass
from conduit.utils.progress.verbosity import Verbosity
from rich.console import Console
from xdg_base_dirs import (
    xdg_config_home,
    xdg_state_home,
    xdg_data_home,
)
import os
from collections.abc import Callable
from typing import TYPE_CHECKING
from importlib.metadata import version

if TYPE_CHECKING:
    from conduit.storage.odometer.odometer_registry import OdometerRegistry
    from conduit.domain.request.generation_params import GenerationParams
    from conduit.domain.config.conduit_options import ConduitOptions
    from conduit.storage.cache.protocol import ConduitCache
    from conduit.storage.repository.protocol import AsyncSessionRepository
    from psycopg2 import connection

# Global odometer instance
_odometer_registry: OdometerRegistry | None = None

# Directories
CONFIG_DIR = Path(xdg_config_home()) / "conduit"
STATE_DIR = Path(xdg_state_home()) / "conduit"
DATA_DIR = Path(xdg_data_home()) / "conduit"

# Version
try:
    __version__ = version("conduit")
except Exception:
    __version__ = "unknown"

# File paths
SYSTEM_PROMPT_PATH = CONFIG_DIR / "system_message.jinja2"
SETTINGS_TOML_PATH = CONFIG_DIR / "settings.toml"
SKILLS_DIR = CONFIG_DIR / "skills"
OLLAMA_CONTEXT_SIZES_PATH = CONFIG_DIR / "ollama_context_sizes.json"
SERVER_MODELS_PATH = STATE_DIR / "server_models.json"
OLLAMA_MODELS_PATH = STATE_DIR / "ollama_models.json"
DEFAULT_HISTORY_FILE = DATA_DIR / "conduit" / "history.json"
DEFAULT_LOG_FILE = DATA_DIR / "conduit" / "conduit.log"


@dataclass
class Settings:
    system_prompt: str
    preferred_model: str
    default_verbosity: Verbosity
    default_console: Console
    server_models: list[str]
    paths: dict[str, Path]
    default_project_name: str
    version: str
    # Lazy loaders
    odometer_registry: Callable[[], OdometerRegistry]
    default_params: Callable[[], GenerationParams]
    default_cache: Callable[[str], ConduitCache]
    default_repository: Callable[[str], ConversationRepository]
    default_conduit_options: Callable[[str], ConduitOptions]


def load_settings() -> Settings:
    # Defaults (lowest priority)
    config: dict[str, object] = {
        "system_prompt": "You are a helpful assistant.",
        "preferred_model": "gpt3",
        "default_verbosity": Verbosity.PROGRESS,
        "default_console": Console(stderr=True),
        "server_models": [],
        "paths": {},
        "default_project_name": "conduit",
        "version": __version__,
    }

    # Config files (medium priority)
    # Ensure critical config files exist or provide defaults if missing logic is preferred
    # (Leaving assertions as is per original file, assuming scaffolding exists)
    if SYSTEM_PROMPT_PATH.exists():
        system_prompt = SYSTEM_PROMPT_PATH.read_text()
    else:
        system_prompt = config["system_prompt"]  # Fallback

    if SETTINGS_TOML_PATH.exists():
        with SETTINGS_TOML_PATH.open("rb") as f:
            toml_config = tomllib.load(f)
        toml_dict = toml_config.get("settings", {})
        preferred_model = toml_dict.get("preferred_model", config["preferred_model"])
        default_project_name = toml_dict.get(
            "default_project_name", config["default_project_name"]
        )
        verbosity_str = toml_dict.get("verbosity", config["default_verbosity"].name)
        verbosity = Verbosity[verbosity_str.upper()]
    else:
        preferred_model = config["preferred_model"]
        default_project_name = config["default_project_name"]
        verbosity = config["default_verbosity"]

    server_models: list[str] = []
    if SERVER_MODELS_PATH.exists():
        with SERVER_MODELS_PATH.open("r") as f:
            try:
                server_models_dict = json.load(f)
                server_models = server_models_dict.get("ollama", [])
            except json.JSONDecodeError:
                pass

    # Environment variables (highest priority)
    system_prompt = (
        os.getenv("CONDUIT_SYSTEM_PROMPT", system_prompt)
        if os.getenv("CONDUIT_SYSTEM_PROMPT")
        else system_prompt
    )
    preferred_model = (
        os.getenv("CONDUIT_PREFERRED_MODEL", preferred_model)
        if os.getenv("CONDUIT_PREFERRED_MODEL")
        else preferred_model
    )
    default_verbosity = (
        Verbosity[os.getenv("CONDUIT_VERBOSITY").upper()]
        if os.getenv("CONDUIT_VERBOSITY")
        else verbosity
    )

    paths = {
        "CONFIG_DIR": CONFIG_DIR,
        "STATE_DIR": STATE_DIR,
        "DATA_DIR": DATA_DIR,
        "SKILLS_DIR": SKILLS_DIR,
        "SYSTEM_PROMPT_PATH": SYSTEM_PROMPT_PATH,
        "SETTINGS_TOML_PATH": SETTINGS_TOML_PATH,
        "SERVER_MODELS_PATH": SERVER_MODELS_PATH,
        "OLLAMA_CONTEXT_SIZES_PATH": OLLAMA_CONTEXT_SIZES_PATH,
        "OLLAMA_MODELS_PATH": OLLAMA_MODELS_PATH,
        "DEFAULT_HISTORY_FILE": DEFAULT_HISTORY_FILE,
        "DEFAULT_LOG_FILE": DEFAULT_LOG_FILE,
    }

    # Default params
    def default_params() -> GenerationParams:
        from conduit.domain.request.generation_params import GenerationParams

        default_params = GenerationParams(
            model=preferred_model,
        )
        return default_params

    # CHANGED: Sync function, returns Lazy Cache
    def default_cache(project_name: str = default_project_name) -> ConduitCache:
        """
        Lazy loader for the default AsyncPostgresCache instance.
        """
        from conduit.storage.cache.postgres_cache_async import AsyncPostgresCache

        # We just pass parameters; the pool is created when accessed inside the loop
        return AsyncPostgresCache(project_name=project_name, db_name="conduit")

    def default_repository(
        project_name: str = default_project_name,
    ) -> ConversationRepository:
        # ... [repository code remains the same] ...
        from conduit.storage.repository.postgres_repository import get_async_repository

        return get_async_repository(project_name)

    def default_conduit_options(name: str = default_project_name) -> ConduitOptions:
        """
        Assemble default ConduitOptions from settings.
        Default is NO cache, no repository.
        """
        from conduit.domain.config.conduit_options import ConduitOptions

        return ConduitOptions(
            verbosity=default_verbosity,
            project_name=name,
            cache=None,
            repository=None,
            console=config["default_console"],
        )

    def get_odometer_registry() -> OdometerRegistry:
        from conduit.storage.odometer.odometer_registry import OdometerRegistry

        global _odometer_registry
        if _odometer_registry is None:
            _odometer_registry = OdometerRegistry()
        return _odometer_registry

    config.update(
        {
            "default_cache": default_cache,
            "default_conduit_options": default_conduit_options,
            "default_params": default_params,
            "default_repository": default_repository,
            "default_verbosity": default_verbosity,
            "odometer_registry": get_odometer_registry,
            "paths": paths,
            "preferred_model": preferred_model,
            "server_models": server_models,
            "system_prompt": system_prompt,
            # Lazy loaders
        }
    )

    return Settings(**config)


# Singleton
settings = load_settings()

```

## FILE: src/conduit/core/clients/anthropic/adapter.py
```py
from __future__ import annotations
from typing import Any
from conduit.domain.message.message import (
    Message,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    ToolMessage,
    TextContent,
    ImageContent,
    AudioContent,
)
import json
import re


def convert_message_to_anthropic(message: Message) -> dict[str, Any]:
    """
    Pure function to adapt an internal Message DTO into an Anthropic-compatible dictionary.

    Key differences from OpenAI:
    - Images use {"type": "image", "source": {...}} instead of image_url
    - Audio is not supported
    - System messages are converted to user messages here; extraction happens at request level
    """
    match message:
        # 1. System Message
        # Anthropic doesn't support system role in messages array
        # This will be filtered out in _convert_request
        case SystemMessage(content=content):
            return {"role": "user", "content": content}

        # 2. User Message (Text or Multimodal)
        case UserMessage(content=content, name=name):
            payload = {"role": "user"}

            if isinstance(content, str):
                payload["content"] = content
                return payload

            # Handle list of content blocks (Multimodal)
            anthropic_content = []
            for block in content:
                match block:
                    case TextContent(text=text):
                        anthropic_content.append({"type": "text", "text": text})
                    case ImageContent(url=url, detail=_):
                        # Parse base64 data URL for Anthropic format
                        # Expected format: data:image/png;base64,iVBORw0KG...
                        image_data = _parse_image_url(url)
                        anthropic_content.append(
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": image_data["media_type"],
                                    "data": image_data["data"],
                                },
                            }
                        )
                    case AudioContent(data=data, format=fmt):
                        raise NotImplementedError(
                            "Anthropic API does not support audio input messages."
                        )
            payload["content"] = anthropic_content
            return payload

        # 3. Assistant Message (Text, Reasoning, Tools)
        case AssistantMessage(content=content, tool_calls=calls, audio=audio):
            payload = {"role": "assistant"}

            # Anthropic requires content to be present
            if content:
                payload["content"] = content
            elif calls:
                # Tool-only response needs empty content array
                payload["content"] = []
            else:
                payload["content"] = ""

            # Handle Tool Calls
            if calls:
                # Anthropic uses 'tool_use' blocks within content
                if not isinstance(payload["content"], list):
                    # Convert string content to text block
                    payload["content"] = [{"type": "text", "text": payload["content"]}]

                for call in calls:
                    payload["content"].append(
                        {
                            "type": "tool_use",
                            "id": call.id,
                            "name": call.function_name,
                            "input": call.arguments,  # Anthropic uses dict, not JSON string
                        }
                    )

            # Audio output is not supported
            if audio:
                raise NotImplementedError(
                    "Anthropic API does not support audio output messages."
                )

            return payload

        # 4. Tool Result
        case ToolMessage(content=result, tool_call_id=call_id):
            return {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": call_id,
                        "content": result,
                    }
                ],
            }

        case _:
            raise ValueError(
                f"Unknown message type for Anthropic Adapter: {type(message)}"
            )


def _parse_image_url(url: str) -> dict[str, str]:
    """
    Parse a data URL to extract media type and base64 data.

    Expected format: data:image/png;base64,iVBORw0KG...
    Returns: {"media_type": "image/png", "data": "iVBORw0KG..."}
    """
    if url.startswith("data:"):
        # Parse data URL
        match = re.match(r"data:([^;]+);base64,(.+)", url)
        if match:
            media_type, data = match.groups()
            return {"media_type": media_type, "data": data}
        raise ValueError(f"Invalid data URL format: {url}")
    else:
        # External URL - Anthropic doesn't support external image URLs directly
        raise ValueError(
            "Anthropic requires base64-encoded images, not external URLs. "
            "Please convert the image to base64 first."
        )

```

## FILE: src/conduit/core/clients/anthropic/client.py
```py
from __future__ import annotations
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.core.clients.anthropic.payload import AnthropicPayload
from conduit.core.clients.anthropic.message_adapter import convert_message_to_anthropic
from conduit.core.clients.anthropic.tool_adapter import convert_tool_to_anthropic
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import AssistantMessage, ToolCall
from functools import cached_property
from typing import TYPE_CHECKING, override, Any
import os
import time
import json

if TYPE_CHECKING:
    from collections.abc import Sequence
    from anthropic import AsyncAnthropic, AsyncStream, Anthropic
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.message.message import Message
    from conduit.domain.result.result import GenerationResult
    from instructor import Instructor


class AnthropicClient(Client):
    """
    Client implementation for Anthropic's Claude API.
    Async only.
    """

    @cached_property
    def async_client(self) -> AsyncAnthropic:
        """
        Provides access to the raw AsyncAnthropic client for advanced use cases.
        """
        from anthropic import AsyncAnthropic

        async_client = AsyncAnthropic(api_key=self._get_api_key())
        return async_client

    @cached_property
    def sync_client(self) -> Anthropic:
        """
        Provides access to the raw synchronous Anthropic client for advanced use cases.
        """
        from anthropic import Anthropic

        sync_client = Anthropic(api_key=self._get_api_key())
        return sync_client

    @cached_property
    def instructor_client(self) -> Instructor:
        """
        Provides access to the Instructor-wrapped Anthropic client for structured responses.
        """
        import instructor

        instructor_client = instructor.from_anthropic(self.async_client)
        return instructor_client

    def _get_api_key(self) -> str:
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if api_key is None:
            raise ValueError("No ANTHROPIC_API_KEY found in environment variables")
        else:
            return api_key

    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Converts a single internal Message DTO into Anthropic's specific dictionary format.
        """
        return convert_message_to_anthropic(message)

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Translates the internal generic Request DTO into the specific
        dictionary parameters required by Anthropic's SDK.

        Key Anthropic-specific handling:
        - System messages are extracted from the messages list and passed as a separate 'system' parameter
        - max_tokens is required (defaults to 4096 if not specified)
        """
        # Convert messages and extract system messages
        converted_messages = []
        system_messages = []

        for message in request.messages:
            # Import here to avoid circular dependency
            from conduit.domain.message.message import SystemMessage

            if isinstance(message, SystemMessage):
                system_messages.append(message.content)
            else:
                converted_messages.append(self._convert_message(message))

        # Combine system messages into a single system parameter
        system_content = "\n\n".join(system_messages) if system_messages else None

        # Convert tools if present
        tools = None
        if request.options.tool_registry:
            tools = [
                convert_tool_to_anthropic(tool)
                for tool in request.options.tool_registry.tools
            ]

        anthropic_payload = AnthropicPayload(
            model=request.params.model,
            messages=converted_messages,
            max_tokens=request.params.max_tokens if request.params.max_tokens else 4096,
            system=system_content,
            temperature=request.params.temperature,
            top_p=request.params.top_p,
            stream=request.params.stream,
            tools=tools,
        )
        return anthropic_payload

    @override
    def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Get token count per official Anthropic api endpoint.
        """

        # CASE 1: Raw String (Benchmarking)
        # We wrap it in a user message to satisfy the API, then subtract the overhead.
        if isinstance(payload, str):
            messages = [{"role": "user", "content": payload}]
            response = self.sync_client.messages.count_tokens(
                model=model,
                messages=messages,
            )
            # Subtract standard overhead (approx 3 tokens for a single-turn user message)
            # to return the "pure" string weight.
            return max(0, response.input_tokens - 3)

        # CASE 2: Message History (Context Window Check)
        if isinstance(payload, list):
            # Convert Message objects to Anthropic dictionaries
            messages_payload = []
            for m in payload:
                try:
                    # Filter out messages that Anthropic doesn't support (e.g. Audio)
                    messages_payload.append(m.to_anthropic())
                except NotImplementedError:
                    continue

            # If filtration resulted in empty list (e.g. only audio messages), return 0
            if not messages_payload:
                return 0

            response = anthropic_client.messages.count_tokens(
                model=model,
                messages=messages_payload,
            )
            return response.input_tokens

        raise ValueError("Payload must be string or Sequence[Message]")

    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResult:
        """
        Generate text using Anthropic's Claude API and return a GenerationResponse.

        Returns:
            - GenerationResponse object for successful non-streaming requests
            - AsyncStream object for streaming requests
        """
        from anthropic import AsyncStream

        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Use the raw client for standard completions
        result = await self.async_client.messages.create(**payload_dict)

        # Handle streaming response
        if isinstance(result, AsyncStream):
            # For streaming, return the AsyncStream object directly
            return result

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = result.model
        input_tokens = (
            result.usage.input_tokens
        )  # Anthropic uses input_tokens not prompt_tokens
        output_tokens = (
            result.usage.output_tokens
        )  # Anthropic uses output_tokens not completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(result, "stop_reason"):
            match result.stop_reason:
                case "end_turn":
                    stop_reason = StopReason.STOP
                case "max_tokens":
                    stop_reason = StopReason.LENGTH
                case "stop_sequence":
                    stop_reason = StopReason.STOP
                case "tool_use":
                    stop_reason = StopReason.TOOL_CALLS

        # Parse content blocks - Anthropic returns a list of content blocks
        text_content = None
        tool_calls = []

        for block in result.content:
            if hasattr(block, "type"):
                if block.type == "text":
                    # Extract text content
                    text_content = block.text
                elif block.type == "tool_use":
                    # Extract tool call
                    tool_call = ToolCall(
                        id=block.id,
                        type="function",
                        function_name=block.name,
                        arguments=block.input,  # Anthropic provides dict directly
                        provider="anthropic",
                        raw={"type": "tool_use", "id": block.id, "name": block.name, "input": block.input},
                    )
                    tool_calls.append(tool_call)

        # Create AssistantMessage with content and/or tool calls
        assistant_message = AssistantMessage(
            content=text_content,
            tool_calls=tool_calls if tool_calls else None,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate a structured response using Anthropic's function calling and return a GenerationResponse.

        Returns:
            - GenerationResponse object with parsed structured data in AssistantMessage.parsed
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Make the API call with function calling using instructor client
        (
            user_obj,
            completion,
        ) = await self.instructor_client.messages.create_with_completion(
            response_model=request.params.response_model, **payload_dict
        )

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = completion.model
        input_tokens = completion.usage.input_tokens
        output_tokens = completion.usage.output_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(completion, "stop_reason"):
            match completion.stop_reason:
                case "end_turn":
                    stop_reason = StopReason.STOP
                case "max_tokens":
                    stop_reason = StopReason.LENGTH
                case "stop_sequence":
                    stop_reason = StopReason.STOP

        # Create AssistantMessage with parsed structured data
        # For Anthropic, content[0].text may be None for structured responses
        content_text = completion.content[0].text if completion.content else None
        assistant_message = AssistantMessage(
            content=content_text,
            parsed=user_obj,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

```

## FILE: src/conduit/core/clients/anthropic/message_adapter.py
```py
from __future__ import annotations
from typing import Any
from conduit.domain.message.message import (
    Message,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    ToolMessage,
    TextContent,
    ImageContent,
    AudioContent,
)
import json
import re


def convert_message_to_anthropic(message: Message) -> dict[str, Any]:
    """
    Pure function to adapt an internal Message DTO into an Anthropic-compatible dictionary.

    Key differences from OpenAI:
    - Images use {"type": "image", "source": {...}} instead of image_url
    - Audio is not supported
    - System messages are converted to user messages here; extraction happens at request level
    """
    match message:
        # 1. System Message
        # Anthropic doesn't support system role in messages array
        # This will be filtered out in _convert_request
        case SystemMessage(content=content):
            return {"role": "user", "content": content}

        # 2. User Message (Text or Multimodal)
        case UserMessage(content=content, name=name):
            payload = {"role": "user"}

            if isinstance(content, str):
                payload["content"] = content
                return payload

            # Handle list of content blocks (Multimodal)
            anthropic_content = []
            for block in content:
                match block:
                    case TextContent(text=text):
                        anthropic_content.append({"type": "text", "text": text})
                    case ImageContent(url=url, detail=_):
                        # Parse base64 data URL for Anthropic format
                        # Expected format: data:image/png;base64,iVBORw0KG...
                        image_data = _parse_image_url(url)
                        anthropic_content.append(
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": image_data["media_type"],
                                    "data": image_data["data"],
                                },
                            }
                        )
                    case AudioContent(data=data, format=fmt):
                        raise NotImplementedError(
                            "Anthropic API does not support audio input messages."
                        )
            payload["content"] = anthropic_content
            return payload

        # 3. Assistant Message (Text, Reasoning, Tools)
        case AssistantMessage(content=content, tool_calls=calls, audio=audio):
            payload = {"role": "assistant"}

            # Anthropic requires content to be present
            if content:
                payload["content"] = content
            elif calls:
                # Tool-only response needs empty content array
                payload["content"] = []
            else:
                payload["content"] = ""

            # Handle Tool Calls
            if calls:
                # Anthropic uses 'tool_use' blocks within content
                if not isinstance(payload["content"], list):
                    # Convert string content to text block
                    payload["content"] = [{"type": "text", "text": payload["content"]}]

                for call in calls:
                    payload["content"].append(
                        {
                            "type": "tool_use",
                            "id": call.id,
                            "name": call.function_name,
                            "input": call.arguments,  # Anthropic uses dict, not JSON string
                        }
                    )

            # Audio output is not supported
            if audio:
                raise NotImplementedError(
                    "Anthropic API does not support audio output messages."
                )

            return payload

        # 4. Tool Result
        case ToolMessage(content=result, tool_call_id=call_id):
            return {
                "role": "user",
                "content": [
                    {
                        "type": "tool_result",
                        "tool_use_id": call_id,
                        "content": result,
                    }
                ],
            }

        case _:
            raise ValueError(
                f"Unknown message type for Anthropic Adapter: {type(message)}"
            )


def _parse_image_url(url: str) -> dict[str, str]:
    """
    Parse a data URL to extract media type and base64 data.

    Expected format: data:image/png;base64,iVBORw0KG...
    Returns: {"media_type": "image/png", "data": "iVBORw0KG..."}
    """
    if url.startswith("data:"):
        # Parse data URL
        match = re.match(r"data:([^;]+);base64,(.+)", url)
        if match:
            media_type, data = match.groups()
            return {"media_type": media_type, "data": data}
        raise ValueError(f"Invalid data URL format: {url}")
    else:
        # External URL - Anthropic doesn't support external image URLs directly
        raise ValueError(
            "Anthropic requires base64-encoded images, not external URLs. "
            "Please convert the image to base64 first."
        )

```

## FILE: src/conduit/core/clients/anthropic/payload.py
```py
from pydantic import BaseModel, Field, ConfigDict
from typing import Any


class AnthropicPayload(BaseModel):
    """
    Anti-corruption Layer for Anthropic Messages API.
    Validates top-level configuration while allowing flexibility in message structure.

    Key differences from OpenAI:
    - 'max_tokens' is REQUIRED (we default to 4096 if not set).
    - 'system' is a top-level parameter, not a message role.
    - 'stop_sequences' instead of 'stop'.
    - 'thinking' parameter for Claude 3.7+ reasoning.
    """

    model_config = ConfigDict(extra="allow")

    # Required
    model: str
    messages: list[dict[str, Any]]
    max_tokens: int = Field(
        default=4096, ge=1
    )  # Anthropic throws an error if max_tokens is missing.

    system: str | list[dict[str, Any]] | None = None  # Quirk of Anthropic API

    temperature: float | None = Field(default=None, ge=0.0, le=1.0)
    top_p: float | None = Field(default=None, ge=0.0, le=1.0)
    top_k: int | None = Field(default=None, ge=0)

    # Anthropic uses 'stop_sequences' (list), not 'stop'
    stop_sequences: list[str] | None = None
    stream: bool | None = None

    # Advanced features
    thinking: dict[str, Any] | None = None
    metadata: dict[str, Any] | None = None

    # Tools
    tools: list[dict[str, Any]] | None = None
    tool_choice: dict[str, Any] | None = None

```

## FILE: src/conduit/core/clients/anthropic/tool_adapter.py
```py
from __future__ import annotations
from typing import Any
from conduit.capabilities.tools.tool import Tool


def convert_tool_to_anthropic(tool: Tool) -> dict[str, Any]:
    """
    Convert a canonical Tool to Anthropic's tool format.

    Anthropic's format:
    {
        "name": "function_name",
        "description": "function description",
        "input_schema": {
            "type": "object",
            "properties": {...},
            "required": [...]
        }
    }

    Note: Unlike OpenAI, Anthropic doesn't wrap tools in a "function" key,
    and uses "input_schema" directly (not "parameters").
    """
    return {
        "name": tool.name,
        "description": tool.description,
        "input_schema": tool.input_schema.model_dump(
            by_alias=True, exclude_none=True
        ),
    }

```

## FILE: src/conduit/core/clients/client_base.py
```py
"""
Base class for clients; openai, anthropic, etc. inherit from this class.
Both sync and async methods are defined here.
"""

from __future__ import annotations
from typing import TYPE_CHECKING, override
import logging

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.core.clients.payload_base import Payload
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.message.message import Message


logger = logging.getLogger(__name__)


class Client:
    def _convert_messages(self, messages: Sequence[Message]) -> list[dict[str, Any]]:
        return [self._convert_message(m) for m in messages]

    def _convert_message(self, message: Message) -> dict[str, Any]:
        raise NotImplementedError("Should be implemented in subclass.")

    def _convert_request(self, request: GenerationRequest) -> Payload:
        raise NotImplementedError("Should be implemented in subclass.")

    async def query(self, request: GenerationRequest) -> GenerationResult:
        raise NotImplementedError("Should be implemented in subclass.")

    def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        raise NotImplementedError("Should be implemented in subclass.")

    @override
    def __repr__(self):
        """
        Standard repr.
        """
        attributes = ", ".join(
            [f"{k}={repr(v)[:50]}" for k, v in self.__dict__.items()]
        )
        return f"{self.__class__.__name__}({attributes})"

```

## FILE: src/conduit/core/clients/google/audio_params.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from enum import Enum


class GoogleTTSVoice(str, Enum):
    """Available voices for Google Gemini TTS"""
    AOEDE = "aoede"
    CHARLIE = "charlie"
    CHARON = "charon"
    FENRIR = "fenrir"
    KORE = "kore"
    PUCK = "puck"


class GoogleTTSModel(str, Enum):
    """Available Google TTS models"""
    GEMINI_2_5_FLASH_TTS = "gemini-2.5-flash-preview-tts"


class GoogleAudioFormat(str, Enum):
    """Available audio formats"""
    MP3 = "mp3"
    WAV = "wav"
    OPUS = "opus"
    AAC = "aac"
    FLAC = "flac"
    PCM = "pcm"


class GoogleAudioParams(BaseModel):
    """Parameters for Google audio generation (TTS)"""

    voice: GoogleTTSVoice = Field(
        default=GoogleTTSVoice.AOEDE,
        description="Voice to use for TTS"
    )
    model: GoogleTTSModel = Field(
        default=GoogleTTSModel.GEMINI_2_5_FLASH_TTS,
        description="TTS model to use"
    )
    response_format: GoogleAudioFormat = Field(
        default=GoogleAudioFormat.MP3,
        description="Audio format for output"
    )

```

## FILE: src/conduit/core/clients/google/client.py
```py
"""
For Google Gemini models.
"""

from __future__ import annotations
from functools import cached_property
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.core.clients.google.payload import GooglePayload
from conduit.core.clients.google.message_adapter import convert_message_to_google
from conduit.core.clients.google.tool_adapter import convert_tool_to_google
from conduit.core.clients.google.image_params import GoogleImageParams
from conduit.core.clients.google.audio_params import GoogleAudioParams
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import AssistantMessage, ImageOutput, ToolCall
from typing import TYPE_CHECKING, override, Any
import json
import os
import time
import base64

if TYPE_CHECKING:
    from collections.abc import Sequence
    from openai import AsyncOpenAI
    from instructor import Instructor
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.message.message import Message


class GoogleClient(Client):
    """
    Client implementation for Google's Gemini API using the OpenAI-compatible endpoint.
    Async by default.
    """

    @cached_property
    def async_client(self) -> AsyncOpenAI:
        """
        Exposes the raw AsyncOpenAI client for direct use if needed.
        """
        from openai import AsyncOpenAI

        async_client = AsyncOpenAI(
            api_key=self._get_api_key(),
            base_url="https://generativelanguage.googleapis.com/v1beta/",
        )
        return async_client

    @cached_property
    def instructor_client(self) -> Instructor:
        """
        Exposes the Instructor-wrapped client for structured responses.
        """
        import instructor

        instructor_client = instructor.from_openai(
            self.async_client,
            mode=instructor.Mode.JSON,
        )
        return instructor_client

    def _get_api_key(self) -> str:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        return api_key

    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Converts a single internal Message DTO into Google's specific dictionary format.
        Since Google uses OpenAI spec, we delegate to the OpenAI adapter.
        """
        return convert_message_to_google(message)

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Translates the internal generic Request DTO into the specific
        dictionary parameters required by Google's SDK (via OpenAI spec).
        """
        # Load client params
        client_params = request.params.client_params or {}
        allowed_params = {"frequency_penalty", "presence_penalty"}
        for param in client_params.keys():
            if param not in allowed_params:
                raise ValueError(f"Unsupported Google client parameter: {param}")
        # Convert messages
        converted_messages = self._convert_messages(request.messages)

        # Convert tools and enable parallel tool calls if tools are present
        tools = None
        parallel_tool_calls = None
        if request.options.tool_registry:
            tools = [
                convert_tool_to_google(tool)
                for tool in request.options.tool_registry.tools
            ]
            parallel_tool_calls = request.options.parallel_tool_calls

        # Build payload
        google_payload = GooglePayload(
            model=request.params.model,
            messages=converted_messages,
            temperature=request.params.temperature,
            top_p=request.params.top_p,
            max_tokens=request.params.max_tokens,
            stream=request.params.stream,
            tools=tools,
            parallel_tool_calls=parallel_tool_calls,
            # Google-specific params
            **client_params,
        )
        return google_payload

    @override
    def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Get the token count per official tokenizer (through Google Native API).
        We use the google.generativeai SDK for this because Gemini tokens != tiktoken.
        """
        import google.generativeai as genai

        genai.configure(api_key=self._get_api_key())
        model_client = genai.GenerativeModel(model_name=model)

        # CASE 1: Raw String
        if isinstance(payload, str):
            response = model_client.count_tokens(payload)
            return response.total_tokens

        # CASE 2: Message History
        if isinstance(payload, list):
            # We must convert conduit Messages (OpenAI-style) to Google Native format
            # just for the token counter.
            # Google Native Format: [{'role': 'user'|'model', 'parts': ['...']}]
            native_contents = []

            for msg in payload:
                # Map roles: 'assistant' -> 'model', everything else -> 'user'
                role = "model" if msg.role == "assistant" else "user"

                parts = []
                # Extract text content safely
                if hasattr(msg, "text_content") and msg.text_content:
                    parts.append(msg.text_content)
                elif isinstance(msg.content, str):
                    parts.append(msg.content)

                # Note: If you want to count image tokens accurately, you would need to
                # convert the base64 to a PIL image or Blob and append it to parts here.
                # For now, we are counting the text weight of the conversation.

                if parts:
                    native_contents.append({"role": role, "parts": parts})

            if not native_contents:
                return 0

            response = model_client.count_tokens(native_contents)
            return response.total_tokens

        raise ValueError("Payload must be string or Sequence[Message]")

    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "image":
                return await self._generate_image(request)
            case "audio":
                return await self._generate_audio(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResult:
        """
        Generate text using Google's Gemini API and return a GenerationResponse.

        Returns:
            - GenerationResponse object for successful non-streaming requests
            - AsyncStream object for streaming requests
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Use the raw client for standard completions
        result = await self.async_client.chat.completions.create(**payload_dict)

        from openai import AsyncStream

        # Handle streaming response
        if isinstance(result, AsyncStream):
            # For streaming, return the AsyncStream object directly
            return result

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = result.model
        input_tokens = result.usage.prompt_tokens
        output_tokens = result.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(result.choices[0], "finish_reason"):
            finish_reason = result.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH
            if finish_reason == "tool_calls":
                stop_reason = StopReason.TOOL_CALLS
            elif finish_reason == "content_filter":
                stop_reason = StopReason.CONTENT_FILTER

        # Process tool calls if present
        if stop_reason == StopReason.TOOL_CALLS:
            # Handle tool calls - iterate through all parallel calls
            tool_calls = []
            for tool_call_data in result.choices[0].message.tool_calls:
                arguments_dict = json.loads(tool_call_data.function.arguments)

                tool_call = ToolCall(
                    id=tool_call_data.id,  # Use provider-supplied ID
                    type="function",
                    function_name=tool_call_data.function.name,
                    arguments=arguments_dict,
                    provider="google",
                    raw=tool_call_data.dict(),
                )
                tool_calls.append(tool_call)

            # Create AssistantMessage with all tool calls
            assistant_message = AssistantMessage(
                content=result.choices[0].message.content
                if hasattr(result.choices[0].message, "content")
                else "",
                tool_calls=tool_calls,
            )
        else:
            # Extract the text content
            content = result.choices[0].message.content
            assistant_message = AssistantMessage(content=content)

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate a structured response using Google's function calling and return a GenerationResponse.

        Returns:
            - GenerationResponse object with parsed structured data in AssistantMessage.parsed
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Make the API call with function calling
        (
            user_obj,
            completion,
        ) = await self.instructor_client.chat.completions.create_with_completion(
            response_model=request.params.response_model, **payload_dict
        )

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = completion.model
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(completion.choices[0], "finish_reason"):
            finish_reason = completion.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH

        # Create AssistantMessage with parsed structured data
        assistant_message = AssistantMessage(
            content=completion.choices[0].message.content,
            parsed=user_obj,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_image(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate an image using Google's Imagen and return a GenerationResponse.

        Returns:
            - Response object with ImageOutput in AssistantMessage.images
        """
        start_time = time.time()

        # Extract text prompt from the last message
        last_message = request.messages[-1]
        if isinstance(last_message.content, str):
            prompt = last_message.content
        else:
            # Handle multimodal content - extract text
            prompt = " ".join(
                [block.text for block in last_message.content if hasattr(block, "text")]
            )

        # Get image parameters (use defaults if not provided)
        image_params = GoogleImageParams()
        if (
            request.params.client_params
            and "image_params" in request.params.client_params
        ):
            image_params = request.params.client_params["image_params"]

        # Call the images.generate endpoint
        response = await self.async_client.images.generate(
            model=image_params.model.value,
            prompt=prompt,
            response_format=image_params.response_format.value,
            n=image_params.n,
        )

        duration = (time.time() - start_time) * 1000

        # Convert response to ImageOutput objects
        image_outputs = []
        for image_data in response.data:
            image_output = ImageOutput(
                url=image_data.url if hasattr(image_data, "url") else None,
                b64_json=image_data.b64_json
                if hasattr(image_data, "b64_json")
                else None,
                revised_prompt=image_data.revised_prompt
                if hasattr(image_data, "revised_prompt")
                else None,
            )
            image_outputs.append(image_output)

        # Create AssistantMessage with images
        assistant_message = AssistantMessage(images=image_outputs)

        # Create ResponseMetadata (Imagen doesn't provide token counts)
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=image_params.model.value,
            input_tokens=0,  # Imagen doesn't provide token counts
            output_tokens=0,
            stop_reason=StopReason.STOP,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_audio(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate audio using Google's TTS API and return a GenerationResponse.

        Returns:
            - Response object with base64-encoded audio data
        """
        start_time = time.time()

        # Extract text from the last message
        last_message = request.messages[-1]
        if isinstance(last_message.content, str):
            text_input = last_message.content
        else:
            # Handle multimodal content - extract text
            text_input = " ".join(
                [block.text for block in last_message.content if hasattr(block, "text")]
            )

        # Get audio parameters (use defaults if not provided)
        audio_params = GoogleAudioParams()
        if (
            request.params.client_params
            and "audio_params" in request.params.client_params
        ):
            audio_params = request.params.client_params["audio_params"]

        # Call the audio.speech.create endpoint
        response = await self.async_client.audio.speech.create(
            model=audio_params.model.value,
            voice=audio_params.voice.value,
            input=text_input,
            response_format=audio_params.response_format.value,
        )

        duration = (time.time() - start_time) * 1000

        # Convert audio bytes to base64
        audio_bytes = response.read()
        audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")

        # Create AssistantMessage with audio data
        assistant_message = AssistantMessage(content=audio_base64)

        # Create ResponseMetadata (TTS doesn't provide token counts)
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=audio_params.model.value,
            input_tokens=0,  # TTS doesn't provide token counts
            output_tokens=0,
            stop_reason=StopReason.STOP,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

```

## FILE: src/conduit/core/clients/google/image_params.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from enum import Enum


class GoogleImageModel(str, Enum):
    """Available Google image generation models"""
    IMAGEN_3_FAST = "imagen-3.0-fast-generate-001"
    IMAGEN_3 = "imagen-3.0-generate-002"


class GoogleImageResponseFormat(str, Enum):
    """Response format for images"""
    URL = "url"
    B64_JSON = "b64_json"


class GoogleImageParams(BaseModel):
    """Parameters for Google image generation (Imagen)"""

    model: GoogleImageModel = Field(
        default=GoogleImageModel.IMAGEN_3,
        description="Imagen model to use"
    )
    response_format: GoogleImageResponseFormat = Field(
        default=GoogleImageResponseFormat.B64_JSON,
        description="Response format"
    )
    n: int = Field(
        default=1,
        ge=1,
        le=4,
        description="Number of images to generate (1-4)"
    )

```

## FILE: src/conduit/core/clients/google/message_adapter.py
```py
from conduit.core.clients.openai.message_adapter import convert_message_to_openai
from conduit.domain.message.message import Message
from typing import Any


def convert_message_to_google(message: Message) -> dict[str, Any]:
    return convert_message_to_openai(message)

```

## FILE: src/conduit/core/clients/google/payload.py
```py
from conduit.core.clients.openai.payload import OpenAIPayload


class GooglePayload(OpenAIPayload):
    frequency_penalty: None = None
    presence_penalty: None = None

```

## FILE: src/conduit/core/clients/google/tool_adapter.py
```py
from conduit.core.clients.openai.tool_adapter import convert_tool_to_openai
from conduit.capabilities.tools.tool import Tool
from typing import Any


def convert_tool_to_google(tool: Tool) -> dict[str, Any]:
    return convert_tool_to_openai(tool)

```

## FILE: src/conduit/core/clients/ollama/client.py
```py
"""
Client subclass for Ollama models.
This doesn't require an API key since these are locally hosted models.
We can use openai api calls to the ollama server, but we use the instructor library to handle the API calls.
This has special logic for updating the models.json file, since the available Ollama models will depend on what we have pulled.
We define preferred defaults for context sizes in a separate json file.
"""

from __future__ import annotations
from conduit.config import settings
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.core.clients.ollama.payload import OllamaPayload
from conduit.core.clients.ollama.message_adapter import convert_message_to_ollama
from conduit.core.clients.ollama.tool_adapter import convert_tool_to_ollama
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import AssistantMessage, ToolCall
from openai import AsyncOpenAI, AsyncStream
from collections import defaultdict
from typing import TYPE_CHECKING, override, Any
import instructor
from instructor import Instructor
import json
import logging
import time

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.message.message import Message

logger = logging.getLogger(__name__)


class OllamaClient(Client):
    """
    Client implementation for Ollama models using OpenAI-compatible endpoint.
    Async only.
    """

    def __init__(self):
        instructor_client, raw_client = self._initialize_client()
        self._client: Instructor = instructor_client
        self._raw_client: AsyncOpenAI = raw_client
        self.update_ollama_models()  # This allows us to keep the model file up to date.

    # Load Ollama context sizes from the JSON file
    try:
        settings.paths["OLLAMA_CONTEXT_SIZES_PATH"].parent.mkdir(
            parents=True, exist_ok=True
        )
        with open(settings.paths["OLLAMA_CONTEXT_SIZES_PATH"]) as f:
            _ollama_context_data = json.load(f)

        # Use defaultdict to set default context size to 4096 if not specified
        _ollama_context_sizes = defaultdict(lambda: 32768)
        _ollama_context_sizes.update(_ollama_context_data)
    except Exception:
        logger.warning(
            f"Could not load Ollama context sizes from {settings.paths['OLLAMA_CONTEXT_SIZES_PATH']}. Using default of 32768."
        )
        _ollama_context_sizes = defaultdict(lambda: 32768)

    @override
    def _initialize_client(self) -> tuple[Instructor, AsyncOpenAI]:
        """
        Creates both raw and instructor-wrapped clients.
        Raw client for standard completions, Instructor for structured responses.
        """
        raw_client = AsyncOpenAI(
            base_url="http://localhost:11434/v1",
            api_key="ollama",  # required but unused
        )
        instructor_client = instructor.from_openai(
            raw_client,
            mode=instructor.Mode.JSON,
        )
        return instructor_client, raw_client

    @override
    def _get_api_key(self) -> str:
        """
        Best thing about Ollama; no API key needed.
        """
        return ""

    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Converts a single internal Message DTO into Ollama's specific dictionary format.
        Since Ollama uses OpenAI spec, we delegate to the OpenAI adapter.
        """
        return convert_message_to_ollama(message)

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Translates the internal generic Request DTO into the specific
        dictionary parameters required by Ollama's SDK (via OpenAI spec).
        Ollama supports extra_body for additional configuration options.
        """
        # load client_params
        client_params = request.params.client_params or {}
        allowed_params = {"num_ctx"}
        for param in client_params:
            if param not in allowed_params:
                raise ValueError(f"Ollama does not support client param: {param}")
        # convert messages
        converted_messages = self._convert_messages(request.messages)

        # Convert tools and enable parallel tool calls if tools are present
        tools = None
        parallel_tool_calls = None
        if request.options.tool_registry:
            tools = [
                convert_tool_to_ollama(tool)
                for tool in request.options.tool_registry.tools
            ]
            parallel_tool_calls = request.options.parallel_tool_calls

        # build payload
        ollama_payload = OllamaPayload(
            model=request.params.model,
            messages=converted_messages,
            temperature=request.params.temperature,
            top_p=request.params.top_p,
            max_tokens=request.params.max_tokens,
            stream=request.params.stream,
            tools=tools,
            parallel_tool_calls=parallel_tool_calls,
            # ollama specific params (nested under extra_body)
            extra_body={**client_params} if client_params else None,
        )
        return ollama_payload

    def update_ollama_models(self):
        """
        Updates the list of Ollama models.
        We run is every time ollama is initialized.
        """
        import ollama

        # Ensure the directory exists
        settings.paths["OLLAMA_MODELS_PATH"].parent.mkdir(parents=True, exist_ok=True)
        # Lazy load ollama module
        ollama_models = [m["model"] for m in ollama.list()["models"]]
        ollama_model_dict = {"ollama": ollama_models}
        with open(settings.paths["OLLAMA_MODELS_PATH"], "w") as f:
            json.dump(ollama_model_dict, f)

    @override
    def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Count tokens using Ollama's API via the official library.
        We set "num_predict" to 0 so we only process the prompt/history and get the eval count.
        """
        import ollama

        # CASE 1: Raw String
        if isinstance(payload, str):
            response = ollama.generate(
                model=model,
                prompt=payload,
                options={"num_predict": 0},  # Minimal generation
            )
            return int(response.get("prompt_eval_count", 0))

        # CASE 2: Message History
        if isinstance(payload, list):
            # Convert internal Messages to OpenAI/Ollama compatible dicts
            messages_payload = [m.to_ollama() for m in payload]

            response = ollama.chat(
                model=model,
                messages=messages_payload,
                options={"num_predict": 0},
            )
            return int(response.get("prompt_eval_count", 0))

        raise ValueError("Payload must be string or Sequence[Message]")

    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResult:
        """
        Generate text using Ollama and return a GenerationResponse.

        Returns:
            - GenerationResponse object for successful non-streaming requests
            - AsyncStream object for streaming requests
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Use the raw client for standard completions
        result = await self._raw_client.chat.completions.create(**payload_dict)

        # Handle streaming response
        if isinstance(result, AsyncStream):
            # For streaming, return the AsyncStream object directly
            return result

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = result.model
        input_tokens = result.usage.prompt_tokens
        output_tokens = result.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(result.choices[0], "finish_reason"):
            finish_reason = result.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH
            if finish_reason == "tool_calls":
                stop_reason = StopReason.TOOL_CALLS
            elif finish_reason == "content_filter":
                stop_reason = StopReason.CONTENT_FILTER

        # Process tool calls if present
        if stop_reason == StopReason.TOOL_CALLS:
            # Handle tool calls - iterate through all parallel calls
            tool_calls = []
            for tool_call_data in result.choices[0].message.tool_calls:
                arguments_dict = json.loads(tool_call_data.function.arguments)

                tool_call = ToolCall(
                    id=tool_call_data.id,  # Use provider-supplied ID
                    type="function",
                    function_name=tool_call_data.function.name,
                    arguments=arguments_dict,
                    provider="ollama",  # Fixed: was incorrectly set to "google"
                    raw=tool_call_data.dict(),
                )
                tool_calls.append(tool_call)

            # Create AssistantMessage with all tool calls
            assistant_message = AssistantMessage(
                content=result.choices[0].message.content
                if hasattr(result.choices[0].message, "content")
                else "",
                tool_calls=tool_calls,
            )
        else:
            # Extract the text content
            content = result.choices[0].message.content
            assistant_message = AssistantMessage(content=content)

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate a structured response using Ollama's function calling and return a GenerationResponse.

        Returns:
            - GenerationResponse object with parsed structured data in AssistantMessage.parsed
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Make the API call with function calling using instructor client
        (
            user_obj,
            completion,
        ) = await self._client.chat.completions.create_with_completion(
            response_model=request.params.response_model, **payload_dict
        )

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = completion.model
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(completion.choices[0], "finish_reason"):
            finish_reason = completion.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH

        # Create AssistantMessage with parsed structured data
        assistant_message = AssistantMessage(
            content=completion.choices[0].message.content,
            parsed=user_obj,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

```

## FILE: src/conduit/core/clients/ollama/message_adapter.py
```py
from conduit.core.clients.openai.message_adapter import convert_message_to_openai
from conduit.domain.message.message import Message
from typing import Any


def convert_message_to_ollama(message: Message) -> dict[str, Any]:
    return convert_message_to_openai(message)

```

## FILE: src/conduit/core/clients/ollama/payload.py
```py
from conduit.core.clients.openai.payload import OpenAIPayload
from typing import Any


class OllamaPayload(OpenAIPayload):
    extra_body: dict[str, Any] | None = None

```

## FILE: src/conduit/core/clients/ollama/tool_adapter.py
```py
from conduit.core.clients.openai.tool_adapter import convert_tool_to_openai
from conduit.capabilities.tools.tool import Tool
from typing import Any


def convert_tool_to_ollama(tool: Tool) -> dict[str, Any]:
    return convert_tool_to_openai(tool)

```

## FILE: src/conduit/core/clients/openai/audio_params.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from enum import Enum


class OpenAITTSVoice(str, Enum):
    """Available voices for OpenAI TTS"""

    ALLOY = "alloy"
    ASH = "ash"
    BALLAD = "ballad"
    CORAL = "coral"
    ECHO = "echo"
    FABLE = "fable"
    ONYX = "onyx"
    NOVA = "nova"
    SAGE = "sage"
    SHIMMER = "shimmer"
    VERSE = "verse"


class OpenAITTSModel(str, Enum):
    """Available TTS models"""

    TTS_1 = "tts-1"
    TTS_1_HD = "tts-1-hd"
    GPT_4O_MINI_TTS = "gpt-4o-mini-tts"


class OpenAIAudioFormat(str, Enum):
    """Available audio formats"""

    MP3 = "mp3"
    WAV = "wav"
    OPUS = "opus"
    AAC = "aac"
    FLAC = "flac"
    PCM = "pcm"


class OpenAIAudioParams(BaseModel):
    """Parameters for OpenAI audio generation (TTS)"""

    voice: OpenAITTSVoice = Field(
        default=OpenAITTSVoice.ALLOY, description="Voice to use for TTS"
    )
    model: OpenAITTSModel = Field(
        default=OpenAITTSModel.TTS_1, description="TTS model to use"
    )
    response_format: OpenAIAudioFormat = Field(
        default=OpenAIAudioFormat.MP3, description="Audio format for output"
    )
    speed: float = Field(
        default=1.0, ge=0.25, le=4.0, description="Speed of speech (0.25-4.0)"
    )

```

## FILE: src/conduit/core/clients/openai/client.py
```py
from __future__ import annotations
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.core.clients.openai.payload import OpenAIPayload
from conduit.core.clients.openai.message_adapter import convert_message_to_openai
from conduit.core.clients.openai.tool_adapter import convert_tool_to_openai
from conduit.core.clients.openai.audio_params import OpenAIAudioParams
from conduit.core.clients.openai.image_params import OpenAIImageParams
from conduit.core.clients.openai.transcription_params import OpenAITranscriptionParams
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import (
    AssistantMessage,
    ImageOutput,
    ToolCall,
    Message,
)
from abc import ABC
from functools import cached_property
import logging
import os
import json
import time
import base64
from typing import TYPE_CHECKING, override, Any

if TYPE_CHECKING:
    from collections.abc import Sequence
    from instructor import Instructor
    from openai import AsyncOpenAI, AsyncStream
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.request.request import GenerationRequest

logger = logging.getLogger(__name__)


class OpenAIClient(Client, ABC):
    """
    Client implementation for OpenAI's API using the official OpenAI Python SDK and Instructor library.
    Async by default.
    """

    @cached_property
    def async_client(self) -> AsyncOpenAI:
        """
        Provides access to the raw AsyncOpenAI client for advanced use cases.
        """
        from openai import AsyncOpenAI

        async_client = AsyncOpenAI(api_key=self._get_api_key())
        return async_client

    @cached_property
    def instructor_client(self) -> Instructor:
        """
        Provides access to the Instructor client for advanced use cases.
        """
        import instructor

        instructor_client = instructor.from_openai(self.async_client)
        return instructor_client

    def _get_api_key(self) -> str:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set.")
        return api_key

    # Convert internal Message DTOs to OpenAI format
    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Converts a single internal Message DTO into OpenAI's specific dictionary format.
        """
        return convert_message_to_openai(message)

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Translates the internal generic Request DTO into the specific
        dictionary parameters required by OpenAI's SDK.
        """
        converted_messages = self._convert_messages(request.messages)

        # Convert tools and enable parallel tool calls if tools are present
        tools = None
        parallel_tool_calls = None
        if request.options.tool_registry:
            tools = [
                convert_tool_to_openai(tool)
                for tool in request.options.tool_registry.tools
            ]
            parallel_tool_calls = request.options.parallel_tool_calls

        openai_payload = OpenAIPayload(
            model=request.params.model,
            messages=converted_messages,
            temperature=request.params.temperature,
            top_p=request.params.top_p,
            max_tokens=request.params.max_tokens,
            stream=request.params.stream,
            tools=tools,
            parallel_tool_calls=parallel_tool_calls,
        )
        return openai_payload

    # Tokenization
    @override
    async def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Return the token count for a string or a message list.
        For Sequence[Message], it calculates the overhead per OpenAI ChatML format.

        Estimations:
        - Images: 85 tokens (low detail) or 765 tokens (high/auto detail estimate).
        - Tool Calls: Counts function name + serialized JSON arguments.
        """
        encoding = self._get_encoding(model)

        # CASE 1: Raw String
        if isinstance(payload, str):
            return len(encoding.encode(payload))

        # CASE 2: Message History (ChatML)
        elif isinstance(payload, list):
            num_tokens = 0

            # Constants for ChatML (gpt-3.5-turbo-0613 / gpt-4)
            # <|start|>{role/name}\n{content}<|end|>\n
            tokens_per_message = 3
            tokens_per_name = 1
            tokens_reply_primer = 3

            for message in payload:
                num_tokens += tokens_per_message

                # 1. Role
                # Handle Enum or string role
                role_str = (
                    message.role.value
                    if hasattr(message.role, "value")
                    else str(message.role)
                )
                num_tokens += len(encoding.encode(role_str))

                # 2. Content (Text or Multimodal)
                if message.content:
                    if isinstance(message.content, str):
                        num_tokens += len(encoding.encode(message.content))
                    elif isinstance(message.content, list):
                        for block in message.content:
                            # We use hasattr/getattr to support both Pydantic objects and dicts
                            block_type = getattr(block, "type", None)

                            # Text Block
                            if block_type == "text":
                                text = getattr(block, "text", "")
                                num_tokens += len(encoding.encode(text))

                            # Image Block (Estimation)
                            elif block_type == "image_url":
                                # Standard OpenAI Pricing:
                                # Low detail: 85 tokens
                                # High detail: 85 (base) + 170 per 512x512 tile.
                                # Without dimensions, we estimate a standard 1024x1024 (4 tiles) for High/Auto.
                                detail = getattr(block, "detail", "auto")
                                if detail == "low":
                                    num_tokens += 85
                                else:
                                    num_tokens += 765  # 85 + (4 * 170)

                # 3. Tool Calls (Assistant)
                if hasattr(message, "tool_calls") and message.tool_calls:
                    for call in message.tool_calls:
                        # Function name
                        num_tokens += len(encoding.encode(call.function_name))
                        # Arguments (JSON string)
                        # We dump the dict to a string to estimate tokens
                        args_str = json.dumps(call.arguments)
                        num_tokens += len(encoding.encode(args_str))

                # 4. Name (Optional override)
                if hasattr(message, "name") and message.name:
                    num_tokens += tokens_per_name
                    num_tokens += len(encoding.encode(message.name))

            # Add reply primer: <|start|>assistant<|message|>
            num_tokens += tokens_reply_primer
            return num_tokens

        raise ValueError("Payload must be string or Sequence[Message]")

    def _get_encoding(self, model: str):
        import tiktoken

        try:
            return tiktoken.encoding_for_model(model)
        except KeyError:
            return tiktoken.get_encoding("cl100k_base")

    def discover_models(self) -> list[str]:
        import openai

        API_KEY = self._get_api_key()
        openai.api_key = API_KEY
        models = openai.models.list()

        return [model.id for model in models.data]

    # Query methods
    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "image":
                return await self._generate_image(request)
            case "audio":
                return await self._generate_audio(request)
            case "transcription":
                return await self._transcribe_audio(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResult:
        """
        Generate text using OpenAI's API and return a ConduitResult.

        Returns:
            - Response object for successful non-streaming requests
            - AsyncStream object for streaming requests

        Also handles tooling calls if specified in the request.
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Use the standard completion method
        result = await self.async_client.chat.completions.create(**payload_dict)

        # Handle streaming response
        from openai import AsyncStream

        if isinstance(result, AsyncStream):
            # For streaming, return the AsyncStream object directly (it's part of ConduitResult)
            return result

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = result.model
        input_tokens = result.usage.prompt_tokens
        output_tokens = result.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(result.choices[0], "finish_reason"):
            finish_reason = result.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH
            elif finish_reason == "tool_calls":
                stop_reason = StopReason.TOOL_CALLS
            elif finish_reason == "content_filter":
                stop_reason = StopReason.CONTENT_FILTER

        if stop_reason == StopReason.TOOL_CALLS:
            # Handle tool calls - iterate through all parallel calls
            tool_calls = []
            for tool_call_data in result.choices[0].message.tool_calls:
                arguments_dict = json.loads(tool_call_data.function.arguments)

                tool_call = ToolCall(
                    id=tool_call_data.id,  # Use provider-supplied ID
                    type="function",
                    function_name=tool_call_data.function.name,
                    arguments=arguments_dict,
                    provider="openai",
                    raw=tool_call_data.dict(),
                )
                tool_calls.append(tool_call)

            # Create AssistantMessage with all tool calls
            assistant_message = AssistantMessage(
                content=result.choices[0].message.content
                if hasattr(result.choices[0].message, "content")
                else "",
                tool_calls=tool_calls,
            )
        else:
            # Extract the text content
            content = result.choices[0].message.content
            assistant_message = AssistantMessage(content=content)

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_image(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate an image using OpenAI's DALL-E and return a ConduitResult.

        Returns:
            - Response object with ImageOutput in AssistantMessage.images
        """
        start_time = time.time()

        # Extract text prompt from the last message
        last_message = request.messages[-1]
        if isinstance(last_message.content, str):
            prompt = last_message.content
        else:
            # Handle multimodal content - extract text
            prompt = " ".join(
                [block.text for block in last_message.content if hasattr(block, "text")]
            )

        # Get image parameters (use defaults if not provided)
        image_params = OpenAIImageParams()
        if (
            request.params.client_params
            and "image_params" in request.params.client_params
        ):
            image_params = request.params.client_params["image_params"]

        # Call the images.generate endpoint
        response = await self.async_client.images.generate(
            model=image_params.model.value,
            prompt=prompt,
            size=image_params.size.value,
            quality=image_params.quality.value,
            style=image_params.style.value,
            response_format=image_params.response_format.value,
            n=image_params.n,
        )

        duration = (time.time() - start_time) * 1000

        # Convert response to ImageOutput objects
        image_outputs = []
        for image_data in response.data:
            image_output = ImageOutput(
                url=image_data.url if hasattr(image_data, "url") else None,
                b64_json=image_data.b64_json
                if hasattr(image_data, "b64_json")
                else None,
                revised_prompt=image_data.revised_prompt
                if hasattr(image_data, "revised_prompt")
                else None,
            )
            image_outputs.append(image_output)

        # Create AssistantMessage with images
        assistant_message = AssistantMessage(images=image_outputs)

        # Create ResponseMetadata (DALL-E doesn't provide token counts)
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=image_params.model.value,
            input_tokens=0,  # DALL-E doesn't provide token counts
            output_tokens=0,
            stop_reason=StopReason.STOP,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_audio(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate audio using OpenAI's TTS API and return a ConduitResult.

        Returns:
            - Response object with base64-encoded audio data
        """
        start_time = time.time()

        # Extract text from the last message
        last_message = request.messages[-1]
        if isinstance(last_message.content, str):
            text_input = last_message.content
        else:
            # Handle multimodal content - extract text
            text_input = " ".join(
                [block.text for block in last_message.content if hasattr(block, "text")]
            )

        # Get audio parameters (use defaults if not provided)
        audio_params = OpenAIAudioParams()
        if (
            request.params.client_params
            and "audio_params" in request.params.client_params
        ):
            audio_params = request.params.client_params["audio_params"]

        # Call the audio.speech.create endpoint
        response = await self.async_client.audio.speech.create(
            model=audio_params.model.value,
            voice=audio_params.voice.value,
            input=text_input,
            response_format=audio_params.response_format.value,
            speed=audio_params.speed,
        )

        duration = (time.time() - start_time) * 1000

        # Convert audio bytes to base64
        audio_bytes = response.read()
        audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")

        # Create AssistantMessage with audio data
        assistant_message = AssistantMessage(content=audio_base64)

        # Create ResponseMetadata (TTS doesn't provide token counts)
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=audio_params.model.value,
            input_tokens=0,  # TTS doesn't provide token counts
            output_tokens=0,
            stop_reason=StopReason.STOP,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _transcribe_audio(self, request: GenerationRequest) -> GenerationResponse:
        """
        Transcribe audio using OpenAI's Whisper API and return a ConduitResult.

        Returns:
            - Response object with transcription text in AssistantMessage.content
        """
        start_time = time.time()

        # Extract audio data from the message
        last_message = request.messages[-1]
        audio_data = None
        audio_format = "mp3"

        if isinstance(last_message.content, list):
            # Find AudioContent in the message
            for block in last_message.content:
                if hasattr(block, "data") and hasattr(block, "format"):
                    audio_data = block.data
                    audio_format = block.format
                    break

        if not audio_data:
            raise ValueError("No audio content found in message")

        # Get transcription parameters (use defaults if not provided)
        transcription_params = OpenAITranscriptionParams()
        if (
            request.params.client_params
            and "transcription_params" in request.params.client_params
        ):
            transcription_params = request.params.client_params["transcription_params"]

        # Convert base64 audio to bytes
        import io

        audio_bytes = base64.b64decode(audio_data)
        audio_file = io.BytesIO(audio_bytes)
        audio_file.name = f"audio.{audio_format}"

        # Call the audio.transcriptions.create endpoint
        response = await self.async_client.audio.transcriptions.create(
            model=transcription_params.model.value,
            file=audio_file,
            language=transcription_params.language,
            prompt=transcription_params.prompt,
            response_format=transcription_params.response_format.value,
            temperature=transcription_params.temperature,
        )

        duration = (time.time() - start_time) * 1000

        # Extract transcription text
        if isinstance(response, str):
            transcription_text = response
        else:
            # For JSON format, extract text field
            transcription_text = response.text

        # Create AssistantMessage with transcription
        assistant_message = AssistantMessage(content=transcription_text)

        # Create ResponseMetadata (Whisper doesn't provide token counts)
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=transcription_params.model.value,
            input_tokens=0,  # Whisper doesn't provide token counts
            output_tokens=0,
            stop_reason=StopReason.STOP,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate a structured response using OpenAI's function calling and return a ConduitResult.

        Returns:
            - Response object with parsed structured data in AssistantMessage.parsed
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Make the API call with function calling
        (
            user_obj,
            completion,
        ) = await self.instructor_client.chat.completions.create_with_completion(
            response_model=request.params.response_model, **payload_dict
        )

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = completion.model
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(completion.choices[0], "finish_reason"):
            finish_reason = completion.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH
            elif finish_reason == "tool_calls":
                stop_reason = StopReason.TOOL_CALLS
            elif finish_reason == "content_filter":
                stop_reason = StopReason.CONTENT_FILTER

        # Construct the ToolCall object
        type = "function"  # Structured response must use function calling
        function_name = completion.choices[0].message.tool_calls[0].function.name
        arguments = completion.choices[0].message.tool_calls[0].function.arguments
        arguments_dict = json.loads(arguments)

        tool_call = ToolCall(
            type=type, function_name=function_name, arguments=arguments_dict
        )

        # Create AssistantMessage with parsed structured data
        assistant_message = AssistantMessage(
            content=completion.choices[0].message.content,
            tool_calls=[tool_call],
            parsed=user_obj,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

```

## FILE: src/conduit/core/clients/openai/image_params.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from enum import Enum


class OpenAIImageModel(str, Enum):
    """Available DALL-E models"""
    DALL_E_2 = "dall-e-2"
    DALL_E_3 = "dall-e-3"


class OpenAIImageSize(str, Enum):
    """Available image sizes"""
    SIZE_256 = "256x256"      # DALL-E 2 only
    SIZE_512 = "512x512"      # DALL-E 2 only
    SIZE_1024 = "1024x1024"   # Both models
    SIZE_1792_1024 = "1792x1024"  # DALL-E 3 only
    SIZE_1024_1792 = "1024x1792"  # DALL-E 3 only


class OpenAIImageQuality(str, Enum):
    """Image quality (DALL-E 3 only)"""
    STANDARD = "standard"
    HD = "hd"


class OpenAIImageStyle(str, Enum):
    """Image style (DALL-E 3 only)"""
    VIVID = "vivid"
    NATURAL = "natural"


class OpenAIImageResponseFormat(str, Enum):
    """Response format for images"""
    URL = "url"
    B64_JSON = "b64_json"


class OpenAIImageParams(BaseModel):
    """Parameters for OpenAI image generation (DALL-E)"""

    model: OpenAIImageModel = Field(
        default=OpenAIImageModel.DALL_E_3,
        description="DALL-E model to use"
    )
    size: OpenAIImageSize = Field(
        default=OpenAIImageSize.SIZE_1024,
        description="Size of generated image"
    )
    quality: OpenAIImageQuality = Field(
        default=OpenAIImageQuality.STANDARD,
        description="Image quality (DALL-E 3 only)"
    )
    style: OpenAIImageStyle = Field(
        default=OpenAIImageStyle.VIVID,
        description="Image style (DALL-E 3 only)"
    )
    response_format: OpenAIImageResponseFormat = Field(
        default=OpenAIImageResponseFormat.B64_JSON,
        description="Response format"
    )
    n: int = Field(
        default=1,
        ge=1,
        le=10,
        description="Number of images (DALL-E 2: 1-10, DALL-E 3: only 1)"
    )

```

## FILE: src/conduit/core/clients/openai/message_adapter.py
```py
from __future__ import annotations
from typing import Any
from conduit.domain.message.message import (
    Message,
    SystemMessage,
    UserMessage,
    AssistantMessage,
    ToolMessage,
    TextContent,
    ImageContent,
    AudioContent,
)
import json


def convert_message_to_openai(message: Message) -> dict[str, Any]:
    """
    Pure function to adapt an internal Message DTO into an OpenAI-compatible dictionary.
    Used by OpenAI, Ollama, Google, Perplexity, and Groq clients.
    """
    match message:
        # 1. System Message
        case SystemMessage(content=content):
            return {"role": "system", "content": content}

        # 2. User Message (Text or Multimodal)
        case UserMessage(content=content, name=name):
            payload = {"role": "user"}
            if name:
                payload["name"] = name

            if isinstance(content, str):
                payload["content"] = content
                return payload

            # Handle list of content blocks (Multimodal)
            openai_content = []
            for block in content:
                match block:
                    case TextContent(text=text):
                        openai_content.append({"type": "text", "text": text})
                    case ImageContent(url=url, detail=detail):
                        openai_content.append(
                            {
                                "type": "image_url",
                                "image_url": {"url": url, "detail": detail},
                            }
                        )
                    case AudioContent(data=data, format=fmt):
                        # GPT-4o-audio format for input audio
                        openai_content.append(
                            {
                                "type": "input_audio",
                                "input_audio": {"data": data, "format": fmt},
                            }
                        )
            payload["content"] = openai_content
            return payload

        # 3. Assistant Message (Text, Reasoning, Tools, Audio)
        case AssistantMessage(content=content, tool_calls=calls, audio=audio):
            payload = {"role": "assistant"}

            # OpenAI allows null content if tool_calls are present
            if content:
                payload["content"] = content
            elif not calls and not audio:
                # If everything is empty (rare), avoid sending null
                payload["content"] = ""

            # Handle Tool Calls
            if calls:
                payload["tool_calls"] = [
                    {
                        "id": call.id,
                        "type": "function",
                        "function": {
                            "name": call.function_name,
                            # OpenAI mandates arguments be a JSON string, not a dict
                            "arguments": json.dumps(call.arguments),
                        },
                    }
                    for call in calls
                ]

            # Handle Audio Response Context (sending audio back to model)
            if audio:
                payload["audio"] = {"id": audio.id}

            return payload

        # 4. Tool Result
        case ToolMessage(content=result, tool_call_id=call_id):
            return {"role": "tool", "content": result, "tool_call_id": call_id}

        case _:
            raise ValueError(
                f"Unknown message type for OpenAI Adapter: {type(message)}"
            )

```

## FILE: src/conduit/core/clients/openai/payload.py
```py
from pydantic import BaseModel, Field, ConfigDict
from typing import Any


class OpenAIPayload(BaseModel):
    """
    Anti-corruption Layer for OpenAI chat completions.
    Validates top-level configuration while allowing flexibility in message structure.
    Since we use OpenAI spec for ollama, Gemini, and Perplexity, we will inherit from this model.
    """

    model_config = ConfigDict(extra="allow")  # Allow extra fields in future

    # Required
    model: str
    messages: list[dict[str, Any]]

    # Optional
    temperature: float | None = Field(default=None, ge=0.0, le=2.0)
    top_p: float | None = Field(default=None, ge=0.0, le=1.0)
    max_tokens: int | None = None
    max_completion_tokens: int | None = None
    stream: bool | None = None
    frequency_penalty: float | None = Field(default=None, ge=-2.0, le=2.0)
    presence_penalty: float | None = Field(default=None, ge=-2.0, le=2.0)
    logit_bias: dict[str, float] | None = None
    stop: str | list[str] | None = None
    seed: int | None = None

    # Tools and output
    response_format: dict[str, Any] | None = None
    tools: list[dict[str, Any]] | None = None
    tool_choice: str | dict[str, Any] | None = None
    parallel_tool_calls: bool | None = None

```

## FILE: src/conduit/core/clients/openai/tool_adapter.py
```py
from conduit.capabilities.tools.tool import Tool


def convert_tool_to_openai(tool: Tool) -> dict:
    return {
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description,
            "parameters": tool.input_schema.model_dump(
                by_alias=True, exclude_none=True
            ),
        },
    }

```

## FILE: src/conduit/core/clients/openai/transcription_params.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from enum import Enum


class OpenAITranscriptionModel(str, Enum):
    """Available Whisper models"""

    WHISPER_1 = "whisper-1"


class OpenAITranscriptionResponseFormat(str, Enum):
    """Response formats for transcription"""

    JSON = "json"
    TEXT = "text"
    SRT = "srt"
    VERBOSE_JSON = "verbose_json"
    VTT = "vtt"


class OpenAITranscriptionParams(BaseModel):
    """Parameters for OpenAI audio transcription (Whisper)"""

    model: OpenAITranscriptionModel = Field(
        default=OpenAITranscriptionModel.WHISPER_1, description="Whisper model to use"
    )
    language: str | None = Field(
        default=None, description="ISO-639-1 language code (e.g., 'en', 'es', 'fr')"
    )
    prompt: str | None = Field(
        default=None,
        description="Optional text to guide the model's style or continue a previous segment",
    )
    response_format: OpenAITranscriptionResponseFormat = Field(
        default=OpenAITranscriptionResponseFormat.JSON,
        description="Format of the transcript output",
    )
    temperature: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Sampling temperature (0-1)"
    )

```

## FILE: src/conduit/core/clients/payload_base.py
```py
from conduit.core.clients.openai.payload import OpenAIPayload
from conduit.core.clients.ollama.payload import OllamaPayload
from conduit.core.clients.google.payload import GooglePayload
from conduit.core.clients.perplexity.payload import PerplexityPayload
from conduit.core.clients.anthropic.payload import AnthropicPayload

Payload = (
    AnthropicPayload | OllamaPayload | GooglePayload | PerplexityPayload | OpenAIPayload
)

```

## FILE: src/conduit/core/clients/perplexity/adapter.py
```py
from conduit.core.clients.openai.adapter import convert_message_to_openai
from conduit.domain.message.message import Message
from typing import Any


def convert_message_to_perplexity(message: Message) -> dict[str, Any]:
    return convert_message_to_openai(message)

```

## FILE: src/conduit/core/clients/perplexity/client.py
```py
"""
For Perplexity models.
NOTE: these use standard OpenAI SDK, the only difference in processing is that the response object has an extra 'citations' field.
You want both the 'content' and 'citations' fields from the response object.
"""

from __future__ import annotations
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.core.clients.perplexity.payload import PerplexityPayload
from conduit.core.clients.perplexity.message_adapter import convert_message_to_perplexity
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import AssistantMessage
from openai import AsyncOpenAI, AsyncStream
from openai.types.chat.chat_completion import ChatCompletion
from typing import TYPE_CHECKING, override, Any
import instructor
from instructor import Instructor
import os
import time

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.message.message import Message


class PerplexityClient(Client):
    """
    Client implementation for Perplexity API using OpenAI-compatible endpoint.
    Async only.
    """

    def __init__(self):
        instructor_client, raw_client = self._initialize_client()
        self._client: Instructor = instructor_client
        self._raw_client: AsyncOpenAI = raw_client

    @override
    def _initialize_client(self) -> tuple[Instructor, AsyncOpenAI]:
        """
        Creates both raw and instructor-wrapped clients.
        Raw client for standard completions, Instructor for structured responses.
        Uses instructor.from_perplexity() for Perplexity-specific handling.
        """
        raw_client = AsyncOpenAI(
            api_key=self._get_api_key(),
            base_url="https://api.perplexity.ai",
        )
        instructor_client = instructor.from_perplexity(raw_client)
        return instructor_client, raw_client

    @override
    def _get_api_key(self) -> str:
        api_key = os.getenv("PERPLEXITY_API_KEY")
        if not api_key:
            raise ValueError("PERPLEXITY_API_KEY environment variable not set.")
        else:
            return api_key

    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Converts a single internal Message DTO into Perplexity's specific dictionary format.
        Since Perplexity uses OpenAI spec, we delegate to the Perplexity adapter (which uses OpenAI).
        """
        return convert_message_to_perplexity(message)

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Translates the internal generic Request DTO into the specific
        dictionary parameters required by Perplexity's SDK (OpenAI-compatible).

        Perplexity-specific parameters:
        - return_citations: Enable citation return
        - return_images: Enable image return
        - search_recency_filter: Time-based filtering
        """
        # load client_params
        client_params = request.params.client_params or {}
        allowed_params = {"return_citations", "search_recency_filter"}
        for param in client_params.keys():
            if param not in allowed_params:
                raise ValueError(
                    f"Invalid client_param '{param}' for PerplexityClient. Allowed params: {allowed_params}"
                )
        # convert messages
        converted_messages = self._convert_messages(request.messages)
        # build payload
        perplexity_payload = PerplexityPayload(
            model=request.params.model,
            messages=converted_messages,
            temperature=request.params.temperature,
            top_p=request.params.top_p,
            max_tokens=request.params.max_tokens,
            stream=request.params.stream,
            # Perplexity-specific params
            **client_params,
        )
        return perplexity_payload

    @override
    def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Return the token count for a string, per model's tokenization function.
        cl100k_base is good enough for Perplexity approximation.
        """
        import tiktoken

        # Perplexity models are often Llama-based, but cl100k_base is the
        # standard fallback for OpenAI-compatible APIs in Python without
        # heavy `transformers` dependencies.
        encoding = tiktoken.get_encoding("cl100k_base")

        # CASE 1: Raw String
        if isinstance(payload, str):
            return len(encoding.encode(payload))

        # CASE 2: Message History
        if isinstance(payload, list):
            # Lazy import to avoid circular dependency
            from conduit.domain.message.textmessage import TextMessage

            # Standard OpenAI-compatible overhead approximation
            tokens_per_message = 3
            num_tokens = 0

            for message in payload:
                num_tokens += tokens_per_message

                # Role
                num_tokens += len(encoding.encode(message.role))

                # Content
                # Perplexity generally only handles Text.
                # If ImageMessage/AudioMessage are passed, we only count their text content.
                content_str = ""
                if hasattr(message, "text_content") and message.text_content:
                    content_str = message.text_content
                elif isinstance(message.content, str):
                    content_str = message.content
                elif isinstance(message.content, list):
                    # Handle complex content (list of BaseModels) by dumping to string
                    try:
                        content_str = json.dumps(
                            [m.model_dump() for m in message.content]
                        )
                    except AttributeError:
                        content_str = str(message.content)
                elif isinstance(message.content, BaseModel):
                    try:
                        content_str = message.content.model_dump_json()
                    except AttributeError:
                        content_str = str(message.content)

                num_tokens += len(encoding.encode(content_str))

            num_tokens += 3  # reply primer
            return num_tokens
        raise ValueError("Payload must be string or Sequence[Message]")

    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResult:
        """
        Generate text using Perplexity and return a GenerationResponse.
        Extracts citations from search_results and stores them in content dict.

        Returns:
            - GenerationResponse object for successful non-streaming requests
            - AsyncStream object for streaming requests
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Use the raw client for standard completions (to access citations)
        result = await self._raw_client.chat.completions.create(**payload_dict)

        # Handle streaming response
        if isinstance(result, AsyncStream):
            # For streaming, return the AsyncStream object directly
            return result

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = result.model
        input_tokens = result.usage.prompt_tokens
        output_tokens = result.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(result.choices[0], "finish_reason"):
            finish_reason = result.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH
            elif finish_reason == "content_filter":
                stop_reason = StopReason.CONTENT_FILTER

        # Extract citations from Perplexity response
        citations_raw = (
            result.search_results if hasattr(result, "search_results") else []
        )

        # Build content dict with text and citations (JSON-serializable)
        content_dict = {
            "text": result.choices[0].message.content,
            "citations": [
                {
                    "title": c.get("title", ""),
                    "url": c.get("url", ""),
                    "source": c.get("source"),
                    "date": c.get("date"),
                }
                for c in citations_raw
            ],
        }

        # Create AssistantMessage with dict content
        # The perplexity_content property will handle creating the rich object
        assistant_message = AssistantMessage(content=content_dict)

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate a structured response using Perplexity's function calling and return a GenerationResponse.
        Also extracts citations from search_results.

        Returns:
            - GenerationResponse object with parsed structured data in AssistantMessage.parsed
        """
        payload = self._convert_request(request)
        payload_dict = payload.model_dump(exclude_none=True)

        # Track timing
        start_time = time.time()

        # Make the API call with function calling using instructor client
        (
            user_obj,
            completion,
        ) = await self._client.chat.completions.create_with_completion(
            response_model=request.params.response_model, **payload_dict
        )

        # Assemble response metadata
        duration = (time.time() - start_time) * 1000  # Convert to milliseconds
        model_stem = completion.model
        input_tokens = completion.usage.prompt_tokens
        output_tokens = completion.usage.completion_tokens

        # Determine stop reason
        stop_reason = StopReason.STOP
        if hasattr(completion.choices[0], "finish_reason"):
            finish_reason = completion.choices[0].finish_reason
            if finish_reason == "length":
                stop_reason = StopReason.LENGTH

        # Extract citations from Perplexity response
        citations_raw = (
            completion.search_results if hasattr(completion, "search_results") else []
        )

        # Build content dict with citations only (structured response has no text)
        content_dict = {
            "citations": [
                {
                    "title": c.get("title", ""),
                    "url": c.get("url", ""),
                    "source": c.get("source"),
                    "date": c.get("date"),
                }
                for c in citations_raw
            ],
        }

        # Create AssistantMessage with parsed structured data and citations
        # The perplexity_content property will use parsed object's JSON as text
        assistant_message = AssistantMessage(
            content=content_dict,
            parsed=user_obj,
        )

        # Create ResponseMetadata
        metadata = ResponseMetadata(
            duration=duration,
            model_slug=model_stem,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            stop_reason=stop_reason,
        )

        # Create and return Response
        return GenerationResponse(
            message=assistant_message,
            request=request,
            metadata=metadata,
        )

```

## FILE: src/conduit/core/clients/perplexity/message_adapter.py
```py
from conduit.core.clients.openai.message_adapter import convert_message_to_openai
from conduit.domain.message.message import Message
from typing import Any


def convert_message_to_perplexity(message: Message) -> dict[str, Any]:
    return convert_message_to_openai(message)

```

## FILE: src/conduit/core/clients/perplexity/payload.py
```py
from conduit.core.clients.openai.payload import OpenAIPayload
from typing import Literal


class PerplexityPayload(OpenAIPayload):
    return_citations: bool | None = None
    return_images: bool | None = None
    search_recency_filter: Literal["month", "week", "day", "hour"] | None = None

```

## FILE: src/conduit/core/clients/perplexity/perplexity_content.py
```py
from pydantic import BaseModel
from typing import override


class PerplexityCitation(BaseModel):
    # Mandatory fields
    title: str
    url: str
    # Optional fields
    source: str | None = None
    date: str | None = None


class PerplexityContent(BaseModel):
    text: str
    citations: list[PerplexityCitation]

    @override
    def __str__(self) -> str:
        """
        Format the content and citations for display, in markdown format.
        """
        if not self.citations:
            return self.text
        citations_strs = []
        for citation in self.citations:
            citation_str = f"[{citation.title}]({citation.url})"
            if citation.date:
                citation_str += f" ({citation.date})"
            citations_strs.append(citation_str)
        citations_str = "\n".join(f"- {c}" for c in citations_strs)
        return f"{self.text}\n\n## Sources:\n{citations_str}"

```

## FILE: src/conduit/core/clients/remote/client.py
```py
from __future__ import annotations
from conduit.config import settings
from conduit.core.clients.client_base import Client
from conduit.core.clients.payload_base import Payload
from conduit.domain.result.response import GenerationResponse
from conduit.domain.result.response_metadata import ResponseMetadata, StopReason
from conduit.domain.message.message import AssistantMessage
from headwater_api.classes import StatusResponse
from headwater_client.client.headwater_client_async import HeadwaterAsyncClient
from typing import override, TYPE_CHECKING, Any
import json
import logging
import time
import asyncio

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.message.message import Message

logger = logging.getLogger(__name__)


class RemoteClient(Client):
    def __init__(self):
        self.is_healthy: None | bool = None
        self.status: None | StatusResponse = None
        # Internal cache storage
        self._cached_client: HeadwaterAsyncClient | None = None
        self._cached_loop: asyncio.AbstractEventLoop | None = None

    @property
    def _client(self) -> HeadwaterAsyncClient:
        """
        Lazily initialized HeadwaterAsyncClient.
        Smart caching: Re-initializes if the asyncio Event Loop has changed
        (which happens between calls in RemoteModelSync).
        """
        try:
            current_loop = asyncio.get_running_loop()
        except RuntimeError:
            # Should not happen inside async methods, but safe fallback
            current_loop = None

        # Check if we have a valid cached client for THIS loop
        if (
            self._cached_client
            and self._cached_loop is current_loop
            and current_loop is not None
            and not current_loop.is_closed()
        ):
            return self._cached_client

        # Initialize new client
        logger.debug("Initializing new HeadwaterAsyncClient (New Event Loop detected)")
        self._cached_client = self._initialize_client()
        self._cached_loop = current_loop
        return self._cached_client

    def _initialize_client(self) -> HeadwaterAsyncClient:
        """Initialize SiphonClient connection"""
        client = HeadwaterAsyncClient()
        return client

    async def _ping_server(self) -> bool:
        """Ping remote server to check health"""
        logger.debug("Pinging remote server...")
        is_healthy = await self._client.ping()
        return is_healthy

    async def _validate_server_model(self, model_name: str) -> bool:
        """
        Validate that the model is available on the server, IF not already in our canonical list of models.
        """
        from conduit.core.model.models.modelstore import ModelStore

        if model_name in ModelStore.cloud_models():
            logger.debug(f"Model '{model_name}' is a registered cloud model.")
            return True
        else:
            logger.debug(f"Validating model '{model_name}' on remote server.")
            available_models = getattr(self.status, "models_available", [])
            logger.info(f"Available models on server: {available_models}")
            # Update server models file
            if available_models:
                with open(settings.paths["SERVER_MODELS_PATH"], "w") as f:
                    json_dict = {"ollama": available_models}
                    _ = f.write(json.dumps(json_dict, indent=4))
                logger.debug(
                    f"Updated server models file at {settings.paths['SERVER_MODELS_PATH']}"
                )
            else:
                raise ValueError("No models available on server.")

            if model_name not in available_models:
                raise ValueError(
                    f"Model '{model_name}' not available on server. Available models: {available_models}"
                )
            else:
                logger.info(f"Model '{model_name}' is available on server.")
                return True

    @override
    def _convert_message(self, message: Message) -> dict[str, Any]:
        """
        Convert internal Message to format expected by remote server.
        TBD: Determine if server expects OpenAI format or custom format.
        """
        # TBD: Implementation needed - may delegate to message.to_openai() or custom format
        raise NotImplementedError("Remote message conversion logic needed")

    @override
    def _convert_request(self, request: GenerationRequest) -> Payload:
        """
        Convert GenerationRequest to format expected by remote server.
        TBD: May not need Payload if server expects raw GenerationRequest.
        """
        # TBD: Implementation needed - determine server's expected request format
        raise NotImplementedError("Remote request conversion logic needed")

    @override
    async def query(
        self,
        request: GenerationRequest,
    ) -> GenerationResult:
        """
        Query the remote model via HeadwaterClient.
        Routes to appropriate generation method based on output_type.
        """
        # Initial handshake / health check
        if self.is_healthy is None:  # First time check
            self.is_healthy = await self._ping_server()
            if self.is_healthy:
                self.status = await self._client.get_status()
        if not self.is_healthy:  # Subsequent checks
            raise ConnectionError("Cannot connect to remote server.")

        # Validate model availability on server
        _ = await self._validate_server_model(model_name=request.params.model)

        # Route to appropriate generation method
        match request.params.output_type:
            case "text":
                return await self._generate_text(request)
            case "image":
                return await self._generate_image(request)
            case "audio":
                return await self._generate_audio(request)
            case "structured_response":
                return await self._generate_structured_response(request)
            case _:
                raise ValueError(
                    f"Unsupported output type: {request.params.output_type}"
                )

    async def _generate_text(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate text via remote server.
        Server returns a complete GenerationResponse including tool calls.
        """
        response = await self._client.conduit.query_generate(request)

        # Server returns complete GenerationResponse with:
        # - message (AssistantMessage with tool_calls)
        # - request (original GenerationRequest)
        # - metadata (ResponseMetadata with stop_reason, tokens, etc.)
        return response

    async def _generate_image(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate image via remote server.
        TBD: Implement if remote server supports image generation.
        """
        raise NotImplementedError("Remote image generation not yet supported")

    async def _generate_audio(self, request: GenerationRequest) -> GenerationResponse:
        """
        Generate audio via remote server.
        TBD: Implement if remote server supports audio generation.
        """
        raise NotImplementedError("Remote audio generation not yet supported")

    async def _generate_structured_response(
        self, request: GenerationRequest
    ) -> GenerationResponse:
        """
        Generate structured response via remote server.
        TBD: Update HeadwaterClient API call for structured responses.
        """
        start_time = time.time()

        # Validate model availability on server
        _ = self._validate_server_model(model_name=request.params.model)

        # TBD: Update this to use async HeadwaterClient method with structured output
        raise NotImplementedError(
            "Remote structured response logic needed - update HeadwaterClient call"
        )

    @override
    async def tokenize(self, model: str, payload: str | Sequence[Message]) -> int:
        """
        Get the token count for a text, per a given model's tokenization function.
        If payload is a Sequence of Messages, we serialize to JSON to approximate the weight
        for the server-side tokenizer which expects a string.
        """
        from headwater_api.classes import TokenizationRequest

        _ = self._validate_server_model(model_name=model)

        if isinstance(payload, str):
            text = payload
        elif isinstance(payload, list):
            # text = json.dumps([self._convert_message(m) for m in payload])
            text = json.dumps([m.model_dump_json() for m in payload])
        else:
            raise ValueError("Payload must be string or Sequence[Message]")

        request = TokenizationRequest(model=model, text=text)
        # TBD: Update to async HeadwaterClient method
        # response = await self._client.conduit.tokenize_async(request)
        response = self._client.conduit.tokenize(request)
        token_count = response.token_count
        return token_count

    # Convenience methods (ping, status)
    async def ping(self) -> bool:
        """Ping the remote server to check connectivity."""
        return await self._ping_server()

    async def get_status(self) -> StatusResponse:
        """Get the status of the remote server."""
        if self.status is None:
            self.status = await self._client.get_status()
        return self.status

```

## FILE: src/conduit/core/conduit/batch/conduit_batch_async.py
```py
from __future__ import annotations
import asyncio
import logging
from typing import Any, override

from conduit.core.conduit.conduit_async import ConduitAsync
from conduit.core.prompt.prompt import Prompt
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

logger = logging.getLogger(__name__)


class ConduitBatchAsync:
    """
    Async implementation of Batch Conduit - a stateless execution engine.
    Handles concurrency control (semaphores) and batch strategy (Template vs String).
    """

    def __init__(self, prompt: Prompt | None = None):
        """
        Initialize with an optional prompt (required only for Template mode).
        """
        self.prompt: Prompt | None = prompt

    async def run(
        self,
        input_variables_list: list[dict[str, Any]] | None,
        prompt_strings_list: list[str] | None,
        params: GenerationParams,
        options: ConduitOptions,
        max_concurrent: int | None = None,
    ) -> list[Conversation]:
        """
        Execute the batch asynchronously.

        Args:
            input_variables_list: List of inputs for template rendering (Mode 1).
            prompt_strings_list: List of pre-rendered strings (Mode 2).
            params: Fully resolved generation parameters.
            options: Fully resolved conduit options.
            max_concurrent: Limit on concurrent tasks.

        Returns:
            list[Conversation]: Results in the same order as inputs.
        """
        # 1. Validate Mode
        if input_variables_list and prompt_strings_list:
            raise ValueError(
                "Provide exactly one of: input_variables_list OR prompt_strings_list"
            )
        if not input_variables_list and not prompt_strings_list:
            raise ValueError(
                "Must provide either input_variables_list or prompt_strings_list"
            )
        if input_variables_list and not self.prompt:
            raise ValueError(
                "input_variables_list mode requires a Prompt to be set on the instance"
            )

        logger.info("Running batch asynchronously.")

        # 2. Setup Concurrency
        semaphore = asyncio.Semaphore(max_concurrent) if max_concurrent else None

        # 3. Create Tasks
        tasks = []

        if input_variables_list:
            # Mode 1: Template mode - reuse one ConduitAsync with stored prompt
            # This is efficient as we don't re-parse the template every time
            conduit = ConduitAsync(self.prompt)
            tasks = [
                self._maybe_with_semaphore(
                    conduit.run(input_vars, params, options),
                    semaphore,
                )
                for input_vars in input_variables_list
            ]
            logger.info(
                f"Executing {len(tasks)} conversations in template mode with max_concurrent={max_concurrent or 'unlimited'}"
            )

        elif prompt_strings_list:
            # Mode 2: String mode - create temporary ConduitAsync for each string
            for prompt_str in prompt_strings_list:
                # We create a lightweight Prompt object for each string
                temp_conduit = ConduitAsync(Prompt(prompt_str))
                tasks.append(
                    self._maybe_with_semaphore(
                        # run with None for variables since prompt is pre-rendered
                        temp_conduit.run(None, params, options),
                        semaphore,
                    )
                )
            logger.info(
                f"Executing {len(tasks)} conversations in string mode with max_concurrent={max_concurrent or 'unlimited'}"
            )

        # 4. Execute
        conversations = await asyncio.gather(*tasks, return_exceptions=False)
        return list(conversations)

    async def _maybe_with_semaphore(
        self,
        coroutine: Any,
        semaphore: asyncio.Semaphore | None,
    ) -> Conversation:
        """
        Optionally wrap a coroutine with semaphore-based rate limiting.
        """
        if semaphore:
            async with semaphore:
                return await coroutine
        else:
            return await coroutine

    @override
    def __repr__(self) -> str:
        return f"ConduitBatchAsync(prompt={self.prompt!r})"

```

## FILE: src/conduit/core/conduit/batch/conduit_batch_sync.py
```py
from __future__ import annotations
import asyncio
import logging
from typing import Any, TYPE_CHECKING, override

from conduit.config import settings
from conduit.core.conduit.batch.conduit_batch_async import ConduitBatchAsync
from conduit.core.prompt.prompt import Prompt
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.utils.concurrency.warn import _warn_if_loop_exists

if TYPE_CHECKING:
    from rich.console import Console
    from conduit.utils.progress.verbosity import Verbosity

logger = logging.getLogger(__name__)


class ConduitBatchSync:
    """
    Synchronous, UX-focused wrapper for ConduitBatchAsync.

    - Holds *how* to process: Prompt, GenerationParams, ConduitOptions.
    - Takes *what* to process at call-time via inputs lists.
    - Manages state, configuration, and the event loop entry point.
    """

    def __init__(
        self,
        prompt: Prompt | None = None,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
    ):
        """
        Initialize the batch conduit.

        Args:
            prompt: Optional prompt template.
            params: Default LLM parameters.
            options: Default Runtime configuration.
        """
        # 1. Instantiate the async implementation
        self._impl = ConduitBatchAsync(prompt)

        # 2. Store execution context for UX
        self.params = params or settings.default_params()
        self.options = options or settings.default_conduit_options()

        logger.debug(
            f"ConduitBatchSync initialized with prompt={self._impl.prompt}, "
            f"params={self.params}, options={self.options}"
        )

    # Main entry point
    def run(
        self,
        input_variables_list: list[dict[str, Any]] | None = None,
        prompt_strings_list: list[str] | None = None,
        *,
        max_concurrent: int | None = None,
        cached: bool | None = None,
        persist: bool | None = None,
        verbosity: Verbosity | None = None,
        param_overrides: dict[str, Any] | None = None,
    ) -> list[Conversation]:
        """
        Execute batch processing synchronously.

        Args:
            input_variables_list: List of input variable dicts (requires Prompt).
            prompt_strings_list: List of pre-rendered prompt strings.
            max_concurrent: Max concurrent requests (for rate limiting).
            cached: Per-call override for caching.
            persist: Per-call override for persistence.
            verbosity: Per-call override for logging.
            param_overrides: Dict merged into GenerationParams.

        Returns:
            list[Conversation]: Completed conversations.
        """
        # 1. Build effective params/options from stored state + overrides
        effective_params = self._build_params(param_overrides)
        effective_options = self._build_options(
            cached=cached,
            persist=persist,
            verbosity=verbosity,
        )

        # 2. Delegate to async implementation, wrap in sync
        return self._run_sync(
            self._impl.run(
                input_variables_list=input_variables_list,
                prompt_strings_list=prompt_strings_list,
                params=effective_params,
                options=effective_options,
                max_concurrent=max_concurrent,
            )
        )

    # Factory
    @classmethod
    def create(
        cls,
        model: str,
        prompt: Prompt | str | None = None,
        *,
        project_name: str = settings.default_project_name,
        persist: bool | str = False,
        cached: bool | str = False,
        verbosity: Verbosity = settings.default_verbosity,
        console: Console | None = None,
        use_remote: bool = False,
        **param_kwargs: Any,
    ) -> "ConduitBatchSync":
        """
        Factory with sensible defaults.
        """
        # Prompt coercion
        prompt_obj = None
        if prompt is not None:
            if isinstance(prompt, str):
                prompt_obj = Prompt(prompt)
            elif isinstance(prompt, Prompt):
                prompt_obj = prompt
            else:
                raise TypeError(f"Unsupported prompt type: {type(prompt)}")

        # Params: seed with model + any extra params
        params = GenerationParams(model=model, **param_kwargs)

        # Options: start from global defaults
        options = settings.default_conduit_options(name=project_name)

        opt_updates: dict[str, Any] = {}

        if verbosity is not None:
            opt_updates["verbosity"] = verbosity

        if console is not None:
            opt_updates["console"] = console

        if cached:
            cache_name = cached if isinstance(cached, str) else project_name
            opt_updates["cache"] = settings.default_cache(name=cache_name)

        if persist:
            repo_name = persist if isinstance(persist, str) else project_name
            opt_updates["repository"] = settings.default_repository(name=repo_name)

        if use_remote:
            opt_updates["use_remote"] = True

        options = options.model_copy(update=opt_updates)

        return cls(prompt=prompt_obj, params=params, options=options)

    # Helpers
    def _build_params(self, param_overrides: dict[str, Any] | None) -> GenerationParams:
        """Build effective params by merging stored params with overrides."""
        if not param_overrides:
            return self.params

        updated_data = self.params.model_dump()
        updated_data.update(param_overrides)
        return GenerationParams(**updated_data)

    def _build_options(
        self,
        *,
        cached: bool | None,
        persist: bool | None,
        verbosity: Verbosity | None,
    ) -> ConduitOptions:
        """Build effective options by merging stored options with overrides."""
        # Start with stored options
        options = self.options or settings.default_conduit_options()
        opt_updates: dict[str, Any] = {}

        if verbosity is not None:
            opt_updates["verbosity"] = verbosity

        # Handle bool flags vs existing config objects
        if cached is not None:
            if not cached:
                opt_updates["cache"] = None
            elif options.cache is None:
                # Enable default cache if requested but not present
                opt_updates["cache"] = settings.default_cache(
                    name=settings.default_project_name
                )

        if persist is not None:
            if not persist:
                opt_updates["repository"] = None
            elif options.repository is None:
                # Enable default repo if requested but not present
                opt_updates["repository"] = settings.default_repository(
                    name=settings.default_project_name
                )

        # Apply updates non-destructively
        return options.model_copy(update=opt_updates)

    # Config methods - mutate stored options
    def enable_cache(self, name: str = settings.default_project_name) -> None:
        """Enable caching with specified name."""
        self.options.cache = settings.default_cache(name)
        logger.info(f"Enabled cache: {name}")

    def enable_repository(self, name: str = settings.default_project_name) -> None:
        """Enable persistence with specified name."""
        self.options.repository = settings.default_repository(name)
        logger.info(f"Enabled repository: {name}")

    def enable_console(self) -> None:
        """Enable rich console output."""
        if self.options.console is None:
            from rich.console import Console

            logger.info("Enabling console.")
            self.options.console = Console()

    def disable_cache(self) -> None:
        """Disable caching."""
        if self.options.cache is not None:
            logger.info("Disabling cache.")
            self.options.cache = None

    def disable_repository(self) -> None:
        """Disable persistence."""
        if self.options.repository is not None:
            logger.info("Disabling repository.")
            self.options.repository = None

    def disable_console(self) -> None:
        """Disable console output."""
        if self.options.console is not None:
            logger.info("Disabling console.")
            self.options.console = None

    # Async plumbing
    def _run_sync(self, coroutine: Any) -> Any:
        """Helper to run async methods synchronously."""
        _warn_if_loop_exists()
        try:
            return asyncio.run(coroutine)
        except KeyboardInterrupt:
            logger.warning("Batch operation cancelled by user.")
            raise

    @override
    def __repr__(self) -> str:
        return (
            f"ConduitBatchSync(prompt={self._impl.prompt!r}, "
            f"params={self.params!r}, options={self.options!r})"
        )

```

## FILE: src/conduit/core/conduit/conduit_async.py
```py
from __future__ import annotations
from conduit.core.conduit.conduit_base import ConduitBase
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from typing import override, TYPE_CHECKING, Any
import logging

if TYPE_CHECKING:
    from conduit.core.prompt.prompt import Prompt

logger = logging.getLogger(__name__)


class ConduitAsync(ConduitBase):
    """
    Async implementation of Conduit - a stateless "dumb pipe".
    Execution context (params/options) passed explicitly to run().
    """

    @override
    async def run(
        self,
        input_variables: dict[str, Any] | None,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> Conversation:
        """
        Execute the Conduit with explicit execution context.

        Args:
            input_variables: Template variables for the prompt
            params: Generation parameters (model, temperature, etc.)
            options: Conduit options (cache, repository, console, etc.)

        Returns:
            Conversation: The completed conversation after execution
        """
        # 1. Render prompt
        rendered = self._render_prompt(input_variables)

        # 2. Prepare conversation (may load from repository)
        conversation = self._prepare_conversation(rendered, params, options)

        # 3. Execute via Engine
        updated_conversation = await self.pipe(conversation, params, options)

        # 4. Save if repository is configured
        if options.repository:
            logger.info("Saving conversation to repository.")
            options.repository.save(updated_conversation)

        return updated_conversation

```

## FILE: src/conduit/core/conduit/conduit_base.py
```py
from __future__ import annotations
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.message.message import UserMessage
from conduit.core.prompt.prompt import Prompt
from typing import Any, override
import logging

logger = logging.getLogger(__name__)


class ConduitBase:
    """
    Stem class for Conduit implementations; not to be used directly.
    Holds shared logic for conversation orchestration via Engine.
    """

    def __init__(self, prompt: Prompt):
        """
        Initialize the Conduit base with only its identity.

        Args:
            prompt: The template/prompt configuration (the unique identity of this Conduit)
        """
        self.prompt: Prompt = prompt

    # Core delegation (the "dumb pipe")
    async def pipe(
        self,
        conversation: Conversation,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> Conversation:
        """
        Given a conversation, execute it using Engine.
        This is the core delegation point - analogous to Model.pipe().
        """
        from conduit.core.engine.engine import Engine

        return await Engine.run(conversation, params, options)

    # Pure CPU helper methods
    def _render_prompt(self, input_variables: dict[str, Any] | None) -> str:
        """
        PURE CPU: Render the prompt template with input variables.
        """
        if input_variables:
            self.prompt.validate_input_variables(input_variables)
            return self.prompt.render(input_variables=input_variables)
        return self.prompt.prompt_string

    def _prepare_conversation(
        self, rendered_prompt: str, params: GenerationParams, options: ConduitOptions
    ) -> Conversation:
        """
        PURE CPU: Build initial conversation object.
        Load from repository if enabled and last conversation exists.
        """
        from conduit.storage.repository.persistence_mode import PersistenceMode

        # Load from repository if persistence is enabled
        if options.repository and options.repository.last:
            logger.info("Loading last conversation from repository.")
            conversation = options.repository.last
            if options.persistence_mode == PersistenceMode.OVERWRITE:
                logger.info("Overwriting conversation as per persistence_mode.")
                conversation.messages = []
            if params.system:
                conversation.ensure_system_message(params.system)
        else:
            logger.info("Creating new conversation.")
            conversation = Conversation()

        # Add rendered prompt as UserMessage
        conversation.add(UserMessage(content=rendered_prompt))
        return conversation

    # Abstract methods (must be implemented by subclasses)
    async def run(
        self,
        input_variables: dict[str, Any] | None,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> Conversation:
        """
        Main entry point for executing the Conduit.
        Must be implemented by subclasses (sync or async).

        Args:
            input_variables: Template variables for the prompt
            params: Generation parameters (model, temperature, etc.)
            options: Conduit options (cache, repository, console, etc.)

        Returns:
            Conversation: The completed conversation after execution
        """
        raise NotImplementedError(
            "run must be implemented in subclasses (sync or async)."
        )

    # Dunders
    @override
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(prompt={self.prompt!r})"

```

## FILE: src/conduit/core/conduit/conduit_sync.py
```py
from __future__ import annotations

import asyncio
import logging
from typing import Any, TYPE_CHECKING, override

from conduit.config import settings
from conduit.core.conduit.conduit_async import ConduitAsync
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.core.prompt.prompt import Prompt
from conduit.utils.concurrency.warn import _warn_if_loop_exists

if TYPE_CHECKING:
    from rich.console import Console
    from conduit.utils.progress.verbosity import Verbosity

logger = logging.getLogger(__name__)


class ConduitSync:
    """
    Synchronous, UX-focused wrapper for ConduitAsync.

    - Holds *how* to process: Prompt, GenerationParams, ConduitOptions.
    - Takes *what* to process at call-time via input variables.
    - Designed for scripts and REPL usage where managing state and event loops
      is unnecessary overhead.
    """

    def __init__(
        self,
        prompt: Prompt,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
    ):
        # 1. Instantiate the async implementation (dumb pipe - only needs prompt)
        self._impl = ConduitAsync(prompt)

        # 2. Store execution context for UX
        self.params = params or settings.default_params()
        self.options = options or settings.default_conduit_options()

    # Sugar: payload-only
    def __call__(self, **input_variables: Any) -> Conversation:
        """
        Syntactic sugar for `run(input_variables=...)`.

        IMPORTANT: this is *payload-only*. No params/options knobs here;
        those live on `run()`.
        """
        return self.run(input_variables=input_variables)

    # Main entry point
    def run(
        self,
        input_variables: dict[str, Any] | None = None,
        *,
        cached: bool | None = None,
        persist: bool | None = None,
        verbosity: Verbosity | None = None,
        param_overrides: dict[str, Any] | None = None,
    ) -> Conversation:
        """
        Execute the configured Conduit synchronously.

        Args:
            input_variables:
                Template variables for the prompt string.
            cached:
                Per-call override for caching:
                - None: keep instance-level option
                - False: disable cache
                - True: enable cache (using a default cache if none configured)
            persist:
                Per-call override for persistence:
                - None: keep instance-level option
                - False: disable repository
                - True: enable repository (using a default repo if none configured)
            verbosity:
                Per-call override for progress / logging verbosity.
            param_overrides:
                Dict of fields to merge into GenerationParams
                (e.g. {"temperature": 0.9}).

        Returns:
            Conversation: the final conversation after Engine.run.
        """
        # 1) Build effective params/options from stored state + overrides
        effective_params = self._build_params(param_overrides)
        effective_options = self._build_options(
            cached=cached,
            persist=persist,
            verbosity=verbosity,
        )

        # 2) Delegate to async implementation, wrap in sync
        return self._run_sync(
            self._impl.run(
                input_variables=input_variables,
                params=effective_params,
                options=effective_options,
            )
        )

    # Factory
    @classmethod
    def create(
        cls,
        model: str,
        prompt: Prompt | str,
        *,
        project_name: str = settings.default_project_name,
        persist: bool | str = False,
        cache: bool | str = False,
        verbosity: Verbosity = settings.default_verbosity,
        console: Console | None = None,
        system: str | None = None,  # placeholder for future system-message wiring
        debug_payload: bool = False,
        use_remote: bool = False,
        **param_kwargs: Any,
    ) -> ConduitSync:
        """
        Factory with sensible defaults.

        - `model`: required model name/alias (used for GenerationParams.model).
        - `prompt`: str or Prompt.
        - `param_kwargs`: go directly into GenerationParams(...).
        - `cache` / `persist` / `verbosity` / `console`:
            baked into ConduitOptions as baseline behavior.
        """
        # Prompt coercion
        if isinstance(prompt, str):
            prompt_obj = Prompt(prompt)
        elif isinstance(prompt, Prompt):
            prompt_obj = prompt
        else:
            raise TypeError(f"Unsupported prompt type: {type(prompt)}")

        # Params: seed with model + any extra params
        params = GenerationParams(model=model, **param_kwargs, system=system)

        # Options: start from global defaults
        options = settings.default_conduit_options()

        # Collect overrides
        opt_updates = {"verbosity": verbosity}

        if console is not None:
            opt_updates["console"] = console

        # Cache wiring
        if cache:
            cache_name = cache if isinstance(cache, str) else project_name
            opt_updates["cache"] = settings.default_cache(project_name=cache_name)

        # Persistence wiring
        if persist:
            repo_name = persist if isinstance(persist, str) else project_name
            opt_updates["repository"] = settings.default_repository(
                project_name=repo_name
            )

        # Debug
        if debug_payload:
            opt_updates["debug_payload"] = True

        # Remote execution
        if use_remote:
            opt_updates["use_remote"] = True

        # Apply updates (Pydantic v2)
        options = options.model_copy(update=opt_updates)

        return cls(prompt=prompt_obj, params=params, options=options)

    # Helpers
    def _build_params(self, param_overrides: dict[str, Any] | None) -> GenerationParams:
        base = self.params or settings.default_params
        if not param_overrides:
            return base
        return base.model_copy(update=param_overrides)

    def _build_options(
        self,
        *,
        cached: bool | None,
        persist: bool | None,
        verbosity: Verbosity | None,
    ) -> ConduitOptions:
        opts = self.options or settings.default_conduit_options()

        if verbosity is not None:
            opts.verbosity = verbosity

        # Simple enable/disable knobs; names are handled in `create`
        if cached is not None:
            if not cached:
                opts.cache = None
            elif opts.cache is None:
                opts.cache = settings.default_cache(
                    project_name=settings.default_project_name
                )

        if persist is not None:
            if not persist:
                opts.repository = None
            elif opts.repository is None:
                opts.repository = settings.default_repository(
                    project_name=settings.default_project_name
                )

        return opts

    # Config methods - mutate stored options
    def enable_cache(self, name: str = settings.default_project_name) -> None:
        """Enable caching with specified name."""
        self.options.cache = settings.default_cache(name)
        logger.info(f"Enabled cache: {name}")

    def enable_repository(self, name: str = settings.default_project_name) -> None:
        """Enable persistence with specified name."""
        self.options.repository = settings.default_repository(name)
        logger.info(f"Enabled repository: {name}")

    def enable_console(self) -> None:
        """Enable rich console output."""
        if self.options.console is None:
            from rich.console import Console

            logger.info("Enabling console.")
            self.options.console = Console()

    def disable_cache(self) -> None:
        """Disable caching."""
        if self.options.cache is not None:
            logger.info("Disabling cache.")
            self.options.cache = None

    def disable_repository(self) -> None:
        """Disable persistence."""
        if self.options.repository is not None:
            logger.info("Disabling repository.")
            self.options.repository = None

    def disable_console(self) -> None:
        """Disable console output."""
        if self.options.console is not None:
            logger.info("Disabling console.")
            self.options.console = None

    # Async plumbing
    def _run_sync(self, coroutine: Any) -> Any:
        _warn_if_loop_exists()
        try:
            return asyncio.run(coroutine)
        except KeyboardInterrupt:
            logger.warning("Operation cancelled by user.")
            raise

    def __getattr__(self, name: str) -> object:
        """
        Proxy attribute access to the underlying ConduitAsync instance.
        Allows access to properties like self.prompt, etc.
        """
        return getattr(self._impl, name)

    @override
    def __repr__(self) -> str:
        return (
            f"ConduitSync(prompt={self._impl.prompt!r}, "
            f"params={self.params!r}, options={self.options!r})"
        )

```

## FILE: src/conduit/core/conduit/old_conduit_tool.py
```py
"""
We will have async by default; we can also create a Sync (for simple one-off queries) with:
```python
async def one_off_query(): ...

asyncio.run(one_off_query())
```
"""

from conduit.config import settings
from conduit.capabilities.tools.tools.fetch_url import FetchUrlTool
from conduit.capabilities.tools.parsing_implementation import is_tool_call, execute_tool_call
from conduit.capabilities.tools.registry import ToolRegistry
from conduit.core.parser.stream.parsers import XMLStreamParser
from conduit.domain.message.textmessage import TextMessage
from conduit.core.model.model_async import ModelAsync
from conduit.core.prompt.prompt import Prompt
from pathlib import Path
import asyncio

SYSTEM_PROMPT_PATH = (
    Path(__file__).parent.parent / "tools" / "prompts" / "system_prompt_template.jinja2"
)
PREFERRED_MODEL = settings.preferred_model


def generate_system_prompt(registry: ToolRegistry) -> str:
    if len(registry.tools) == 0:
        raise ValueError("No tools registered in the registry.")
    system_prompt_template = SYSTEM_PROMPT_PATH.read_text()
    # Input variables
    input_variables = {
        "system_prompt": settings.system_prompt,
        "tools_schema": registry.xml_schema,
        "example_call": registry.tools[0].xml_example,
    }
    system_prompt = Prompt(system_prompt_template)
    rendered = system_prompt.render(input_variables=input_variables)
    return rendered


async def main():
    registry = ToolRegistry()
    registry.register(FetchUrlTool)
    model = ModelAsync(PREFERRED_MODEL)
    system_prompt = generate_system_prompt(registry)
    system_message = TextMessage(role="system", content=system_prompt)
    prompt_str = "Summarize this article: https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/"
    user_message = TextMessage(role="user", content=prompt_str)
    messages = [system_message, user_message]
    stream = await model.query(query_input=messages, stream=True)
    parser = XMLStreamParser(stream, tag_name="tool_call")
    text_content, xml_object, full_buffer = await parser.parse_async(
        close_on_match=True
    )
    if xml_object:
        print(f"Extracted XML: {xml_object}")
    print(f"Remaining Text: {text_content}")
    # Async

    if is_tool_call(xml_object):
        print(f"Is tool call: {True}")
        # Execute the tool call (Note: this is async method)
        result = await execute_tool_call(xml_object, registry)
        print(f"Tool Execution Result: {result}")


asyncio.run(main())


"""
# Execute a tool call (typically parsed from LLM output)
result = registry.parse_and_execute(
    tool_name="file_read",
    parameters={"path": "/path/to/file.txt"}
)

# Parses the stream, stopping and closing connection once the full XML tag is found
text_content, xml_object, full_buffer = parser.parse(close_on_match=True)
"""

```

## FILE: src/conduit/core/engine/__init__.py
```py
from .engine import Engine

__all__ = ["Engine"]

```

## FILE: src/conduit/core/engine/engine.py
```py
"""
Our Engine takes a Conversation and runs it through an LLM or other processing.
Think of it as a FSM that processes the conversation.
LLMs produce the next token; Conduit produces the next Message.
"""

from __future__ import annotations
from conduit.domain.conversation.conversation import (
    Conversation,
    ConversationState,
)
from conduit.domain.exceptions.exceptions import EngineError
from typing import TYPE_CHECKING
import logging

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from conduit.domain.request.generation_params import GenerationParams
    from conduit.domain.config.conduit_options import ConduitOptions


class Engine:
    @staticmethod
    async def run(
        conversation: Conversation,
        params: GenerationParams,
        options: ConduitOptions,
        max_steps: int = 10,  # Safety limit for auto-looping
    ) -> Conversation:
        step_count = 0

        while step_count < max_steps:
            if not conversation.last:
                raise EngineError("Conversation is empty.")

            match conversation.state:
                # 1. LLM Generation
                case ConversationState.GENERATE:
                    conversation = await Engine._generate(conversation, params, options)

                # 2. Tool Execution (The Loop back)
                case ConversationState.EXECUTE:
                    conversation = await Engine._execute(conversation, params, options)

                # 3. Stop Conditions
                case ConversationState.TERMINATE:
                    return conversation

                case ConversationState.INCOMPLETE:
                    raise EngineError("Conversation is incomplete.")

            step_count += 1

        # If we exit the loop, we hit the limit
        logger.warning(f"Engine hit max_steps ({max_steps}). returning current state.")
        return conversation

    # Handlers
    @staticmethod
    async def _generate(
        conversation: Conversation,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> Conversation:
        from conduit.core.engine.generate import generate

        return await generate(conversation, params, options)

    @staticmethod
    async def _execute(
        conversation: Conversation,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> Conversation:
        from conduit.core.engine.execute import execute

        return await execute(conversation, params, options)

```

## FILE: src/conduit/core/engine/execute.py
```py
"""
Execute a Tool Call.
"""

from __future__ import annotations
from conduit.domain.exceptions.exceptions import EngineError
from conduit.domain.message.message import ToolMessage
from typing import TYPE_CHECKING
import logging

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from conduit.domain.request.generation_params import GenerationParams
    from conduit.domain.config.conduit_options import ConduitOptions
    from conduit.domain.conversation.conversation import Conversation


async def execute(
    conversation: Conversation, params: GenerationParams, options: ConduitOptions
) -> Conversation:
    # Unpack params
    _ = params  # Unused parameter
    if not options.tool_registry:
        raise EngineError("Tool registry is not configured in options.")
    tool_registry = options.tool_registry
    tool_calls = conversation.tool_calls
    if len(tool_calls) == 0:
        raise EngineError("No tool calls found in the conversation.")
    # Execute each tool call and add the result to the conversation
    logger.debug(f"Executing {len(tool_calls)} tool calls.")
    for index, tool_call in enumerate(tool_calls):
        # Execute the tool call
        logger.debug(f"Executing tool call {index + 1}/{len(tool_calls)}: {tool_call}")
        content = await tool_registry.call_tool(tool_call)
        tool_call_id = tool_call.id
        name = tool_call.function_name
        # Create a ToolMessage from the result and add it to the conversation
        tool_message = ToolMessage.from_result(
            result=content,
            tool_call_id=tool_call_id,
            name=name,
        )
        conversation.add(tool_message)
    logger.debug("Finished executing tool calls.")
    return conversation

```

## FILE: src/conduit/core/engine/generate.py
```py
"""
We need request params.
How does request params differ from request class?
"""

from conduit.domain.conversation.conversation import Conversation
from conduit.domain.result.response import GenerationResponse
from conduit.domain.request.request import GenerationRequest
from conduit.core.model.model_async import ModelAsync
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.domain.message.message import Message


async def generate(
    conversation: Conversation, params: GenerationParams, options: ConduitOptions
) -> Conversation:
    # 1. Create a mutable copy of the messages to avoid side effects.
    messages = list(conversation.messages)

    # 2. Ensure system prompt is correctly placed.
    has_system_message = any(msg.role == "system" for msg in messages)
    if not has_system_message and params.system:
        # Prepend a new SystemMessage if one doesn't exist and is provided in params.
        from conduit.domain.message.message import SystemMessage
        messages.insert(0, SystemMessage(content=params.system))

    # 3. Prepare and execute the request.
    # We create a temporary request object with the potentially modified message list.
    request = GenerationRequest(messages=messages, params=params, options=options)
    
    model = ModelAsync(params.model)
    response = await model.pipe(request)
    
    assert isinstance(response, GenerationResponse), (
        "Expected response to be of type Response"
    )

    # 4. Append the new assistant message to our copied list.
    new_message: Message = response.message
    messages.append(new_message)
    
    # 5. Return a new Conversation object, preserving the original's topic/id.
    return conversation.model_copy(update={"messages": messages})

```

## FILE: src/conduit/core/engine/terminate.py
```py
"""
Display to user.
"""

from conduit.domain.conversation.conversation import Conversation


def terminate(conversation: Conversation) -> Conversation:
    raise NotImplementedError("Termination handling is not yet implemented.")

```

## FILE: src/conduit/core/model/modalities/audio.py
```py
from __future__ import annotations
from conduit.core.model.model_async import ModelAsync
from conduit.domain.result.response import GenerationResponse
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.request.request import GenerationRequest
from conduit.domain.message.message import (
    UserMessage,
    TextContent,
    AudioContent,
)
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal

if TYPE_CHECKING:
    from conduit.core.model.model_sync import ModelSync
    from conduit.domain.result.response import GenerationResponse


AudioInput = str | bytes | AudioContent | Path


class AudioAsync:
    def __init__(self, parent: ModelAsync):
        self._parent: ModelAsync = parent

    def coerce_audio_input(self, audio: AudioInput) -> AudioContent:
        if isinstance(audio, AudioContent):
            return audio

        # Use Path object for cleaner checks
        path = Path(audio) if isinstance(audio, (str, Path)) else None

        if path and path.is_file():
            return AudioContent.from_file(path)

        if isinstance(audio, bytes):
            import base64

            return AudioContent(
                data=base64.b64encode(audio).decode("utf-8"), format="mp3"
            )

        if isinstance(audio, str):
            # Assume it's already base64 if it's not a file
            return AudioContent(data=audio, format="mp3")

        raise ValueError(f"Cannot coerce {type(audio)} to AudioContent")

    async def generate(
        self,
        prompt_str: str,
        voice: str = "alloy",
        format: str = "mp3",
        speed: float = 1.0,
    ) -> GenerationResponse:
        """
        Models: gpt-4o-mini-tts, gpt-4o-tts
        """
        # Construct params
        client_params = {
            "voice": voice,
            "response_format": format,
            "speed": speed,
        }
        params = GenerationParams(
            output_type="audio",
            model=self._parent.model_name,
            client_params=client_params,
        )
        options = ConduitOptions(project_name="test")

        # Construct messages
        user_message = UserMessage(content=prompt_str)
        messages = [user_message]
        request = GenerationRequest(messages=messages, params=params, options=options)

        response: GenerationResponse = await self._parent.pipe(request)
        return response

    async def analyze(self, prompt_str: str, audio: AudioInput) -> GenerationResponse:
        """
        Models: gpt-4o-audio-preview
        """
        params = GenerationParams(
            output_type="text",
            model=self._parent.model_name,
        )
        options = ConduitOptions(project_name="test")
        # Coerce audio input
        audio_content: AudioContent = self.coerce_audio_input(audio)
        text_content = TextContent(text=prompt_str)
        user_message = UserMessage(content=[text_content, audio_content])
        messages = [user_message]
        request = GenerationRequest(messages=messages, params=params, options=options)
        response: GenerationResponse = await self._parent.pipe(request)
        return response

    async def transcribe(
        self, audio: str | bytes | AudioContent | Path
    ) -> GenerationResponse:
        """
        Models: whisper-1
        """
        params = GenerationParams(
            output_type="transcription",
            model=self._parent.model_name,
        )
        options = ConduitOptions(project_name="test")
        # Coerce audio input
        audio_content: AudioContent = self.coerce_audio_input(audio)
        user_message = UserMessage(content=[audio_content])
        messages = [user_message]
        request = GenerationRequest(messages=messages, params=params, options=options)
        response: GenerationResponse = await self._parent.pipe(request)
        return response


class AudioSync:
    def __init__(self, parent: ModelSync):
        self._parent = parent
        self._impl = AudioAsync(parent._impl)

    def generate(
        self,
        prompt_str: str,
        voice: str = "alloy",
        format: str = "mp3",
        speed: float = 1.0,
    ) -> GenerationResponse:
        return self._parent._run_sync(
            self._impl.generate(
                prompt_str=prompt_str,
                voice=voice,
                format=format,
                speed=speed,
            )
        )

    def analyze(self, prompt_str: str, audio: AudioInput) -> GenerationResponse:
        return self._parent._run_sync(
            self._impl.analyze(prompt_str=prompt_str, audio=audio)
        )

    def transcribe(
        self, audio: str | bytes | AudioContent | Path
    ) -> GenerationResponse:
        return self._parent._run_sync(self._impl.transcribe(audio=audio))


if __name__ == "__main__":
    from conduit.examples.sample_objects import sample_audio_file
    import asyncio

    def test_audio_generate():
        model = ModelAsync("gpt-4o-mini-tts")
        audio = AudioAsync(model)

        async def _main():
            response = await audio.generate(
                prompt_str="Hello, this is a test of OpenAI's text to speech capabilities.",
                voice="alloy",
                format="mp3",
                speed=1.0,
            )
            return response

        result = asyncio.run(_main())
        print(result)

    def test_audio_analyze():
        model = ModelAsync("gpt-4o-audio-preview")
        audio = AudioAsync(model)

        async def _main():
            response = await audio.analyze(
                prompt_str="Please transcribe the following audio.",
                audio=sample_audio_file,
            )
            return response

        result = asyncio.run(_main())
        print(result)

    def test_audio_transcribe():
        model = ModelAsync("whisper-1")
        audio = AudioAsync(model)

        async def main():
            response = await audio.transcribe(
                audio=sample_audio_file,
            )
            return response

        response = asyncio.run(main())
        print(response)

```

## FILE: src/conduit/core/model/modalities/image.py
```py
from __future__ import annotations
from conduit.core.model.model_async import ModelAsync
from conduit.domain.result.response import GenerationResponse
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.request.request import GenerationRequest
from conduit.domain.message.message import (
    UserMessage,
    TextContent,
    ImageContent,
)
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal

if TYPE_CHECKING:
    from conduit.core.model.model_sync import ModelSync


ImageInput = str | bytes | ImageContent | Path


class ImageAsync:
    def __init__(self, parent: ModelAsync):
        self._parent: ModelAsync = parent

    def coerce_image_input(self, image: ImageInput) -> ImageContent:
        """
        Standardizes various input types into an ImageContent DTO.
        Early-exit strategy avoids redundant reads and complex nesting.
        """
        # 1. Direct Pass-through
        if isinstance(image, ImageContent):
            return image

        # 2. Path/File Detection
        # Path objects or strings that look like local file paths
        is_path = isinstance(image, (Path, str))
        if is_path:
            p = Path(image)
            # Check if it's a file before doing anything expensive
            if (
                p.suffix.lower() in {".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"}
                and p.exists()
            ):
                # Use the existing DTO logic to handle the read/encode in one go
                return ImageContent.from_file(p)

        # 3. Raw Bytes
        if isinstance(image, bytes):
            import base64

            # Default to PNG if format is unknown; Client/Adapter can override later
            b64_str = base64.b64encode(image).decode("utf-8")
            return ImageContent(url=f"data:image/png;base64,{b64_str}")

        # 4. Fallback for Strings (URLs or already encoded Base64)
        if isinstance(image, str):
            # If it already looks like a Data URI or URL, pass it through
            if image.startswith(("http", "data:image")):
                return ImageContent(url=image)
            # Otherwise, assume it's a raw base64 string
            return ImageContent(url=f"data:image/png;base64,{image}")

        raise ValueError(f"Unsupported image input type: {type(image)}")

    async def generate(
        self,
        prompt_str: str,
        size: str = "1024x1024",
        quality: Literal["standard", "hd"] = "standard",
        style: Literal["vivid", "natural"] = "vivid",
        n: int = 1,
        response_format: Literal["url", "b64_json"] = "b64_json",
    ):
        """
        Models: dall-e-2, dall-e-3
        """
        from conduit.core.clients.openai.image_params import OpenAIImageParams

        # Construct params
        client_params = OpenAIImageParams(
            size=size,
            quality=quality,
            style=style,
            n=n,
            response_format=response_format,
        )
        params = GenerationParams(
            output_type="image",
            model=self._parent.model_name,
            client_params=client_params.model_dump(),
        )
        options = ConduitOptions(project_name="test")

        # Construct messages
        user_message = UserMessage(content=prompt_str)
        messages = [user_message]
        request = GenerationRequest(messages=messages, params=params, options=options)

        response: GenerationResponse = await self._parent.pipe(request)
        return response

    async def analyze(self, prompt_str: str, image: ImageInput) -> GenerationResponse:
        """
        Models: gpt-4.1
        """
        params = GenerationParams(
            output_type="text",
            model=self._parent.model_name,
        )
        options = ConduitOptions(project_name="test")
        # Coerce image input
        image_content: ImageContent = self.coerce_image_input(image)
        text_content = TextContent(text=prompt_str)
        user_message = UserMessage(content=[text_content, image_content])
        messages = [user_message]
        request = GenerationRequest(messages=messages, params=params, options=options)
        response: GenerationResponse = await self._parent.pipe(request)
        return response


class ImageSync:
    def __init__(self, parent: ModelSync):
        self._parent = parent
        self._impl = ImageAsync(parent._impl)

    def generate(
        self,
        prompt_str: str,
        size: str = "1024x1024",
        quality: Literal["standard", "hd"] = "standard",
        style: Literal["vivid", "natural"] = "vivid",
        n: int = 1,
        response_format: Literal["url", "b64_json"] = "b64_json",
    ) -> GenerationResponse:
        return self._parent._run_sync(
            self._impl.generate(
                prompt_str=prompt_str,
                size=size,
                quality=quality,
                style=style,
                n=n,
                response_format=response_format,
            )
        )

    def analyze(self, prompt_str: str, image: ImageInput) -> GenerationResponse:
        return self._parent._run_sync(
            self._impl.analyze(prompt_str=prompt_str, image=image)
        )


if __name__ == "__main__":
    from conduit.examples.sample_objects import sample_image_file
    import asyncio

    def test_image_generate():
        model = ModelAsync("dall-e-3")
        image = ImageAsync(model)

        async def _main():
            response = await image.generate(
                prompt_str="A los angeles beach babe smoking a joint, digital art",
                size="1024x1024",
                quality="hd",
                style="vivid",
                n=1,
                response_format="b64_json",
            )
            return response

        response = asyncio.run(_main())
        return response

    response = test_image_generate()
    response.display()

    def test_image_analyze():
        model = ModelAsync("gpt-4.1")
        image = ImageAsync(model)

        async def _main():
            response = await image.analyze(
                prompt_str="Describe the content of the image in detail.",
                image=sample_image_file,
            )
            return response

        result = asyncio.run(_main())
        print(result)

```

## FILE: src/conduit/core/model/model_async.py
```py
from __future__ import annotations
from conduit.core.model.model_base import ModelBase
from conduit.core.clients.client_base import Client
from conduit.domain.result.result import GenerationResult
from conduit.domain.request.query_input import QueryInput
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from typing import override, TYPE_CHECKING
import logging

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.domain.request.request import GenerationRequest
    from conduit.core.model.modalities.audio import AudioAsync
    from conduit.core.model.modalities.image import ImageAsync
    from conduit.domain.message.message import Message

logger = logging.getLogger(__name__)


class ModelAsync(ModelBase):
    """
    Async implementation of Model - a stateless "dumb pipe".
    Execution context (params/options) passed explicitly to methods.
    """

    def __init__(self, model: str, client: Client | None = None):
        """
        Initialize the async model with only its identity.

        Args:
            model: The model name/alias (e.g., "gpt-4o", "claude-sonnet-4")
            client: Optional client injection for advanced use cases (e.g., remote models)
        """
        super().__init__(model=model, client=client)
        # Plugins
        self._audio: AudioAsync | None = None
        self._image: ImageAsync | None = None

    @property
    def audio(self) -> AudioAsync:
        """
        Lazy loading of audio generation/analyzation namespace.
        """
        from conduit.core.model.modalities.audio import AudioAsync

        if self._audio is None:
            self._audio = AudioAsync(parent=self)
        return self._audio

    @property
    def image(self) -> ImageAsync:
        """
        Lazy loading of image generation/analyzation namespace.
        """
        from conduit.core.model.modalities.image import ImageAsync

        if self._image is None:
            self._image = ImageAsync(parent=self)
        return self._image

    @override
    def get_client(self, model_name: str) -> Client:
        logger.info(f"Retrieving client for model: {model_name}")
        from conduit.core.model.models.modelstore import ModelStore

        return ModelStore.get_client(model_name, "sdk")

    @override
    async def query(
        self,
        request: GenerationRequest | None = None,
        query_input: QueryInput | str | Sequence[Message] | None = None,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
    ) -> GenerationResult:
        """
        Execute a query with explicit execution context.
        You have two options:
        - Provide a pre-built GenerationRequest via 'request' (other args ignored)
        - Provide 'query_input', 'params', and 'options' (all required)

        Args:
            request: Pre-built GenerationRequest (optional)
            query_input: The input messages or string
            params: Generation parameters (model, temperature, etc.)
            options: Conduit options (cache, console, etc.)

        Returns:
            GenerationResult from the LLM
        """
        # First, pass through the request if provided
        if request is not None:
            logger.info("ModelAsync.query called with pre-built request")
            result = await self.pipe(request)
            return result
        # Otherwise, the other three args are mandatory
        else:
            if query_input is None or params is None or options is None:
                raise ValueError(
                    "If 'request' is not provided, 'query_input', 'params', and 'options' must all be provided."
                )
            logger.info("ModelAsync.query called with model: %s", self.model_name)
            request = self._prepare_request(query_input, params, options)
            result = await self.pipe(request)
            return result

    @override
    async def tokenize(self, payload: str | Sequence[Message]) -> int:
        """
        Get the token length for the given model.
        Implementation at the client level.
        """
        logger.info("ModelAsync.tokenize called with model: %s", self.model_name)
        return await self.client.tokenize(model=self.model_name, payload=payload)

```

## FILE: src/conduit/core/model/model_base.py
```py
from __future__ import annotations
from conduit.domain.request.request import GenerationRequest
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.request.query_input import QueryInput, constrain_query_input
from conduit.core.clients.client_base import Client
from conduit.middleware.middleware import middleware
from typing import TYPE_CHECKING, override
import logging

# Load only if type checking
if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.domain.message.message import Message
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.config.conduit_options import ConduitOptions
    from conduit.core.model.modalities.audio import AudioSync, AudioAsync
    from conduit.core.model.modalities.image import ImageSync, ImageAsync

logger = logging.getLogger(__name__)


class ModelBase:
    """
    Stem class for Model implementations; not to be used directly.
    Holds only model identity (model_name + client).
    Execution context (params/options) passed to methods, not stored.
    """

    def __init__(self, model: str, client: Client | None = None):
        """
        Initialize the Model base with only its identity.

        Args:
            model: The model name/alias (e.g., "gpt-4o", "claude-sonnet-4")
            client: Optional client injection for advanced use cases (e.g., remote models)
        """
        from conduit.core.model.models.modelstore import ModelStore

        # Model identity - the only thing stored
        self.model_name: str = ModelStore.validate_model(model)
        self.client: Client = client if client is not None else self.get_client(model_name=self.model_name)
        # Plugins
        self._audio: AudioSync | AudioAsync | None = None
        self._image: ImageSync | ImageAsync | None = None

    # Class methods for global info
    @classmethod
    def models(cls) -> dict[str, list[str]]:
        """
        Returns a dictionary of available models.
        This is useful for introspection and debugging.
        """
        from conduit.core.model.models.modelstore import ModelStore

        return ModelStore.models()

    # Our pipe method
    @middleware
    async def pipe(self, request: GenerationRequest) -> GenerationResult:
        """
        Core delegation point - passes request to client.
        Options used by middleware for caching, console, etc.
        """
        return await self.client.query(request)

    # Helper methods
    def _prepare_request(
        self, query_input: QueryInput, params: GenerationParams, options: ConduitOptions
    ) -> GenerationRequest:
        """
        PURE CPU: Constructs and validates the GenerationRequest object.
        """
        # Constrain query_input per model capabilities
        query_input_list: Sequence[Message] = constrain_query_input(
            query_input=query_input
        )

        # Ensure params has correct model name
        if params.model != self.model_name:
            params = params.model_copy(update={"model": self.model_name})

        request = GenerationRequest(
            messages=query_input_list,
            params=params,
            options=options,
        )
        return request

    # Abstract methods (must be implemented by subclasses)
    def get_client(self, model_name: str) -> Client:
        raise NotImplementedError("get_client must be implemented in subclasses.")

    async def query(
        self,
        query_input: QueryInput,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> GenerationResult:
        raise NotImplementedError("query must be implemented in subclasses.")

    async def tokenize(self, payload: str | Sequence[Message]) -> int:
        raise NotImplementedError("tokenize must be implemented in subclasses.")

    # Dunders
    @override
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(model_name={self.model_name!r})"

```

## FILE: src/conduit/core/model/model_remote.py
```py
"""
Remote Model implementations that use RemoteClient for server-based execution.

Provides both sync and async interfaces with remote server capabilities like
ping, status checks, and batch operations.
"""

from __future__ import annotations
from typing import Any, TYPE_CHECKING
import logging

from conduit.core.model.model_sync import ModelSync
from conduit.core.model.model_async import ModelAsync
from conduit.core.clients.remote.client import RemoteClient
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

if TYPE_CHECKING:
    from headwater_api.classes.server_classes.status import StatusResponse
    from collections.abc import Sequence
    from conduit.domain.request.query_input import QueryInput
    from conduit.domain.result.result import GenerationResult

logger = logging.getLogger(__name__)


class RemoteModelSync(ModelSync):
    """
    Synchronous remote model that uses RemoteClient for server-based execution.

    Inherits all ModelSync functionality while adding remote-specific capabilities
    like ping(), get_status(), and batch() operations.
    """

    def __init__(
        self,
        model: str,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
        **kwargs: Any,
    ):
        """
        Initialize the synchronous remote model.

        Args:
            model: Model name/alias (e.g., "claude-3-sonnet", "gpt-4o")
            params: LLM parameters (temperature, max_tokens, etc.)
            options: Runtime configuration (caching, console, etc.)
            **kwargs: Additional parameters merged into GenerationParams
        """
        # Create RemoteClient and inject it
        remote_client = RemoteClient()
        super().__init__(
            model=model, params=params, options=options, client=remote_client, **kwargs
        )

    def ping(self) -> bool:
        """
        Ping the remote server to check connectivity.

        Returns:
            Server ping response with timing and status information
        """
        if not isinstance(self._impl.client, RemoteClient):
            raise TypeError("ping() requires RemoteClient")
        return self._run_sync(self._impl.client.ping())

    def get_status(self) -> StatusResponse:
        """
        Get the current status of the remote server.

        Returns:
            Server status information including health and model availability
        """
        if not isinstance(self._impl.client, RemoteClient):
            raise TypeError("get_status() requires RemoteClient")
        return self._run_sync(self._impl.client.get_status())

    def batch(self, query_inputs: Sequence[QueryInput]) -> list[GenerationResult]:
        """
        Process multiple queries in a batch on the remote server.

        Args:
            query_inputs: Sequence of input queries to process

        Returns:
            List of GenerationResults corresponding to each input
        """
        if not isinstance(self._impl.client, RemoteClient):
            raise TypeError("batch() requires RemoteClient")
        # Note: This will need to be implemented in RemoteClient
        return self._run_sync(self._impl.client.batch(query_inputs))

    @property
    def is_remote(self) -> bool:
        """Check if this model uses remote execution."""
        return True


class RemoteModelAsync(ModelAsync):
    """
    Asynchronous remote model that uses RemoteClient for server-based execution.

    Inherits all ModelAsync functionality while adding remote-specific capabilities
    like ping(), get_status(), and batch() operations.
    """

    def __init__(self, model: str):
        """
        Initialize the asynchronous remote model.

        Args:
            model: Model name/alias (e.g., "claude-3-sonnet", "gpt-4o")
        """
        # Create RemoteClient and inject it
        remote_client = RemoteClient()
        super().__init__(model=model, client=remote_client)

    async def ping(self) -> bool:
        """
        Ping the remote server to check connectivity.

        Returns:
            Server ping response with timing and status information
        """
        if not isinstance(self.client, RemoteClient):
            raise TypeError("ping() requires RemoteClient")
        return await self.client.ping()

    async def get_status(self) -> StatusResponse:
        """
        Get the current status of the remote server.

        Returns:
            Server status information including health and model availability
        """
        if not isinstance(self.client, RemoteClient):
            raise TypeError("get_status() requires RemoteClient")
        return await self.client.get_status()

    async def batch(self, query_inputs: Sequence[QueryInput]) -> list[GenerationResult]:
        """
        Process multiple queries in a batch on the remote server.

        Args:
            query_inputs: Sequence of input queries to process

        Returns:
            List of GenerationResults corresponding to each input
        """
        if not isinstance(self.client, RemoteClient):
            raise TypeError("batch() requires RemoteClient")
        # Note: This will need to be implemented in RemoteClient
        return await self.client.batch(query_inputs)

    @property
    def is_remote(self) -> bool:
        """Check if this model uses remote execution."""
        return True


# Factory functions for convenient instantiation
def remote_model_sync(
    model: str,
    params: GenerationParams | None = None,
    options: ConduitOptions | None = None,
    **kwargs: Any,
) -> RemoteModelSync:
    """
    Factory function to create a synchronous remote model.

    Args:
        model: Model name/alias (e.g., "claude-3-sonnet", "gpt-4o")
        params: LLM parameters (temperature, max_tokens, etc.)
        options: Runtime configuration (caching, console, etc.)
        **kwargs: Additional parameters merged into GenerationParams

    Returns:
        Configured RemoteModelSync instance
    """
    return RemoteModelSync(model=model, params=params, options=options, **kwargs)


def remote_model_async(model: str) -> RemoteModelAsync:
    """
    Factory function to create an asynchronous remote model.

    Args:
        model: Model name/alias (e.g., "claude-3-sonnet", "gpt-4o")

    Returns:
        Configured RemoteModelAsync instance
    """
    return RemoteModelAsync(model=model)


if __name__ == "__main__":
    from conduit.domain.request.generation_params import GenerationParams
    from conduit.domain.config.conduit_options import ConduitOptions
    from conduit.utils.progress.verbosity import Verbosity

    # Instantiate RemoteModelSync
    model = RemoteModelSync(
        model="gpt-4o",
        params=GenerationParams(model="gpt-4o", temperature=0.7),
        options=ConduitOptions(project_name="test", verbosity=Verbosity.PROGRESS),
    )

    # Check if remote
    print(f"Is remote: {model.is_remote}")

    # Try ping (will fail if no server, but tests instantiation)
    try:
        result = model.ping()
        print(f"Ping result: {result}")
    except Exception as e:
        print(f"Ping failed (expected if no server): {type(e).__name__}")

    # Query
    result = model.query("Hello, remote model!")
    print(f"Query result: {result}")

```

## FILE: src/conduit/core/model/model_sync.py
```py
from __future__ import annotations
import asyncio
import logging
from typing import Any, TYPE_CHECKING, override

from conduit.config import settings
from conduit.core.model.model_async import ModelAsync
from conduit.core.clients.client_base import Client
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.request.request import GenerationRequest
from conduit.utils.progress.verbosity import Verbosity
from conduit.utils.concurrency.warn import _warn_if_loop_exists

if TYPE_CHECKING:
    from collections.abc import Sequence
    from conduit.core.model.modalities.audio import AudioSync
    from conduit.core.model.modalities.image import ImageSync
    from conduit.domain.config.conduit_options import ConduitOptions
    from conduit.domain.request.query_input import QueryInput
    from conduit.domain.result.result import GenerationResult
    from conduit.domain.message.message import Message
    from rich.console import Console

logger = logging.getLogger(__name__)


class ModelSync:
    """
    Synchronous, UX-focused wrapper for ModelAsync.

    - Holds *how* to execute: GenerationParams, ConduitOptions.
    - Takes *what* to process at call-time via query_input.
    - Designed for scripts and REPL usage where managing state and event loops
      is unnecessary overhead.
    """

    def __init__(
        self,
        model: str | None = None,
        params: GenerationParams | None = None,
        options: ConduitOptions | None = None,
        client: Client | None = None,
        **kwargs: Any,
    ):
        """
        Initialize the synchronous model wrapper.

        Args:
            model: Model name/alias (e.g., "gpt-4o", "claude-sonnet-4").
            params: LLM parameters (temperature, max_tokens, etc.).
                    If not provided, defaults will be used with the specified model.
            options: Runtime configuration (caching, console, etc.).
            client: Optional client injection for advanced use cases (e.g., remote models)
            **kwargs: Additional parameters merged into GenerationParams (e.g., temperature=0.7).
        """
        # Quick validation: we need either model or params with model
        if model is None:
            if params is None or params.model is None:
                raise ValueError("Either 'model' or 'params.model' must be provided.")
            model = params.model
        # 1. Instantiate the async implementation (dumb pipe - only needs model identity)
        self._impl = ModelAsync(model, client=client)

        # 2. Store execution context
        if params is None:
            # Create minimal params with correct model
            self.params = GenerationParams(model=model)
        else:
            # Use provided params, ensuring model matches
            if params.model != model:
                # Override for consistency
                self.params = params.model_copy(update={"model": model})
            else:
                self.params = params

        self.options: ConduitOptions = options or settings.default_conduit_options()

        # 3. Merge kwargs into params for convenience
        if kwargs:
            updated_data = self.params.model_dump()
            updated_data.update(kwargs)
            self.params = GenerationParams(**updated_data)

        # Plugins
        self._audio: AudioSync | None = None
        self._image: ImageSync | None = None

    @property
    def audio(self) -> AudioSync:
        """
        Lazy loading of audio generation/analyzation namespace.
        """
        from conduit.core.model.modalities.audio import AudioSync

        if self._audio is None:
            self._audio = AudioSync(parent=self)
        return self._audio

    @property
    def image(self) -> ImageSync:
        """
        Lazy loading of image generation/analyzation namespace.
        """
        from conduit.core.model.modalities.image import ImageSync

        if self._image is None:
            self._image = ImageSync(parent=self)
        return self._image

    def query(
        self, query_input: QueryInput | None = None, **kwargs: Any
    ) -> GenerationResult:
        """
        Synchronous entry point for generation.

        Args:
            query_input: The input messages or string
            **kwargs: Either param overrides (temperature, max_tokens, etc.)
                     or a pre-built 'request' for advanced usage

        Currently accepted kwargs:
            - request: GenerationRequest (bypass normal flow
            - verbosity: Verbosity
            - cache: bool
            - include_history: bool

        Returns:
            GenerationResult from the LLM
        """
        # Handle pre-built request (advanced usage)
        if "request" in kwargs:
            request = kwargs["request"]
            if not isinstance(request, GenerationRequest):
                raise TypeError(
                    f"request must be a GenerationRequest, got {type(request)}"
                )
            # Bypass normal flow and call pipe directly
            return self._run_sync(self._impl.pipe(request))

        # Ensure query_input is provided
        if query_input is None:
            raise ValueError("query_input is required")

        # Construct effective params -- these are overrides.
        # First make a copy of stored params, then apply any overrides from kwargs.
        effective_params = self.params.model_copy()
        param_overrides = {
            k: v for k, v in kwargs.items() if k in effective_params.model_fields
        }
        if param_overrides:
            updated_data = effective_params.model_dump()
            updated_data.update(param_overrides)
            effective_params = GenerationParams(**updated_data)
        else:
            effective_params = self.params

        # Build Request from stored params/options
        request = self._impl._prepare_request(
            query_input, effective_params, self.options
        )

        # Implement overrides
        if "verbosity" in kwargs:
            request.verbosity_override = kwargs["verbosity"]
        if "cache" in kwargs:
            request.use_cache = kwargs["cache"]
        if "include_history" in kwargs:
            request.include_history = kwargs["include_history"]

        return self._run_sync(self._impl.query(request))

    def tokenize(self, payload: str | Sequence[Message]) -> int:
        """
        Synchronous entry point for tokenization.
        """
        return self._run_sync(self._impl.tokenize(payload))

    # Factory
    @classmethod
    def create(
        cls,
        model: str,
        *,
        project_name: str = settings.default_project_name,
        persist: bool | str = False,
        cached: bool | str = False,
        verbosity: Verbosity = settings.default_verbosity,
        console: Console | None = None,
        system: str | None = None,
        debug_payload: bool = False,
        use_remote: bool = False,
        **param_kwargs: Any,
    ) -> ModelSync:
        """
        Factory with sensible defaults.

        - `model`: required model name/alias (used for GenerationParams.model).
        - `param_kwargs`: go directly into GenerationParams(...).
        - `cached` / `persist` / `verbosity` / `console`:
            baked into ConduitOptions as baseline behavior.
        """
        # Params: seed with model + any extra params
        params = GenerationParams(model=model, **param_kwargs, system=system)

        # Options: start from global defaults
        options = settings.default_conduit_options()

        # Collect overrides
        opt_updates: dict[str, Any] = {"verbosity": verbosity}

        if console is not None:
            opt_updates["console"] = console

        # Cache wiring
        if cached:
            cache_name = cached if isinstance(cached, str) else project_name
            opt_updates["cache"] = settings.default_cache(project_name=cache_name)

        # Persistence wiring
        if persist:
            repo_name = persist if isinstance(persist, str) else project_name
            opt_updates["repository"] = settings.default_repository(
                project_name=repo_name
            )

        # Debug
        if debug_payload:
            opt_updates["debug_payload"] = True

        # Remote execution
        if use_remote:
            opt_updates["use_remote"] = True

        # Apply updates (Pydantic v2)
        options = options.model_copy(update=opt_updates)

        return cls(model=model, params=params, options=options)

    # Config methods - mutate stored options
    def enable_cache(self, name: str = settings.default_project_name) -> None:
        """Enable caching with specified name."""
        self.options.cache = settings.default_cache(name)
        logger.info(f"Enabled cache: {name}")

    def enable_console(self) -> None:
        """Enable rich console output."""
        if self.options.console is None:
            from rich.console import Console

            logger.info("Enabling console.")
            self.options.console = Console()

    def disable_cache(self) -> None:
        """Disable caching."""
        if self.options.cache is not None:
            logger.info("Disabling cache.")
            self.options.cache = None

    def disable_console(self) -> None:
        """Disable console output."""
        if self.options.console is not None:
            logger.info("Disabling console.")
            self.options.console = None

    # Helper methods
    def _build_params(self, param_overrides: dict[str, Any] | None) -> GenerationParams:
        """
        Build effective params by merging stored params with overrides.
        """
        if not param_overrides:
            return self.params

        updated_data = self.params.model_dump()
        updated_data.update(param_overrides)
        return GenerationParams(**updated_data)

    def _run_sync(self, coroutine: Any) -> Any:
        """
        Helper to run async methods synchronously.
        """
        _warn_if_loop_exists()
        try:
            return asyncio.run(coroutine)
        except KeyboardInterrupt:
            # Handle Ctrl+C gracefully during blocking calls
            logger.warning("Operation cancelled by user.")
            raise

    def __getattr__(self, name: str) -> Any:
        """
        Proxy attribute access to the underlying ModelAsync instance.
        Allows access to properties like self.model_name, self.client, etc.
        """
        return getattr(self._impl, name)

    @override
    def __repr__(self) -> str:
        return (
            f"ModelSync(model={self._impl.model_name!r}, "
            f"params={self.params!r}, options={self.options!r})"
        )

```

## FILE: src/conduit/core/model/models/modelspec.py
```py
from pydantic import BaseModel, Field
from conduit.core.model.models.provider import Provider


class ModelSpec(BaseModel):
    model: str = Field(
        ...,
        description="The name of the model, e.g. gpt-4o, claude-2, gemini-1.5-flash, etc.",
    )
    description: str | None = Field(
        ..., description="A brief description of the model's capabilities and features."
    )

    # Provider-specific capabilities
    provider: Provider = Field(
        ...,
        description="The provider of the model, e.g. openai, ollama, perplexity, google, anthropic, etc.",
    )
    temperature_range: list[float] = Field(
        ...,
        description="The supported temperature range for controlling randomness in model outputs, where lower values produce more deterministic responses and higher values increase creativity. OpenAI's standard range is 0.0 to 2.0, e.g. [0.0, 2.0]",
    )
    context_window: int = Field(
        ...,
        description="The maximum number of tokens the model can process in a single request, including both input and output tokens. OpenAI refers to this as 'max_tokens' in their API documentation. This represents Ollama's 'num_ctx' parameter, e.g. 128000",
    )

    # Model characteristics
    parameter_count: str | None = Field(
        None, description="Parameter count (e.g., '7b', '70b', '405b')"
    )
    knowledge_cutoff: str | None = Field(
        default=None,
        description="Date when the model's knowledge was last updated; None if unknown or continuously updated",
    )
    text_completion: bool = Field(
        ...,
        description="The ability to generate, continue, or complete text based on prompts and context, e.g. gpt-4o",
    )
    image_analysis: bool = Field(
        ...,
        description="The capability to analyze, understand, and describe visual content in images, e.g. claude-sonnet-4-20250514",
    )
    image_gen: bool = Field(
        ...,
        description="The ability to create or generate new images from text descriptions or prompts, e.g. gemini-2.0-flash-001",
    )
    audio_analysis: bool = Field(
        ...,
        description="The capability to process, transcribe, or analyze audio content and speech, e.g. gpt-4o-audio-preview",
    )
    audio_gen: bool = Field(
        ...,
        description="The ability to synthesize speech or generate audio content from text or other inputs, e.g. gpt-4o-audio-preview",
    )
    video_analysis: bool = Field(
        ...,
        description="The capability to analyze, understand, and extract information from video content, e.g. llama3.2-vision:11b",
    )
    video_gen: bool = Field(
        ...,
        description="The ability to create or generate video content from text descriptions or other inputs, e.g. gemini-2.5-pro-preview-05-06",
    )
    reasoning: bool = Field(
        ...,
        description="The capability to perform logical reasoning, problem-solving, and complex analytical tasks, e.g. o1-preview",
    )

    def __str__(self):
        return f"{self.model} ({self.provider})"

    def __repr__(self):
        return f"ModelSpec(model={self.model}, provider={self.provider})"

    @property
    def card(self):
        """
        Pretty print the model capabilities.
        """
        from rich.console import Console
        from rich.markdown import Markdown
        from rich.table import Table

        console = Console()
        table = Table(
            title=f"[bold bright_yellow]Model Capabilities for {self.provider + '/' + self.model}[/bold bright_yellow]"
        )
        table.add_column("Capability", justify="left", style="cyan")
        table.add_column("Supported", justify="center", style="green")
        table.add_column("Description", justify="left", style="white")
        if self.temperature_range:
            table.add_row(
                "Temperature Range",
                f"{self.temperature_range[0]} to {self.temperature_range[1]}",
                "The range of temperature values supported by the model",
            )
        else:
            table.add_row(
                "Temperature Range",
                "Unknown",
                "The range of temperature values supported by the model",
            )
        table.add_row(
            "Context Window",
            str(self.context_window),
            "The maximum number of tokens the model can process in a single request",
        )
        table.add_row(
            "Parameter Count",
            self.parameter_count or "Unknown",
            "The number of parameters in the model",
        )
        table.add_row(
            "Knowledge Cutoff",
            self.knowledge_cutoff or "Unknown",
            "The date when the model's knowledge was last updated",
        )
        table.add_row(
            "Text Completion",
            str(self.text_completion),
            "The ability to generate, continue, or complete text based on prompts and context",
        )
        table.add_row(
            "Image Analysis",
            str(self.image_analysis),
            "The capability to analyze, understand, and describe visual content in images",
        )
        table.add_row(
            "Image Generation",
            str(self.image_gen),
            "The ability to create or generate new images from text descriptions or prompts",
        )
        table.add_row(
            "Audio Analysis",
            str(self.audio_analysis),
            "The capability to process, transcribe, or analyze audio content and speech",
        )
        table.add_row(
            "Audio Generation",
            str(self.audio_gen),
            "The ability to synthesize speech or generate audio content from text or other inputs",
        )
        table.add_row(
            "Video Analysis",
            str(self.video_analysis),
            "The capability to analyze, understand, and extract information from video content",
        )
        table.add_row(
            "Video Generation",
            str(self.video_gen),
            "The ability to create or generate video content from text descriptions or other inputs",
        )
        table.add_row(
            "Reasoning",
            str(self.reasoning),
            "The capability to perform logical reasoning, problem-solving, and complex analytical tasks",
        )
        description = f"\n[bold green]Description:[/bold green] [yellow]{self.description or 'No description available'}[/yellow]\n"

        # Add a bar for each capability
        console.print("\n")
        console.print(table)
        console.print(description)

    # Validation methods
    def is_image_analysis_supported(self) -> bool:
        """Check if image analysis is supported."""
        return self.image_analysis

    def is_image_generation_supported(self) -> bool:
        """Check if image generation is supported."""
        return self.image_gen

    def is_audio_analysis_supported(self) -> bool:
        """Check if audio analysis is supported."""
        return self.audio_analysis

    def is_audio_generation_supported(self) -> bool:
        """Check if audio generation is supported."""
        return self.audio_gen

    def is_video_analysis_supported(self) -> bool:
        """Check if video analysis is supported."""
        return self.video_analysis

    def is_video_generation_supported(self) -> bool:
        """Check if video generation is supported."""
        return self.video_gen

    def is_text_completion_supported(self) -> bool:
        """Check if text completion is supported."""
        return self.text_completion

    def is_reasoning_supported(self) -> bool:
        """Check if reasoning capabilities are supported."""
        return self.reasoning


class ModelSpecList(BaseModel):
    """
    A list of model capabilities.
    This is entirely for requests (to Perplexity).
    """

    specs: list[ModelSpec]


if __name__ == "__main__":
    example = ModelSpec(
        model="example-model",
        description="An example model for demonstration purposes.",
        provider="openai",
        temperature_range=[0.0, 2.0],
        context_window=4096,
        parameter_count="7b",
        knowledge_cutoff="2023-10-01",
        text_completion=True,
        image_analysis=False,
        image_gen=True,
        audio_analysis=False,
        audio_gen=True,
        video_analysis=False,
        video_gen=False,
        reasoning=True,
    )
    example.card

```

## FILE: src/conduit/core/model/models/modelspecs_CRUD.py
```py
"""
CRUD functions for TinyDB; handles serializing / deserializing our ModelCapabilities objects.
"""

from conduit.core.model.models.modelspec import ModelSpec
from tinydb import TinyDB, Query
from pathlib import Path

dir_path = Path(__file__).parent
modelspecs_file = dir_path / "modelspecs.json"
db = TinyDB(modelspecs_file)


def create_modelspecs_from_scratch(modelspecs=list[ModelSpec]) -> None:
    """
    Create a new empty database for ModelSpecs.
    This will overwrite any existing data in modelspecs.json.
    """
    global db
    db.close()  # Close the existing database if it exists
    if modelspecs_file.exists():
        modelspecs_file.unlink()  # Delete the existing file
    db = TinyDB(modelspecs_file)  # Reopen the database
    print(f"Created new ModelSpecs database at {modelspecs_file}")
    # Now populate the database with any initial data if needed
    if modelspecs:
        for spec in modelspecs:
            add_modelspec(spec)


def add_modelspec(model_spec: ModelSpec) -> None:
    """
    Add a ModelSpec to the database.
    """
    db.insert(model_spec.model_dump())


def create_modelspec(model_spec: ModelSpec) -> None:
    t


def get_all_modelspecs() -> list[ModelSpec]:
    """
    Retrieve all ModelSpecs from the database.
    """
    return [ModelSpec(**item) for item in db.all()]


def get_modelspec_by_name(model: str) -> ModelSpec:
    """
    Retrieve a ModelSpec by its name.
    """
    ModelQuery = Query()
    item = db.search(ModelQuery.model == model)
    if item:
        return ModelSpec(**item[0])
    else:
        raise ValueError(f"ModelSpec with name '{model}' not found.")


def get_all_model_names() -> list[str]:
    """
    Retrieve all model names from the database.
    """
    ModelQuery = Query()
    return [item["model"] for item in db.all()]


def update_modelspec(model: str, updated_spec: ModelSpec) -> None:
    """
    Update a ModelSpec in the database.
    """
    ModelQuery = Query()
    db.update(updated_spec.model_dump(), ModelQuery.model == model)


def delete_modelspec(model: str) -> None:
    """
    Delete a ModelSpec from the database.
    """
    ModelQuery = Query()
    db.remove(ModelQuery.model == model)


def in_db(model_spec: ModelSpec) -> bool:
    """
    Check if a ModelSpec is already in the database.
    """
    ModelQuery = Query()
    item = db.search(ModelQuery.model == model_spec.model)
    return bool(item)

```

## FILE: src/conduit/core/model/models/modelstore.py
```py
"""
Our list of API llm models is in models.json, in our repo, as is the list of aliases.
The list of ollama models is host-specific, and therefore in our state directory.
Ollama context sizes is user-configurable, and therefore in our config directory.
"""

from __future__ import annotations
from conduit.config import settings
from conduit.core.model.models.providerstore import ProviderStore
from conduit.core.model.models.provider import Provider
from conduit.core.clients.client_base import Client
from pathlib import Path
from typing import Literal, TYPE_CHECKING
import json
import itertools
import logging
from rich.console import RenderableType

if TYPE_CHECKING:
    from conduit.core.model.models.modelspec import ModelSpec
    from conduit.core.model.models.modelspecs_CRUD import (
        get_modelspec_by_name,
        get_all_modelspecs,
    )

logger = logging.getLogger(__name__)

# Our data stores
DIR_PATH = Path(__file__).parent
MODELS_PATH = DIR_PATH / "models.json"
ALIASES_PATH = DIR_PATH / "aliases.json"


class ModelStore:
    """
    Class to manage model information for the Conduit library.
    Provides methods to retrieve supported models, aliases, and validate model names.
    """

    @classmethod
    def models(cls) -> dict[str, list[str]]:
        """
        Definitive list of models supported by Conduit library, as well as the local list of ollama models.
        """
        with open(MODELS_PATH) as f:
            models_json = json.load(f)
        if settings.paths["OLLAMA_MODELS_PATH"].exists():
            with open(settings.paths["OLLAMA_MODELS_PATH"]) as f:
                ollama_models = json.load(f)
            models_json["ollama"] = ollama_models["ollama"]
        if settings.paths["SERVER_MODELS_PATH"].exists():
            with open(settings.paths["SERVER_MODELS_PATH"]) as f:
                server_models = json.load(f)
            models_json["ollama"] += server_models["ollama"]
        return models_json

    @classmethod
    def list_models(cls) -> list[str]:
        """List of all models supported by Conduit library."""
        models = cls.models()
        return list(itertools.chain.from_iterable(models.values()))

    @classmethod
    def list_model_types(cls) -> list[str]:
        """List of model types supported by Conduit library."""
        return [
            "image_analysis",
            "image_gen",
            "audio_analysis",
            "audio_gen",
            "video_analysis",
            "video_gen",
            "reasoning",
            "text_completion",
        ]

    @classmethod
    def list_providers(cls) -> list[str]:
        """Definitive list of providers supported by Conduit library."""
        providers = ProviderStore.get_all_providers()
        return [provider.provider for provider in providers]

    @classmethod
    def identify_provider(cls, model: str) -> Provider:
        """
        Identify the provider for a given model.
        Returns the Provider object if found, raises ValueError otherwise.
        """
        models = cls.models()
        for provider, model_list in models.items():
            if model in model_list:
                return provider
        raise ValueError(f"Provider not found for model: {model}")

    @classmethod
    def local_models(cls) -> list[str]:
        """List of all locally hosted models supported by Conduit library."""
        models = cls.models()
        local_models = []
        for provider, model_list in models.items():
            if provider in ["ollama", "local"]:
                local_models.extend(model_list)
        return local_models

    @classmethod
    def cloud_models(cls) -> list[str]:
        """List of all cloud-hosted models supported by Conduit library."""
        models = cls.models()
        cloud_models: list[str] = []
        for provider, model_list in models.items():
            if provider not in ["ollama", "local"]:
                cloud_models.extend(model_list)
        return cloud_models

    @classmethod
    def aliases(cls) -> dict[str, str]:
        """Definitive list of model aliases supported by Conduit library."""
        with open(ALIASES_PATH) as f:
            return json.load(f)

    @classmethod
    def is_supported(cls, model: str) -> bool:
        """
        Check if the model is supported by the Conduit library.
        Returns True if the model is supported, False otherwise.
        """
        in_aliases = model in cls.aliases().keys()
        in_models = model in list(itertools.chain.from_iterable(cls.models().values()))
        return in_aliases or in_models

    @classmethod
    def validate_model(cls, model: str) -> str:
        """
        Validate the model name against the supported models and aliases.
        Converts aliases to their corresponding model names if necessary.
        """
        # Load aliases
        aliases = cls.aliases()
        # Assign models based on aliases
        if model in cls.aliases().keys():
            model = aliases[model]
            return model
        elif cls.is_supported(model):
            model = model
            return model
        else:
            raise ValueError(
                f"WARNING: Model not found locally: {model}. This may cause errors."
            )

    @classmethod
    def get_num_ctx(cls, ollama_model: str) -> int:
        """
        Get the preferred num_ctx for a given ollama model.
        """
        default_value = 32768
        if settings.paths["OLLAMA_CONTEXT_SIZES_PATH"].exists():
            with open(settings.paths["OLLAMA_CONTEXT_SIZES_PATH"]) as f:
                context_sizes: dict = json.load(f)
            if ollama_model in context_sizes:
                return context_sizes[ollama_model]
            else:
                logger.warning(
                    f"Model {ollama_model} not found in context sizes file. Using default value of {default_value}."
                )
                return default_value  # Default context size if not specified -- may throw an error for smaller models
        else:
            raise FileNotFoundError(
                f"Context sizes file not found: {settings.paths['ollama_context_sizes_path']}"
            )

    @classmethod
    def _generate_renderable_model_list(cls) -> RenderableType:
        """
        Generate a Rich renderable object displaying the list of models in three columns.
        """
        from rich.columns import Columns
        from rich.text import Text

        models = cls.models()

        # Calculate total items and split points
        all_items = []
        for provider, model_list in models.items():
            all_items.append((provider, None))  # Provider header
            for model in model_list:
                all_items.append((provider, model))

        # Split into three roughly equal columns
        total_items = len(all_items)
        col1_end = total_items // 3
        col2_end = 2 * total_items // 3

        # Create three columns
        left_column = Text()
        middle_column = Text()
        right_column = Text()

        for i, (provider, model) in enumerate(all_items):
            if i < col1_end:
                target_column = left_column
            elif i < col2_end:
                target_column = middle_column
            else:
                target_column = right_column

            if model is None:  # Provider header
                target_column.append(f"{provider.upper()}\n", style="bold cyan")
            else:  # Model name
                target_column.append(f"  {model}\n", style="yellow")

        renderable_columns = Columns(
            [left_column, middle_column, right_column], equal=True, expand=True
        )
        return renderable_columns

    @classmethod
    def display(cls):
        """
        Display all available models in a formatted three-column layout to the console.
        """
        from rich.console import Console

        console = Console()
        renderable_columns = cls._generate_renderable_model_list()
        console.print(renderable_columns)

    # Consistency
    @classmethod
    def update(cls):
        """
        Compare model list from models.json, and make sure there are corresponding ModelSpec objects in database.
        Delete objects that don't have their model name in models.json; and create new ModelSpec objects if they are not in db.
        """
        if not cls._is_consistent():
            print(
                "Model specifications are not consistent with models.json. Updating..."
            )
            logger.info(
                "Model specifications are not consistent with models.json. Updating..."
            )
            cls._update_models()
        else:
            print(
                "Model specifications are consistent with models.json. No update needed."
            )
            logger.info(
                "Model specifications are consistent with models.json. No update needed."
            )

    @classmethod
    def _update_models(cls):
        from conduit.core.model.models.research_models import create_modelspec
        from conduit.core.model.models.modelspecs_CRUD import (
            get_all_modelspecs,
            delete_modelspec,
            get_all_model_names,
        )

        # Get all ModelSpec objects from the database
        modelspec_db_names = set(get_all_model_names())
        # Create a set of model names from the models.json file
        models = cls.models()
        models_json_names = set(itertools.chain.from_iterable(models.values()))
        # Find models that are in models.json but not in the database
        models_not_in_modelspec_db = models_json_names - modelspec_db_names
        # Find models that are in the database but not in models.json
        models_not_in_model_list = modelspec_db_names - models_json_names
        logger.info(
            f"Found {len(models_not_in_modelspec_db)} models not in the database."
        )
        logger.info(f"Found {len(models_not_in_model_list)} models not in models.json.")
        # Delete all ModelSpec objects that are not in models.json
        logger.info(
            f"Deleting {len(models_not_in_model_list)} models not in models.json."
        )
        [delete_modelspec(model) for model in models_not_in_model_list]
        # Create all Modelspec objects that are in models.json but not in the database
        logger.info(
            f"Creating {len(models_not_in_modelspec_db)} models not in the database."
        )
        [create_modelspec(model) for model in models_not_in_modelspec_db]
        if cls._is_consistent():
            logger.info(
                "Model specifications are now consistent with models.json. Update complete."
            )
            return
        else:
            raise ValueError(
                "Model specifications are not consistent with models.json, after running .update()."
            )

    @classmethod
    def _is_consistent(cls) -> bool:
        """
        Check if the model specifications in the database are consistent with the models.json file.
        Returns True if consistent, False otherwise.
        """
        from conduit.core.model.models.modelspecs_CRUD import get_all_modelspecs

        # Get list of models from models.json
        models = cls.models()

        # Get all ModelSpec objects from the database
        model_specs = get_all_modelspecs()

        # Create a set of model names from the models.json file
        model_names = set(itertools.chain.from_iterable(models.values()))

        consistent = True
        # Check if all ModelSpec names are in the models.json file
        for model_spec in model_specs:
            if model_spec.model not in model_names:
                consistent = False
        # Check if all models in models.json have a corresponding ModelSpec object
        for model_list in models.values():
            for model in model_list:
                if not any(model_spec.model == model for model_spec in model_specs):
                    consistent = False

        return consistent

    # Getters
    @classmethod
    def get_model(cls, model: str) -> ModelSpec:
        """
        Get the model name, validating against aliases and supported models.
        """
        model = cls.validate_model(model)
        try:
            return get_modelspec_by_name(model)
        except ValueError:
            raise ValueError(f"Model '{model}' not found in the database.")

    @classmethod
    def get_all_models(cls) -> list[ModelSpec]:
        """
        Get all models as ModelSpec objects.
        """
        return get_all_modelspecs()

    @classmethod
    def get_client(
        cls, model_name: str, execution_mode: Literal["sdk", "remote"]
    ) -> Client:
        """
        Get the client for a specific model.
        """
        model_name = cls.validate_model(model_name)
        from conduit.core.model.models.modelstore import ModelStore

        model_list = ModelStore.models()
        # Handle remote execution mode first
        if execution_mode == "remote":
            from conduit.core.clients.remote.client import RemoteClient

            return RemoteClient(model_name)

        # Now all sync / async modes
        if model_name in model_list["openai"]:
            from conduit.core.clients.openai.client import OpenAIClient

            return OpenAIClient()
        elif model_name in model_list["anthropic"]:
            from conduit.core.clients.anthropic.client import AnthropicClient

            return AnthropicClient()
        elif model_name in model_list["google"]:
            from conduit.core.clients.google.client import GoogleClient

            return GoogleClient()
        elif model_name in model_list["ollama"]:
            from conduit.core.clients.ollama.client import OllamaClient

            return OllamaClient()
        elif model_name in model_list["perplexity"]:
            from conduit.core.clients.perplexity.client import PerplexityClient

            return PerplexityClient()
        else:
            raise ValueError(f"Model {model_name} not found in ModelStore")

    ## Get subsets of models by provider
    @classmethod
    def by_provider(cls, provider: Provider) -> list[ModelSpec]:
        """
        Get a list of models for a specific provider.
        """
        return [
            modelspec
            for modelspec in cls.get_all_models()
            if modelspec.provider == provider
        ]

    ## Get subsets of models by type
    @classmethod
    def by_type(cls, model_type: str) -> list[ModelSpec]:
        """
        Get a list of models by type.
        Raises ValueError if the model type is not valid.
        """
        if model_type not in cls.list_model_types():
            raise ValueError(
                f"Invalid model type: {model_type}. Must be one of: {', '.join(cls.list_model_types())}."
            )
        match model_type:
            case "image_analysis":
                return cls.image_analysis_models()
            case "image_gen":
                return cls.image_gen_models()
            case "audio_analysis":
                return cls.audio_analysis_models()
            case "audio_gen":
                return cls.audio_gen_models()
            case "video_analysis":
                return cls.video_analysis_models()
            case "video_gen":
                return cls.video_gen_models()
            case "reasoning":
                return cls.reasoning_models()
            case "text_completion":
                return cls.text_completion_models()
            case _:
                raise ValueError(
                    f"Invalid model type: {model_type}. Must be one of: {', '.join(cls.list_model_types())}."
                )

    ## Get lists of models by capability
    @classmethod
    def image_analysis_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support image analysis.
        """
        return [
            modelspec for modelspec in cls.get_all_models() if modelspec.image_analysis
        ]

    @classmethod
    def image_gen_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support image generation.
        """
        return [modelspec for modelspec in cls.get_all_models() if modelspec.image_gen]

    @classmethod
    def audio_analysis_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support audio analysis.
        """
        return [
            modelspec for modelspec in cls.get_all_models() if modelspec.audio_analysis
        ]

    @classmethod
    def audio_gen_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support audio generation.
        """
        return [modelspec for modelspec in cls.get_all_models() if modelspec.audio_gen]

    @classmethod
    def video_analysis_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support video analysis.
        """
        return [
            modelspec for modelspec in cls.get_all_models() if modelspec.video_analysis
        ]

    @classmethod
    def video_gen_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support video generation.
        """
        return [modelspec for modelspec in cls.get_all_models() if modelspec.video_gen]

    @classmethod
    def reasoning_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support reasoning.
        """
        return [modelspec for modelspec in cls.get_all_models() if modelspec.reasoning]

    @classmethod
    def text_completion_models(cls) -> list[ModelSpec]:
        """
        Get a list of models that support text completion.
        """
        return [
            modelspec for modelspec in cls.get_all_models() if modelspec.text_completion
        ]

```

## FILE: src/conduit/core/model/models/provider.py
```py
from typing import Literal

Provider = Literal["openai", "ollama", "anthropic", "google", "perplexity"]


```

## FILE: src/conduit/core/model/models/providerspec.py
```py
from pydantic import BaseModel, Field
from typing import Literal
from pathlib import Path

dir_path = Path(__file__).parent
providers_file = dir_path / "providers.jsonl"

class ProviderSpec(BaseModel):
    """
    A specification for a provider's capabilities.
    This is entirely for requests (to Perplexity).
    """
    provider: str = Field(..., description="The name of the provider")
    temperature_range: list[float] | Literal["Not supported"] = Field(
        ..., description="The range of temperature settings available for the provider"
    )

def populate_providers_file():
    """
    Populate the providers.jsonl file with provider specifications.
    """
    if providers_file.exists():
        providers_file.unlink()  # Remove existing file if it exists

    providers_file.touch()  # Create a new empty file

    # Define provider specifications
    openai = ProviderSpec(
        provider="openai",
        temperature_range=[0.0, 2.0]
    )

    anthropic = ProviderSpec(
        provider="anthropic",
        temperature_range=[0.0, 1.0]
        )

    google = ProviderSpec(
        provider="google",
        temperature_range=[0.0, 1.0]
    )

    groq = ProviderSpec(
        provider="groq",
        temperature_range=[0.0, 1.0]
    )

    perplexity = ProviderSpec(
        provider="perplexity",
        temperature_range="Not supported"
    )

    ollama = ProviderSpec(
        provider="ollama",
        temperature_range=[0.0, 1.0]
    )

    providers = [openai, anthropic, google, groq, perplexity, ollama]

    providers_file.write_text(
        "\n".join(provider.model_dump_json() for provider in providers)
    )
    print(f"Populated {providers_file} with provider specifications.")

if __name__ == "__main__":
    populate_providers_file()

```

## FILE: src/conduit/core/model/models/providerstore.py
```py
"""
Repository class for managing provider specifications.
This module provides functions to create, read, update, and delete provider specifications.
"""

from conduit.core.model.models.providerspec import ProviderSpec
from pathlib import Path

dir_path = Path(__file__).parent
providers_file = dir_path / "providers.jsonl"


class ProviderStore:
    @classmethod
    def create_provider(cls, provider: ProviderSpec) -> None:
        """
        Create a new provider specification and save it to the JSONL file.

        Args:
            provider (ProviderSpec): The provider specification to create.
        """
        with open(providers_file, "a") as f:
            f.write(provider.model_dump_json() + "\n")

    @classmethod
    def get_all_providers(cls) -> list[ProviderSpec]:
        """
        Read all provider specifications from the JSONL file.

        Returns:
            list[ProviderSpec]: A list of provider specifications.
        """
        providers = []
        with open(providers_file, "r") as f:
            for line in f:
                providers.append(ProviderSpec.model_validate_json(line.strip()))
        return providers

    @classmethod
    def update_provider(cls, provider: ProviderSpec) -> None:
        """
        Update an existing provider specification in the JSONL file.

        Args:
            provider (ProviderSpec): The provider specification to update.
        """
        providers = cls.get_all_providers()
        updated = False
        with open(providers_file, "w") as f:
            for p in providers:
                if p.provider == provider.provider:
                    f.write(provider.model_dump_json() + "\n")
                    updated = True
                else:
                    f.write(p.model_dump_json() + "\n")
        if not updated:
            raise ValueError(f"Provider {provider.provider} not found for update.")

    @classmethod
    def delete_provider(cls, provider_name: str) -> None:
        """
        Delete a provider specification from the JSONL file.

        Args:
            provider_name (str): The name of the provider to delete.
        """
        providers = cls.get_all_providers()
        with open(providers_file, "w") as f:
            deleted = False
            for p in providers:
                if p.provider != provider_name:
                    f.write(p.model_dump_json() + "\n")
                else:
                    deleted = True
            if not deleted:
                raise ValueError(f"Provider {provider_name} not found for deletion.")

    @classmethod
    def get_provider(cls, provider_name: str) -> ProviderSpec:
        """
        Get a specific provider specification by name.

        Args:
            provider_name (str): The name of the provider to retrieve.

        Returns:
            ProviderSpec: The provider specification if found.

        Raises:
            ValueError: If the provider is not found.
        """
        providers = cls.get_all_providers()
        for p in providers:
            if p.provider == provider_name:
                return p
        raise ValueError(f"Provider {provider_name} not found.")

```

## FILE: src/conduit/core/model/models/research_models.py
```py
from conduit.core.model.models.modelspec import ModelSpecList, ModelSpec
from conduit.core.model.models.modelspecs_CRUD import (
    create_modelspecs_from_scratch,
    add_modelspec,
    get_all_modelspecs,
    in_db,
)
from conduit.core.model.model_sync import ModelSync as Model
from conduit.core.conduit.conduit_sync import ConduitSync
from conduit.core.prompt.prompt import Prompt
from conduit.core.parser.parser import Parser
from rich.console import Console


list_prompt_str = """
You are an assistant who will help me identify the capabilities of a list of LLMs.

<provider>
{{provider}}
</provider>

<model_list>
{{model_list}}
</model_list>

You will be identifying, for each model, whether it supports the following functionalities:

- **text_completion**: The ability to generate, continue, or complete text based on prompts and context
- **image_analysis**: The capability to analyze, understand, and describe visual content in images
- **image_gen**: The ability to create or generate new images from text descriptions or prompts
- **audio_analysis**: The capability to process, transcribe, or analyze audio content and speech
- **audio_gen**: The ability to synthesize speech or generate audio content from text or other inputs
- **video_analysis**: The capability to analyze, understand, and extract information from video content
- **video_gen**: The ability to create or generate video content from text descriptions or other inputs
- **reasoning**: The capability to perform logical reasoning, problem-solving, and complex analytical tasks

For each model, please return a ModelCapabilities object with boolean values for each capability, indicating whether the model supports (True) or does not support (False) each functionality.

You should also generate factual description of each model (50-80 words), including:
- The model's architecture type and parameter size (if known)
- Specific capabilities it supports (multimodal inputs, function calling, etc.)
- Context window size and key technical specifications
- Release date or version information
- Primary intended use cases or design focus
- Any notable technical features or limitations

Avoid promotional language or subjective quality assessments. Focus on objective, verifiable information about what the model can do and its technical characteristics.

Return a different object for each model -- since there are {{length}} models in our list, return {{length}} ModelCapabilities objects.
""".strip()

individual_prompt_str = """
You are an assistant who will help me identify the capabilities of a specific LLM.

Here's the provider:

<provider>
{{provider}}
</provider>

And here's the specific model:

<model>
{{model}}
</model>

You will be identifying, for the above model, whether it supports the following functionalities:
- **text_completion**: The ability to generate, continue, or complete text based on prompts and context
- **image_analysis**: The capability to analyze, understand, and describe visual content in images
- **image_gen**: The ability to create or generate new images from text descriptions or prompts
- **audio_analysis**: The capability to process, transcribe, or analyze audio content and speech
- **audio_gen**: The ability to synthesize speech or generate audio content from text or other inputs
- **video_analysis**: The capability to analyze, understand, and extract information from video content
- **video_gen**: The ability to create or generate video content from text descriptions or other inputs
- **reasoning**: The capability to perform logical reasoning, problem-solving, and complex analytical tasks

Return a ModelCapabilities object with boolean values for each capability, indicating whether the model supports (True) or does not support (False) each functionality.

You should also generate a factual description of the model (50-80 words), including:
- The model's architecture type and parameter size (if known)
- Specific capabilities it supports (multimodal inputs, function calling, etc.)
- Context window size and key technical specifications
- Release date or version information
- Primary intended use cases or design focus
- Any notable technical features or limitations

Avoid promotional language or subjective quality assessments. Focus on objective, verifiable information about what the model can do and its technical characteristics.
""".strip()


def get_capabilities_by_provider(
    provider: str, model_list: list[str]
) -> list[ModelSpec]:
    console = Console()
    SyncConduit._console = console  # Set the console for Conduit to use
    length = len(model_list)
    model = Model("sonar-pro")
    prompt = Prompt(prompt_str)
    parser = Parser(ModelSpecList)
    sync_conduit = SyncConduit(model=model, prompt=prompt, parser=parser)
    response = sync_conduit.run(
        input_variables={
            "provider": provider,
            "model_list": model_list,
            "length": length,
        }
    )
    return response.content.specs


def get_all_capabilities() -> list[ModelSpec]:
    """
    Get capabilities for all models across all providers. This shouldn't need to be run often, since it replaces the entire database of model specs.
    """
    all_models = Model.models()
    all_specs = []
    for index, (provider, models) in enumerate(all_models.items()):
        print(
            f"Processing {index + 1}/{len(all_models)}: {provider} with {len(models)} models"
        )
        specs = get_capabilities_by_provider(provider=provider, model_list=models)
        all_specs.extend(specs)
    return all_specs


def create_from_scratch() -> None:
    """
    Create a new empty database for ModelSpecs and populate it with capabilities from all providers.
    This will overwrite any existing data in modelspecs.json.
    """
    all_specs = get_all_capabilities()
    create_modelspecs_from_scratch(all_specs)
    print(f"Populated ModelSpecs database with {len(all_specs)} entries.")
    # Test retrieval of all specs
    retrieved_specs = get_all_modelspecs()
    assert len(retrieved_specs) == len(all_specs), (
        "Retrieved specs do not match created specs."
    )


def get_capabilities_by_model(provider: str, model: str) -> ModelSpec:
    """
    Get capabilities for a specific model.
    """
    console = Console()
    SyncConduit._console = console  # Set the console for Conduit to use
    model_obj = Model("sonar-pro")
    prompt = Prompt(individual_prompt_str)
    parser = Parser(ModelSpec)
    sync_conduit = SyncConduit(model=model_obj, prompt=prompt, parser=parser)
    response = sync_conduit.run(input_variables={"provider": provider, "model": model})
    return response.content


def create_modelspec(model: str) -> None:
    """
    Create a new ModelSpec in the database.
    """
    from conduit.core.model.models.modelstore import ModelStore

    provider = ModelStore.identify_provider(model)
    model_spec = get_capabilities_by_model(provider, model)
    if isinstance(model_spec, ModelSpec):
        model_spec.model = model
    else:
        raise ValueError(
            f"Expected ModelSpec, got {type(model_spec)} for model {model}"
        )
    if not in_db(model_spec):
        add_modelspec(model_spec)
        print(f"Added ModelSpec for {model_spec.model} to the database.")
    else:
        print(f"ModelSpec for {model_spec.model} already exists in the database.")


if __name__ == "__main__":
    modelspec = get_capabilities_by_model("ollama", "qwq:latest")

```

## FILE: src/conduit/core/model/models/tokens.py
```py
"""
Token estimation module for various LLM providers. NOTE: our odometer utilizes the token count provided in API response, which is more acccurate. This is for estimation only, primary usage is estimating window usage before making an API call.

Example usage:

    from tokens import OpenAITokenizer, AnthropicTokenizer, GeminiTokenizer, HuggingFaceTokenizer

    # OpenAI tokenizer
    openai_tokenizer = OpenAITokenizer()
    openai_tokens = openai_tokenizer.count_tokens("Hello, world!")

    # Anthropic tokenizer
    anthropic_tokenizer = AnthropicTokenizer()
    anthropic_tokens = anthropic_tokenizer.count_tokens("Hello, world!")

    # Gemini tokenizer
    gemini_tokenizer = GeminiTokenizer()
    gemini_tokens = gemini_tokenizer.count_tokens("Hello, world!")

    # Hugging Face tokenizer for a specific model
    hf_tokenizer = HuggingFaceTokenizer(model_hf_name="gpt2")
    hf_tokens = hf_tokenizer.count_tokens("Hello, world!")
"""

import abc
import os


class BaseTokenizer(abc.ABC):
    """
    Abstract base class for a tokenizer.
    Defines the common interface for encoding text and counting tokens.
    """

    @abc.abstractmethod
    def encode(self, text: str) -> list[int]:
        """
        Encodes a text string into a list of token IDs.
        """
        pass

    def count_tokens(self, text: str) -> int:
        """
        Counts the number of tokens in a given text string.
        """
        return len(self.encode(text))


class OpenAITokenizer(BaseTokenizer):
    """
    Tokenizer for OpenAI models using tiktoken for exact token counting.

    Provides accurate token counting for OpenAI models by lazy-loading the tiktoken library
    on first access. Uses the cl100k_base encoding by default, which is standard for most
    OpenAI models (GPT-3.5, GPT-4, etc.).
    """

    def __init__(self, encoding_name: str = "cl100k_base"):
        self.encoding_name = encoding_name
        self._tokenizer = None

    @property
    def tokenizer(self):
        """Lazy-loads the tiktoken tokenizer."""
        if self._tokenizer is None:
            try:
                import tiktoken

                print(f"[Lazy Load] Loading tiktoken for '{self.encoding_name}'...")
                self._tokenizer = tiktoken.get_encoding(self.encoding_name)
            except ImportError:
                raise ImportError(
                    "tiktoken is not installed. Please run: pip install tiktoken"
                )
        return self._tokenizer

    def encode(self, text: str) -> list[int]:
        return self.tokenizer.encode(text)


class AnthropicTokenizer(OpenAITokenizer):
    """
    Approximation tokenizer for Anthropic models using tiktoken encoding.

    Provides token counting for Anthropic Claude models by leveraging OpenAI's cl100k_base
    encoding. Note that actual server-side token counts from Anthropic often exceed reported
    values by 15-30%, making this suitable for estimates but not precise accounting.
    """

    def __init__(self, encoding_name: str = "cl100k_base"):
        print("---")
        print("WARNING: AnthropicTokenizer is an APPROXIMATION.")
        print("The actual token count may be 15-30% higher than reported.")
        print("---")
        super().__init__(encoding_name)


class GeminiTokenizer(BaseTokenizer):
    """
    Tokenizer for Google Gemini models using Hugging Face transformers.

    Provides near-exact token counting for Gemini by leveraging the google/gemma-7b
    tokenizer from Hugging Face. Lazy-loads the transformers library on first use
    and supports HuggingFace gated model access via environment variable tokens.
    """

    def __init__(self, model_hf_name: str = "google/gemma-7b"):
        self.model_hf_name = model_hf_name
        self._tokenizer = None

    @property
    def tokenizer(self):
        """
        Lazy-loads the transformers tokenizer.
        """
        if self._tokenizer is None:
            try:
                from transformers import AutoTokenizer

                # --- START MODIFICATION ---
                # Read token from env var
                token = os.getenv("HUGGINGFACEHUB_API_TOKEN") or os.getenv("HF_TOKEN")
                if not token:
                    print(
                        "WARNING: HUGGINGFACEHUB_API_TOKEN or HF_TOKEN env var not set. "
                        "Gated models will fail."
                    )

                print(f"[Lazy Load] Loading transformers for '{self.model_hf_name}'...")
                self._tokenizer = AutoTokenizer.from_pretrained(
                    self.model_hf_name,
                    token=token,  # Pass the token here
                )
                # --- END MODIFICATION ---

            except ImportError:
                raise ImportError(
                    "transformers is not installed. "
                    "Please run: pip install transformers sentencepiece"
                )
        return self._tokenizer

    def encode(self, text: str) -> list[int]:
        return self.tokenizer.encode(text)


class HuggingFaceTokenizer(BaseTokenizer):
    """
    Generic tokenizer for any Hugging Face model using transformers library.

    Supports local models via Ollama (e.g., Llama 3, Mistral) by lazy-loading
    the transformers AutoTokenizer. Handles authentication via HuggingFace API tokens
    from environment variables for gated model access.
    """

    def __init__(self, model_hf_name: str):
        if not model_hf_name:
            raise ValueError("model_hf_name must be provided for HuggingFaceTokenizer")
        self.model_hf_name = model_hf_name
        self._tokenizer = None

    @property
    def tokenizer(self):
        """
        Lazy-loads the transformers tokenizer.
        """
        if self._tokenizer is None:
            try:
                from transformers import AutoTokenizer

                # --- START MODIFICATION ---
                # Read token from env var
                token = os.getenv("HUGGINGFACEHUB_API_TOKEN") or os.getenv("HF_TOKEN")
                if not token:
                    print(
                        "WARNING: HUGGINGFACEHUB_API_TOKEN or HF_TOKEN env var not set. "
                        "Gated models will fail."
                    )

                print(f"[Lazy Load] Loading transformers for '{self.model_hf_name}'...")
                self._tokenizer = AutoTokenizer.from_pretrained(
                    self.model_hf_name,
                    token=token,  # Pass the token here
                )
                # --- END MODIFICATION ---

            except ImportError:
                raise ImportError(
                    "transformers is not installed. "
                    "Please run: pip install transformers"
                )
        return self._tokenizer

    def encode(self, text: str) -> list[int]:
        return self.tokenizer.encode(text)

```

## FILE: src/conduit/core/model/models/validate_capabilities.py
```py
"""
A set of test scripts that will validate whether a ModelCapabilities is valid or not.
For example, if a model spec says image generation is possible, generate an image.
"""


# Test functions
def generate_text_completion(model):
    """Test if the model can generate text completions."""
    raise NotImplementedError("Text completion generation is not implemented yet.")


def analyze_image(model):
    """Test if the model can analyze images."""
    raise NotImplementedError("Image analysis is not implemented yet.")


def generate_image(model):
    """Test if the model can generate images."""
    raise NotImplementedError("Image generation is not implemented yet.")


def analyze_audio(model):
    """Test if the model can analyze audio."""
    raise NotImplementedError("Audio analysis is not implemented yet.")


def generate_audio(model):
    """Test if the model can generate audio."""
    raise NotImplementedError("Audio generation is not implemented yet.")


def analyze_video(model):
    """Test if the model can analyze video."""
    raise NotImplementedError("Video analysis is not implemented yet.")


def generate_video(model):
    """Test if the model can generate video."""
    raise NotImplementedError("Video generation is not implemented yet.")

```

## FILE: src/conduit/core/parser/stream/logger.py
```py
"""
Logging utilities for debugging stream parsers.

Provides a wrapper that logs chunk arrivals in real-time, making it easy
to see exactly what content arrives in each chunk and diagnose parsing issues.
"""

import logging
from collections.abc import Iterator, AsyncIterator
from conduit.core.parser.stream.protocol import SyncStream, AsyncStream, StreamChunk


logger = logging.getLogger(__name__)


class StreamLogger:
    """
    Wrapper that logs stream chunks as they arrive.

    Three display modes:
    - "inline": Print content on same line as it arrives (for user display)
    - "per-chunk": Log each chunk separately with metadata (for debugging)
    - "silent": Collect chunks without output (for production)

    Usage:
        # For debugging - see each chunk
        logged_stream = StreamLogger(stream, name="debug", mode="per-chunk")

        # For user display - see flowing text
        logged_stream = StreamLogger(stream, name="response", mode="inline")

        # For production - just collect, no output
        logged_stream = StreamLogger(stream, name="prod", mode="silent")
    """

    def __init__(
        self,
        stream: SyncStream | AsyncStream,
        name: str = "stream",
        mode: str = "inline",  # "inline", "per-chunk", "silent"
        max_content_display: int = 100,
    ):
        """
        Initialize stream logger.

        Args:
            stream: The underlying stream to wrap
            name: Identifier for this stream in logs
            mode: Display mode - "inline", "per-chunk", or "silent"
            max_content_display: Max chars to show per chunk in per-chunk mode
        """
        self.stream = stream
        self.name = name
        self.mode = mode
        self.max_content_display = max_content_display
        self.chunk_count = 0
        self.total_chars = 0
        self._buffer = ""

        if mode not in ("inline", "per-chunk", "silent"):
            raise ValueError(
                f"Invalid mode: {mode}. Must be 'inline', 'per-chunk', or 'silent'"
            )

    def __iter__(self) -> Iterator[StreamChunk]:
        """Iterate over chunks with appropriate logging based on mode."""
        if self.mode == "inline":
            print(f"[{self.name}] ", end="", flush=True)
        elif self.mode == "per-chunk":
            logger.debug(f"[{self.name}] Stream starting...")

        for chunk in self.stream:
            self.chunk_count += 1

            # Extract content from chunk
            content = self._get_chunk_content(chunk)

            if content:
                self.total_chars += len(content)
                self._buffer += content

                if self.mode == "inline":
                    # Print on same line
                    print(content, end="", flush=True)
                elif self.mode == "per-chunk":
                    # Log each chunk with metadata
                    display_content = content
                    if len(content) > self.max_content_display:
                        display_content = content[: self.max_content_display] + "..."

                    logger.debug(
                        f"[{self.name}] Chunk {self.chunk_count}: {repr(display_content)} "
                        f"({len(content)} chars, {self.total_chars} total)"
                    )
            else:
                # Empty chunk (likely final usage chunk)
                if self.mode == "per-chunk":
                    logger.debug(
                        f"[{self.name}] Chunk {self.chunk_count}: <empty> "
                        f"(usage: {chunk.usage if hasattr(chunk, 'usage') else None})"
                    )

            yield chunk

        # Final summary
        if self.mode == "inline":
            print()  # Newline after inline content

        if self.mode != "silent":
            logger.info(
                f"[{self.name}] Complete: {self.chunk_count} chunks, {self.total_chars} chars"
            )

    async def __aiter__(self) -> AsyncIterator[StreamChunk]:
        """Async iterate over chunks with appropriate logging based on mode."""
        if self.mode == "inline":
            print(f"[{self.name}] ", end="", flush=True)
        elif self.mode == "per-chunk":
            logger.debug(f"[{self.name}] Async stream starting...")

        async for chunk in self.stream:
            self.chunk_count += 1

            content = self._get_chunk_content(chunk)

            if content:
                self.total_chars += len(content)
                self._buffer += content

                if self.mode == "inline":
                    print(content, end="", flush=True)
                elif self.mode == "per-chunk":
                    display_content = content
                    if len(content) > self.max_content_display:
                        display_content = content[: self.max_content_display] + "..."

                    logger.debug(
                        f"[{self.name}] Chunk {self.chunk_count}: {repr(display_content)} "
                        f"({len(content)} chars, {self.total_chars} total)"
                    )
            else:
                if self.mode == "per-chunk":
                    logger.debug(
                        f"[{self.name}] Chunk {self.chunk_count}: <empty> "
                        f"(usage: {chunk.usage if hasattr(chunk, 'usage') else None})"
                    )

            yield chunk

        if self.mode == "inline":
            print()

        if self.mode != "silent":
            logger.info(
                f"[{self.name}] Async complete: {self.chunk_count} chunks, {self.total_chars} chars"
            )

    def close(self) -> None:
        """Close the underlying stream."""
        if self.mode == "per-chunk":
            logger.debug(f"[{self.name}] Closing stream...")
        self.stream.close()

    async def aclose(self) -> None:
        """Close the underlying async stream."""
        if self.mode == "per-chunk":
            logger.debug(f"[{self.name}] Closing async stream...")
        await self.stream.close()

    def _get_chunk_content(self, chunk: StreamChunk) -> str | None:
        """
        Extract content from chunk, handling different provider formats.

        Different providers structure chunks differently:
        - OpenAI: chunk.choices[0].delta.content
        - Anthropic: chunk.delta.text
        - Google: chunk.text
        - Our test chunks: chunk.content
        """
        # Try test format first
        if hasattr(chunk, "content"):
            return chunk.content

        # Try OpenAI format
        if hasattr(chunk, "choices") and chunk.choices:
            delta = chunk.choices[0].delta
            if hasattr(delta, "content"):
                return delta.content

        # Try Anthropic format
        if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
            return chunk.delta.text

        # Try Google format
        if hasattr(chunk, "text"):
            return chunk.text

        return None

    def get_buffer(self) -> str:
        """Get accumulated content so far."""
        return self._buffer

    def get_stats(self) -> dict:
        """Get statistics about the stream."""
        return {
            "chunk_count": self.chunk_count,
            "total_chars": self.total_chars,
            "buffer_length": len(self._buffer),
        }

    def reset_stats(self) -> None:
        """Reset chunk count and character count."""
        self.chunk_count = 0
        self.total_chars = 0
        self._buffer = ""

```

## FILE: src/conduit/core/parser/stream/parsers.py
```py
import json
from abc import ABC, abstractmethod
from typing import Any
from conduit.core.parser.stream.protocol import SyncStream, AsyncStream


class StreamParser(ABC):
    """
    Abstract base class for stream parsers.

    Parsers consume streaming responses and extract structured content
    (XML or JSON) while tracking the full buffer for tokenization.

    Supports early termination: can close stream as soon as first complete
    object is found (useful for token efficiency with parallel async requests).
    """

    def __init__(self, stream: SyncStream | AsyncStream):
        self.stream = stream
        self.buffer = ""

    def parse(
        self, close_on_match: bool = True, check_interval: int = 1
    ) -> tuple[str, Any, str]:
        """
        Consume sync stream and parse.
        """
        self.buffer = ""
        chunk_count = 0

        try:
            for chunk in self.stream:
                content = self._get_chunk_content(chunk)
                if content:
                    self.buffer += content
                    chunk_count += 1

                # Check for complete object periodically
                if close_on_match and chunk_count % check_interval == 0:
                    text, obj = self._parse_buffer(self.buffer)
                    if obj:
                        # Found complete object - close stream to save tokens
                        if hasattr(self.stream, "close"):
                            self.stream.close()
                        break
        finally:
            # Ensure stream is always closed
            if hasattr(self.stream, "close"):
                self.stream.close()

        # Final parse on complete buffer
        text, obj = self._parse_buffer(self.buffer)
        return text, obj, self.buffer

    async def parse_async(
        self, close_on_match: bool = True, check_interval: int = 1
    ) -> tuple[str, Any, str]:
        """
        Consume async stream and parse.
        """
        self.buffer = ""
        chunk_count = 0

        try:
            # Iterate asynchronously over the stream
            async for chunk in self.stream:
                content = self._get_chunk_content(chunk)
                if content:
                    self.buffer += content
                    chunk_count += 1

                if close_on_match and chunk_count % check_interval == 0:
                    text, obj = self._parse_buffer(self.buffer)
                    if obj:
                        # Found object, close stream
                        if hasattr(self.stream, "close"):
                            # check if close is async (it usually is for async streams)
                            close_method = self.stream.close
                            if hasattr(close_method, "__call__") and hasattr(
                                close_method, "__await__"
                            ):
                                await close_method()
                            else:
                                close_method()
                        break
        finally:
            if hasattr(self.stream, "close"):
                close_method = self.stream.close
                if hasattr(close_method, "__call__") and hasattr(
                    close_method, "__await__"
                ):
                    await close_method()
                else:
                    close_method()

        text, obj = self._parse_buffer(self.buffer)
        return text, obj, self.buffer

    @abstractmethod
    def _parse_buffer(self, buffer: str) -> tuple[str, Any]:
        """
        Parse buffer and extract first structured object.
        Returns: (text_before_object, extracted_object)
        """
        ...

    def _get_chunk_content(self, chunk) -> str | None:
        """
        Extract content from chunk, handling different provider formats.
        """
        # Try test fixture format first
        if hasattr(chunk, "content"):
            return chunk.content

        # Try OpenAI format
        if hasattr(chunk, "choices") and chunk.choices:
            delta = chunk.choices[0].delta
            if hasattr(delta, "content"):
                return delta.content

        # Try Anthropic format
        if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
            return chunk.delta.text

        # Try Google format
        if hasattr(chunk, "text"):
            return chunk.text

        return None


class XMLStreamParser(StreamParser):
    """
    Parser for extracting XML objects from streams using state machine.
    """

    def __init__(
        self, stream: SyncStream | AsyncStream, tag_name: str = "function_calls"
    ):
        super().__init__(stream)
        self.tag_name = tag_name
        self.start_tag = f"<{tag_name}"
        self.end_tag = f"</{tag_name}>"

    def _parse_buffer(self, buffer: str) -> tuple[str, str | None]:
        # Find first occurrence of opening tag
        start_index = buffer.find(self.start_tag)
        if start_index == -1:
            return buffer, None

        # Initialize state machine
        depth = 1
        scan_head = start_index + len(self.start_tag)

        # Scan through buffer tracking depth
        while depth > 0:
            next_start = buffer.find(self.start_tag, scan_head)
            next_end = buffer.find(self.end_tag, scan_head)

            if next_end == -1:
                return buffer, None

            if next_start != -1 and next_start < next_end:
                depth += 1
                scan_head = next_start + len(self.start_tag)
            else:
                depth -= 1
                scan_head = next_end + len(self.end_tag)

        object_end_index = scan_head

        # Extract the complete XML object
        xml_obj = buffer[start_index:object_end_index]

        # Extract text BEFORE the object.
        # Everything after object_end_index is discarded (hallucination)
        text_before = buffer[:start_index]

        return text_before, xml_obj


class JSONStreamParser(StreamParser):
    """
    Parser for extracting JSON objects from streams using state machine.
    """

    def __init__(self, stream: SyncStream | AsyncStream):
        super().__init__(stream)

    def _parse_buffer(self, buffer: str) -> tuple[str, dict | list | None]:
        scan_head = 0
        while scan_head < len(buffer):
            # 1. Find first opening brace or bracket
            start_index = -1
            for i in range(scan_head, len(buffer)):
                if buffer[i] == "{" or buffer[i] == "[":
                    start_index = i
                    break

            if start_index == -1:
                return buffer, None

            start_char = buffer[start_index]
            end_char = "}" if start_char == "{" else "]"

            # 2. Initialize state machine
            depth = 1
            in_string = False
            escaped = False

            # 3. Scan char by char
            for i in range(start_index + 1, len(buffer)):
                char = buffer[i]

                if escaped:
                    escaped = False
                    continue

                if char == "\\":
                    escaped = True
                    continue

                if char == '"':
                    in_string = not in_string
                    continue

                if not in_string:
                    if char == start_char:
                        depth += 1
                    elif char == end_char:
                        depth -= 1

                    if depth == 0:
                        # 4. Found balanced structure
                        end_index = i
                        candidate_str = buffer[start_index : end_index + 1]

                        try:
                            # 5. Validate with json.loads()
                            parsed_obj = json.loads(candidate_str)

                            # Success!
                            # Return text BEFORE the object. Discard tail.
                            text_before = buffer[:start_index]
                            return text_before, parsed_obj

                        except json.JSONDecodeError:
                            scan_head = start_index + 1
                            break

            if depth != 0:
                return buffer, None

        return buffer, None

```

## FILE: src/conduit/core/parser/stream/protocol.py
```py
"""
Protocol definition for streaming response objects from LLM providers.

This protocol defines the minimal interface that streaming objects from different
providers (OpenAI, Anthropic, Google, Ollama) must satisfy to work with our parsers.
"""

from typing import Protocol, Any, runtime_checkable
from collections.abc import Iterator, AsyncIterator


@runtime_checkable
class StreamChunk(Protocol):
    """Protocol for individual chunks in a stream."""

    @property
    def content(self) -> str | None:
        """Text content delta in this chunk."""
        ...

    @property
    def usage(self) -> Any | None:
        """Usage information (only present in final chunk for some providers)."""
        ...


@runtime_checkable
class SyncStream(Protocol):
    """Protocol for synchronous streaming response objects."""

    def __iter__(self) -> Iterator[StreamChunk]:
        """Iterate over chunks as they arrive."""
        ...

    def close(self) -> None:
        """Close the stream and release resources."""
        ...


@runtime_checkable
class AsyncStream(Protocol):
    """Protocol for asynchronous streaming response objects."""

    def __aiter__(self) -> AsyncIterator[StreamChunk]:
        """Async iterate over chunks as they arrive."""
        ...

    async def close(self) -> None:
        """Close the stream and release resources."""
        ...

```

## FILE: src/conduit/core/prompt/prompt.py
```py
"""
Prompt class -- coordinates templates, input variables, and rendering.
"""

from __future__ import annotations
from jinja2 import Environment, StrictUndefined, meta, Template
from typing import TYPE_CHECKING, override
import logging

if TYPE_CHECKING:
    from pathlib import Path

logger = logging.getLogger(__name__)

# Define jinja2 environment that we will use across all prompts.
env = Environment(
    undefined=StrictUndefined
)  # set jinja2 to throw errors if a variable is undefined


class Prompt:
    """ "
    Takes a jinja2 ready string (note: not an actual Template object; that's created by the class).
    The three stages of prompt creation:
    - the prompt string, which is provided to the class
    - the jinja template created from the prompt string
    - the rendered prompt, which is returned by the class and submitted to the LLM model.
    """

    # Init methods
    def __init__(self, prompt_string: str):
        self.prompt_string: str = prompt_string
        self.template: Template = env.from_string(prompt_string)
        self.input_schema: set[str] = self._get_input_schema()

    def _get_input_schema(self) -> set[str]:
        """
        Returns a set of variable names from the template.
        This can be used to validate that the input variables match the template.
        """
        parsed_content = env.parse(self.prompt_string)
        return meta.find_undeclared_variables(parsed_content)

    # Main methods
    def render(self, input_variables: dict[str, str]) -> str:
        """
        takes a dictionary of variables
        """
        rendered_prompt = self.template.render(
            **input_variables
        )  # this takes all named variables from the dictionary we pass to this.
        return rendered_prompt

    # Factory functions
    @classmethod
    def from_file(cls, filename: str | Path) -> Prompt:
        """
        Creates a Prompt object from a file containing the prompt string.
        """
        from pathlib import Path

        # Coerce to Path object
        if not isinstance(filename, str):
            try:
                filename = Path(filename)
            except TypeError as e:
                logger.error(f"Invalid filename type: {type(filename)}")
                raise e

        assert isinstance(filename, Path), "filename must be a Path object"

        # Confirm file exists
        if not filename.exists():
            raise FileNotFoundError(f"Prompt file {filename} does not exist.")

        # Confirm file is .jinja2 or .jinja
        if filename.suffix not in {".jinja2", ".jinja"}:
            raise ValueError(
                f"Prompt file {filename} must be a .jinja2 or .jinja file."
            )

        # Read the file content
        with filename.open("r", encoding="utf-8") as file:
            prompt_string = file.read()

        # Create and return a Prompt object
        return cls(prompt_string)

    # Validation methods
    def validate_input_variables(self, input_variables: dict[str, str]) -> None:
        """
        Validates that the input variables match the template.
        """
        # Determine if prompt is expecting variables that are not provided
        missing_vars: set[str] = self.input_schema - input_variables.keys()
        if missing_vars:
            raise ValueError(
                f'Prompt is missing required input variable(s): "{'", "'.join(missing_vars)}"'
            )
        # Determine if extra variables are provided that the prompt does not expect
        extra_vars: set[str] = input_variables.keys() - self.input_schema
        if extra_vars:
            raise ValueError(
                f'Provided input variable(s) are not referenced in prompt: "{'", "'.join(extra_vars)}"'
            )

    def tokenize(self, model: str) -> int:
        """
        Tokenizes the prompt string using the specified model's tokenizer.
        """
        from conduit.core.model.model_sync import ModelSync

        model_obj = ModelSync(model)
        tokens: int = model_obj.tokenize(text=self.prompt_string)
        return tokens

    # Dunders
    @override
    def __repr__(self):
        attributes = ", ".join(
            [f"{k}={repr(v)[:50]}" for k, v in self.__dict__.items()]
        )
        return f"{self.__class__.__name__}({attributes})"

```

## FILE: src/conduit/core/prompt/prompt_loader.py
```py
"""
PromptLoader: Lazy-loading prompt registry.
This module provides a class to manage prompts, allowing for lazy loading of prompt files from a specified directory.

It supports auto-discovery of prompt files with extensions .jinja and .jinja2, and allows for custom keys to be specified.
It also ensures that prompt files are created if they do not exist, and caches loaded prompts for efficient access.

Production use case (auto-discovery)
    loader = PromptLoader('/path/to/prompts')
    prompt = loader['example_prompt']  # Loads example_prompt.jinja2 lazily

Development use case (scaffolding the directory + files):
    loader = PromptLoader('/path/to/prompts', keys=['example_prompt'])
    prompt = loader['example_prompt']  # Loads example_prompt.jinja2 lazily, creating the file if it doesn't exist
"""

from pathlib import Path


class PromptLoader:
    """Lazy-loading prompt registry for jinja/jinja2 template files."""

    def __init__(self, base_dir: str | Path, keys=None):
        self.base_dir = Path(base_dir)

        # Ensure base_dir exists
        self.base_dir.mkdir(parents=True, exist_ok=True)

        if keys is None:
            # Auto-discover jinja files
            self.file_map = {}
            for pattern in ["*.jinja", "*.jinja2"]:
                for file_path in self.base_dir.glob(pattern):
                    key = file_path.stem  # filename without extension
                    self.file_map[key] = file_path.name
        else:
            # Generate file_map from keys (key -> key.jinja2)
            self.file_map = {}
            for key in keys:
                filename = f"{key}.jinja2"
                self.file_map[key] = filename

                # Create file if it doesn't exist
                full_path = self.base_dir / filename
                if not full_path.exists():
                    full_path.touch()

        self._cache = {}

    @property
    def keys(self):
        return list(self.file_map.keys())

    def __getitem__(self, key):
        if key not in self._cache:
            if key not in self.file_map:
                raise KeyError(f"Unknown prompt: {key}")

            # Handle both strings and Path objects
            file_path = Path(self.file_map[key])
            if not file_path.is_absolute():
                file_path = self.base_dir / file_path

            from conduit.core.prompt.prompt import Prompt

            self._cache[key] = Prompt.from_file(file_path)

        return self._cache[key]

    def __contains__(self, key):
        return key in self.file_map

    def __str__(self):
        return f"PromptLoader(base_dir={self.base_dir}, keys={self.keys})"

```

## FILE: src/conduit/core/workflow/eval_adapter.py
```py
"""
Adapter layer for integrating Conduit workflows with Promptfoo evaluation framework. This module dynamically loads workflow definitions from dot-notation paths and executes them within the ConduitHarness observability context, enabling tracing and configuration injection. The `load_workflow_target()` function performs runtime module/class resolution to support both standalone functions and callable class instances, while `call_api()` wraps the execution for Promptfoo's evaluation protocol by extracting workflow references from config, running them with context management, and returning results with trace metadata for cost analysis and debugging.

Usage:
```yaml
providers:
  - id: python:eval_adapter.py
    label: "Email Flow (GPT-4)"
    config:
      # THE DYNAMIC PARTS
      workflow_target: "library.workflows.email.EmailWorkflow"
      entry_point: "run"

      # THE TUNABLE PARTS (Passed to Harness)
      model: "gpt-4"
      verbosity: "high"
```
```python
# Promptfoo config specifies workflow target and entry point
result = call_api(
    prompt="Your input prompt",
    options={"config": {"workflow_target": "my_app.flows.EmailWorkflow", "entry_point": "run"}},
    context={}
)
# Returns {"output": result_value, "metadata": {"trace": [...]}}
```
"""

import importlib
from collections.abc import Callable

# Use your actual imports here
from conduit.core.workflow.workflow import ConduitHarness


def load_workflow_target(target_path: str, method_name: str = None) -> Callable:
    """
    Dynamically loads a workflow function or class method from a string path.

    Args:
        target_path: Dot-notation path (e.g. "my_pkg.flows.email.EmailWorkflow")
        method_name: Optional method to call on the class instance (e.g. "run")

    Returns:
        The callable entry point for the workflow.
    """
    try:
        # 1. Split module and object name
        module_path, obj_name = target_path.rsplit(".", 1)

        # 2. Import the module
        module = importlib.import_module(module_path)

        # 3. Get the object (Function or Class)
        target_obj = getattr(module, obj_name)

        # CASE A: It's a Function -> Return it directly
        if not isinstance(target_obj, type):
            return target_obj

        # CASE B: It's a Class -> Instantiate it
        # Note: We assume a no-arg constructor for generic composability.
        # If your workflows need init params, inject them via Harness config instead.
        instance = target_obj()

        # CASE C: Specific Method (e.g., instance.run_workflow)
        if method_name:
            return getattr(instance, method_name)

        # CASE D: Callable Class (instance.__call__)
        return instance

    except (ImportError, AttributeError) as e:
        raise ImportError(f"Could not load workflow target '{target_path}': {e}")


def call_api(prompt, options, context):
    """
    Generic Promptfoo bridge.
    """
    # 1. Extract Config
    # We look for special keys 'workflow' and 'entry_point'
    run_config = options.get("config", {})

    target_path = run_config.pop("workflow_target", None)
    method_name = run_config.pop("entry_point", None)

    if not target_path:
        return {
            "error": "Missing 'workflow_target' in config (e.g. 'flows.email.EmailWorkflow')"
        }

    try:
        # 2. Dynamically Load the Logic
        workflow_callable = load_workflow_target(target_path, method_name)

        # 3. Initialize Harness (Trace/Config Context)
        harness = ConduitHarness(config=run_config)

        # 4. Execute
        # Note: We pass *prompt as the first arg. If your workflow takes multiple args,
        # you might need to parse 'prompt' as JSON or use run_config for others.
        output = harness.run(workflow_callable, prompt)

        # 5. Return to Promptfoo
        return {
            "output": output,
            "metadata": {
                "trace": harness.trace,
                # "cost": calculate_cost(harness.trace) # Add your helper here
            },
        }
    except Exception as e:
        # Return full traceback for easier debugging in Promptfoo UI
        import traceback

        return {"error": f"{str(e)}\n\n{traceback.format_exc()}"}

```

## FILE: src/conduit/core/workflow/workflow.py
```py
"""
OVERVIEW
--------
Workflow separates the DEFINITION of a workflow (code) from its OBSERVATION and
CONFIGURATION (runtime). It uses a "Harness" pattern to inject state via
ContextVars, keeping the domain logic pure.

CORE CONCEPTS
-------------
1. WORKFLOW (The Orchestrator)
   - A callable that defines the sequence of operations (A -> B -> C).
   - It is purely functional and stateless regarding infrastructure.

2. STEP (The Unit of Work)
   - A function decorated with `@step`.
   - It performs actual logic (LLM calls, data processing).
   - It automatically logs its inputs/outputs/latency to the Trace.
   - It pulls configuration via `get_param()` for runtime tuning.

3. STRATEGY (The Interchangeable Unit)
   - A Step that adheres to a strict interface (Protocol).
   - Used when a slot in a workflow can be filled by multiple implementations
     (e.g., 'Summarizer' can be 'FastSummarizer' or 'DeepSummarizer').

4. HARNESS (The Runtime)
   - Wraps the execution of a Workflow.
   - Manages the lifecycle of Trace and Config context variables.

TUNING & NAMESPACING
--------------------
Configuration is resolved using a specific precedence rule in `get_param()`:
   1. SCOPED:  "{step_name}.{param_key}" (e.g., "DraftEmail.model")
   2. GLOBAL:  "{param_key}"             (e.g., "model")
   3. DEFAULT: The value provided in code.

This allows you to set a global default (e.g., "gpt-3.5") while tuning specific
critical steps to use higher-performance models (e.g., "gpt-4") during evals.
"""

import functools
import inspect
import time
from contextvars import ContextVar
from typing import Any, Protocol, runtime_checkable

# --- INFRASTRUCTURE (THE "BUS") ---
# Holds the active configuration for the current run
_config_ctx = ContextVar("config", default={})
# Holds the trace log for the current run. If None, tracing is disabled.
_trace_ctx = ContextVar("trace", default=None)
# Holds a scratchpad for the CURRENTLY executing step -- for capturing things like token usage
_step_meta_ctx = ContextVar("step_meta", default=None)


# 2. NEW: The helper your strategies will use
def add_metadata(key: str, value: Any):
    """
    Log metadata to the current step (e.g., token usage, model version)
    without cluttering the return value.
    """
    meta = _step_meta_ctx.get()
    if meta is not None:
        meta[key] = value


def get_param(key: str, default: Any = None, scope: str | None = None) -> Any:
    """
    Retrieves a tunable parameter from the active context.

    Resolution Order:
    1. "{scope}.{key}" (Specific override)
    2. "{key}"         (Global override)
    3. default         (Hardcoded fallback)

    If `scope` is not provided, it attempts to infer the calling function's name.
    """
    cfg = _config_ctx.get()

    # Auto-detect scope if not provided
    if scope is None:
        # stack[1] is this function, stack[2] is the caller
        try:
            scope = inspect.stack()[1].function
        except (IndexError, AttributeError):
            scope = "unknown"

    # 1. Check Scoped
    scoped_key = f"{scope}.{key}"
    if scoped_key in cfg:
        return cfg[scoped_key]

    # 2. Check Global
    if key in cfg:
        return cfg[key]

    # 3. Default
    return default


def step(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()

        # Create a fresh scratchpad for this step
        current_meta = {}
        token_meta = _step_meta_ctx.set(current_meta)

        try:
            # Run the function (which might call add_metadata)
            result = func(*args, **kwargs)
            status = "success"
        except Exception as e:
            result = str(e)
            status = "error"
            raise e
        finally:
            duration = time.time() - start

            # Save to the global Trace Log
            trace_list = _trace_ctx.get()
            if trace_list is not None:
                trace_list.append(
                    {
                        "step": func.__name__,
                        "inputs": {"args": args, "kwargs": kwargs},  # Simplify for prod
                        "output": result,
                        "duration": round(duration, 4),
                        "status": status,
                        # HERE IS YOUR METADATA (Tokens, etc.)
                        "metadata": current_meta,
                    }
                )

            # Clean up the scratchpad (ContextVars handles nesting automatically!)
            _step_meta_ctx.reset(token_meta)

        return result

    return wrapper


# --- PROTOCOL DEFINITIONS ---
@runtime_checkable
class Step(Protocol):
    """
    A unit of work. Must be decorated with @step.
    """

    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...


@runtime_checkable
class Strategy(Protocol):
    """
    A Step with a standardized signature for interchangeability.
    """

    def __call__(self, input_data: Any) -> Any: ...


@runtime_checkable
class Workflow(Protocol):
    """
    The orchestrator script.
    """

    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...


class ConduitHarness:
    """
    The runtime container that manages Observability and Configuration contexts.
    """

    def __init__(self, config: dict = None):
        self.config = config or {}
        self.trace_log: list = []

    def run(self, workflow: Workflow, *args, **kwargs) -> Any:
        """
        Executes a workflow within the managed context.
        """
        # Mount Context
        token_conf = _config_ctx.set(self.config)
        token_trace = _trace_ctx.set(self.trace_log)

        try:
            return workflow(*args, **kwargs)
        finally:
            # Unmount Context
            _config_ctx.reset(token_conf)
            _trace_ctx.reset(token_trace)

    @property
    def trace(self) -> list:
        return self.trace_log


if __name__ == "__main__":
    # --- EXAMPLE USAGE ---
    @step
    def DraftEmail(topic: str) -> str:
        model = get_param("model", default="gpt-3.5")
        # result = Model(model).query(f"Draft an email about {topic}")
        add_metadata("model_used", model)
        add_metadata("input_tokens", 50)
        add_metadata("output_tokens", 150)
        return f"Drafted email about '{topic}' using model {model}."

    @step
    def SummarizeEmail(draft: str) -> str:
        model = get_param("model", default="gpt-3.5")
        length = get_param("length", default="short")
        # result = Model(model).query(f"Summarize this email in a {length} format: {draft}")
        add_metadata("model_used", model)
        add_metadata("input_tokens", 150)
        add_metadata("output_tokens", 50)
        return f"Summarized ({length}): {draft}"

    def EmailWorkflow(topic: str) -> str:
        draft = DraftEmail(topic)
        summary = SummarizeEmail(draft)
        return summary

    harness = ConduitHarness(
        config={
            "DraftEmail.model": "gpt-4",
            "SummarizeEmail.model": "gp3",
            "length": "detailed",
        }
    )

    result = harness.run(EmailWorkflow, topic="AI in Healthcare")

```

## FILE: src/conduit/domain/config/conduit_options.py
```py
from __future__ import annotations
from conduit.utils.progress.verbosity import Verbosity
from conduit.storage.repository.persistence_mode import PersistenceMode
from pydantic import BaseModel, Field, ConfigDict, field_validator
from conduit.storage.cache.protocol import ConduitCache
from conduit.storage.repository.protocol import AsyncSessionRepository
from conduit.capabilities.tools.registry import ToolRegistry
from rich.console import Console


class ConduitOptions(BaseModel):
    """
    Muscle Tissue: Runtime configuration for the Conduit application.
    Controls side-effects like logging, caching, and UI, separate from LLM inference parameters.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    project_name: str
    verbosity: Verbosity = Field(
        default=Verbosity.PROGRESS, description="Verbosity level for logging and UI."
    )
    cache: ConduitCache | None = Field(
        default=None,
        description="Cache backend for storing/retrieving generations.",
        exclude=True,
    )
    repository: AsyncSessionRepository | None = Field(
        default=None,
        description="Repository backend for persisting conversations.",
        exclude=True,
    )
    console: Console | None = Field(
        default_factory=lambda: Console(),
        description="Rich console for enhanced logging/UI.",
        exclude=True,
    )
    tool_registry: ToolRegistry | None = Field(
        default=None,
        description="Registry of available tools for the LLM to use.",
        exclude=True,
    )
    parallel_tool_calls: bool = Field(
        default=True,
        description="Enable parallel tool calls (multiple tools in one turn). Supported by OpenAI, Google, and Ollama.",
    )

    # Overrides for request behavior
    use_cache: bool | None = True  # Technically: "if cache exists, use it"
    include_history: bool = True  # Whether to include conversation history
    persistence_mode: PersistenceMode = PersistenceMode.RESUME

    # Dev options
    debug_payload: bool = False  # Log full request/response payloads for debugging
    use_remote: bool = False  # Whether to use remote server for model execution

    @field_validator("verbosity")
    @classmethod
    def verbosity_must_not_be_none(cls, v):
        if v is None:
            raise ValueError("verbosity cannot be None")
        return v

```

## FILE: src/conduit/domain/conversation/conversation.py
```py
from __future__ import annotations
import uuid
from typing import TYPE_CHECKING, override
from collections.abc import Sequence
from enum import Enum

from pydantic import BaseModel, model_validator, Field

from conduit.config import settings
from conduit.domain.message.message import MessageUnion, Message, ToolCall
from conduit.domain.message.role import Role
from conduit.domain.exceptions.exceptions import ConversationError
from conduit.domain.conversation.session import Session

if TYPE_CHECKING:
    from rich.console import Console, ConsoleOptions, RenderResult


class ConversationState(Enum):
    GENERATE = "generate"
    EXECUTE = "execute"
    TERMINATE = "terminate"
    INCOMPLETE = "incomplete"


class Conversation(BaseModel):
    """
    A Conversation is a sequence of Messages with metadata and helper methods.
    This a runtime object, not a persistence object. Conversation orchestrates the Session and Messages, with Session happening under the hood.
    A Conversation can be generated entirely from the message ID of a leaf Message.
    ONLY use the 'add' method to append new messages to ensure validation rules.
    Conversations are the primary interface for agents and chat-based LLM interactions, and as such, are where Messages are given their session and predecessor metadata.
    """

    topic: str = "Untitled"  # Note: for most applications, we auto label the Session, not the Conversation
    messages: list[MessageUnion] = Field(default_factory=list)
    session: Session | None = Field(default=None, repr=False)
    leaf: str | None = Field(default=None, repr=False)

    # Initialization Hooks
    @override
    def model_post_init(self, __context: object):
        """
        Bootstrap the session if initialized with a list of messages.
        """
        if self.messages:
            last_msg = self.messages[-1]
            self.leaf = last_msg.message_id

            # Check for viral session_id on the last message
            seed_id = getattr(last_msg, "session_id", None)

            self.initialize_session(
                leaf=self.leaf, session_id=seed_id, initial_messages=self.messages
            )

    def initialize_session(
        self,
        leaf: str,
        session_id: str | None = None,
        initial_messages: Sequence[MessageUnion] | None = None,
    ) -> None:
        """
        Lazily creates the Session object.
        """
        if self.session is None:
            # 1. Resolve ID (Viral > Random)
            final_id = session_id or f"session_{uuid.uuid4()}"

            # 2. Build initial dict
            msgs = initial_messages or []
            msg_dict = {m.message_id: m for m in msgs}

            # 3. Create Session
            self.session = Session(
                session_id=final_id, message_dict=msg_dict, leaf=leaf
            )

    @model_validator(mode="after")
    def validate_messages(self):
        messages = self.messages
        system_messages = [m for m in messages if m.role == Role.SYSTEM]

        if len(system_messages) > 1:
            raise ConversationError(
                "Multiple system messages found in the conversation."
            )
        elif len(system_messages) == 1:
            self.ensure_system_message(system_messages[0].content)
        return self

    # Methods to manipulate conversation
    def add(self, message: Message) -> None:
        """
        Append a new message to the conversation.
        Intercepts the addition to bootstrap the Session if needed.
        Enforces validation rules on message addition.
        1. Validates that system messages are only first, and no consecutive same-role messages
        2. Bootstraps the Session if not already initialized
        3. Updates the Session state with the new message
        4. Populates message metadata (predecessor_id, session_id)
        5. Updates the Conversation view (messages list, leaf pointer)
        """
        # --- 1. Validation First ---
        if len(self.messages) > 0:
            if message.role == Role.SYSTEM:
                raise ConversationError(
                    "System messages can only be the first message."
                )

            if message.role != Role.TOOL and message.role == self.messages[-1].role:
                raise ConversationError(
                    f"Cannot add two consecutive messages with the same role: {message.role.value}."
                )

        # --- 2. Bootstrap Session (Lazy Load) ---
        if self.session is None:
            seed_id = getattr(message, "session_id", None)
            # Initialize with THIS message
            self.initialize_session(
                leaf=message.message_id, session_id=seed_id, initial_messages=[message]
            )

        # --- 3. Update State ---
        assert self.session is not None
        self.session.register(message)

        # --- 4. Message metadata
        ## Lots of assertions to ensure integrity
        if self.last:
            assert self.last.predecessor_id == self.leaf
        predecessor_id = self.leaf
        assert self.session.session_id is not None, (
            "Session ID should be set after initialization."
        )
        if self.last:
            assert self.session.session_id == self.last.session_id, (
                "Session ID mismatch between Conversation and last Message."
            )
        session_id = self.session.session_id
        # Populate message metadata
        message.predecessor_id = predecessor_id
        message.session_id = session_id

        # --- 5. Update View ---
        self.messages.append(message)
        self.leaf = message.message_id

    def wipe(self) -> None:
        self.messages = []

    def prune(self, keep: int = 10) -> None:
        if len(self.messages) > keep:
            self.messages = self.messages[-keep:]

    def tokens(self, model_name: str) -> int:
        raise NotImplementedError(
            "Token counting not yet implemented for Conversation."
        )

    def ensure_system_message(
        self, system_content: str = settings.system_prompt
    ) -> None:
        from conduit.domain.message.message import SystemMessage

        system_message = SystemMessage(content=system_content)
        self.system = system_message

    def label(self, topic: str) -> None:
        """
        Automatic labeling happens to Session, not Conversation, but there are times when you might want to label a Conversation directly, for persistence. This stores a topic on the Message leaf metadata.
        """
        self.topic = topic
        self.leaf.metadata["topic"] = topic

    # Properties
    @property
    def last(self) -> Message | None:
        if self.messages:
            return self.messages[-1]
        return None

    @property
    def system(self) -> Message | None:
        system_messages = [m for m in self.messages if m.role == Role.SYSTEM]
        if len(system_messages) == 1:
            return system_messages[0]
        elif len(system_messages) > 1:
            self.messages = [m for m in self.messages if m.role != Role.SYSTEM]
            self.messages.insert(0, system_messages[0])
            return system_messages[0]
        elif len(system_messages) == 0:
            return None

    @system.setter
    def system(self, message: Message):
        if message.role != Role.SYSTEM:
            raise ConversationError("Only system messages can be assigned to system.")

        existing_system = self.system
        if existing_system:
            index = self.messages.index(existing_system)
            self.messages[index] = message
        else:
            self.messages.insert(0, message)

        # Sync to session
        if self.session:
            self.session.register(message)

    @property
    def content(self) -> str:
        if self.last:
            return str(self.last.content)
        return ""

    @property
    def roles(self) -> str:
        roles_string = ""
        for message in self.messages:
            roles_string += message.role.value[0].upper()
        return roles_string

    @property
    def tool_calls(self) -> list[ToolCall]:
        if self.last:
            if self.last.tool_calls:
                return self.last.tool_calls
            else:
                raise ConversationError("Last message has no tool calls.")
        else:
            raise ConversationError(
                "No messages in conversation; cannot get tool calls."
            )

    @property
    def state(self) -> ConversationState:
        if not self.last:
            return ConversationState.INCOMPLETE
        match self.last.role:
            case Role.USER:
                return ConversationState.GENERATE
            case Role.TOOL:
                return ConversationState.GENERATE
            case Role.ASSISTANT:
                if self.last.tool_calls:
                    return ConversationState.EXECUTE
                else:
                    return ConversationState.TERMINATE
            case Role.SYSTEM:
                return ConversationState.INCOMPLETE

    # Display Methods
    @override
    def __str__(self) -> str:
        output = ""
        for message in self.messages:
            output += f"{message.role.value.upper()}: {message.content}\n"
        return output.strip()

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        for message in self.messages:
            yield from message.__rich_console__(console, options)
        return

    def pretty_print(self) -> None:
        from rich.console import Console

        console = Console()
        console.print(self)

    def print_history(self, max_messages: int = 100) -> None:
        from rich.console import Console

        console = Console()
        truncated_messages = []
        for message in self.messages[-max_messages:]:
            truncated_content = str(message.content)
            truncated_content = " ".join(truncated_content.split())
            truncated_content = " ".join(truncated_content.split("  "))
            truncated_content = truncated_content.replace("**", "").replace("*", "")
            truncated_content = truncated_content.replace("`", "").replace("```", "")
            truncated_content = truncated_content.replace("_", "")
            truncated_content = truncated_content.replace("#", "")
            if len(truncated_content) > 120:
                truncated_content = truncated_content[:117] + "..."
            truncated_message = message.model_copy(
                update={"content": truncated_content}
            )
            truncated_messages.append(truncated_message)
        truncated_conversation = Conversation(
            topic=self.topic,
            messages=truncated_messages,
        )
        console.print(truncated_conversation)

```

## FILE: src/conduit/domain/conversation/generate_title.py
```py
from conduit.core.conduit.conduit_sync import ConduitSync
from conduit.core.prompt.prompt import Prompt
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.conversation.conversation import Conversation


prompt_str = """
Generate a concise title for this conversation.

<system_context>{{ system_message }}</system_context>

<user_request>{{ user_message }}</user_request>

Output a noun phrase (4-7 words) that captures the user's intent. No articles at the start. No punctuation. No quotes. No preamble. Just the title.
""".strip()

prompt = Prompt(prompt_str)
conduit = ConduitSync.create(prompt=prompt, model="gpt3", verbose=Verbosity.SILENT)


def generate_title(conversation: Conversation) -> str:
    if conversation.roles != "SU":
        raise ValueError(
            "Conversation must have roles 'SU' (System, User) to generate title."
        )

    system_string = str(conversation.system.content) if conversation.system else ""
    user_string = conversation.content
    title = _generate_title(system_string, user_string)
    return title


def _generate_title(system_message: str, user_message: str) -> str:
    response = conduit(
        system_message=system_message,
        user_message=user_message,
    )
    title = response.content.strip()
    return title

```

## FILE: src/conduit/domain/conversation/session.py
```py
from __future__ import annotations
from pydantic import BaseModel, Field
from conduit.domain.message.message import Message
import time
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.domain.conversation.conversation import Conversation


class Session(BaseModel):
    """
    Session is both a runtime and persistent object.
    It holds all messages generated in a session (the "Graph").
    """

    session_id: str = Field(..., description="Unique identifier for the session")
    message_dict: dict[str, Message] = Field(
        ..., description="Mapping of message_id to Message objects"
    )
    leaf: str = Field(
        ..., description="The latest message id in the session -- for persistence"
    )
    created_at: int = Field(default_factory=lambda: int(time.time() * 1000))

    def register(self, message: Message) -> None:
        """
        Idempotent registration of a message into the session graph.
        Updates the session's leaf pointer to this new message.
        """
        # 1. Add to graph (Idempotent)
        self.message_dict[message.message_id] = message

        # 2. Update Cursor
        # In a strict tree append, the new message is always the new leaf.
        self.leaf = message.message_id

    def to_conversation(
        self, message_id: str, ensure_leaf: bool = True
    ) -> Conversation:
        """
        Reconstruct a conversation for a given leaf (by message_id).
        """
        from conduit.domain.conversation.conversation import Conversation

        if message_id not in self.message_dict:
            raise KeyError(f"Message ID {message_id} not found in session.")

        # If ensure_leaf, verify that the requested message_id is a TERMINATING message (no children).
        if ensure_leaf:
            is_leaf = all(
                msg.predecessor_id != message_id for msg in self.message_dict.values()
            )
            if not is_leaf:
                raise ValueError(f"Message ID {message_id} is not a leaf message.")

        # Reconstruct the message chain
        messages = []
        current_id = message_id
        while current_id:
            msg = self.message_dict[current_id]
            messages.append(msg)
            current_id = msg.predecessor_id
            if current_id == message_id:
                raise ValueError(
                    "Conversation is an infinite loop due to circular reference."
                )
        messages.reverse()  # Reverse to get chronological order
        return Conversation(messages=messages)

    def branch(self, from_message_id: str) -> Conversation:
        """
        Create a new conversation branch from an existing message in the session.
        """
        if from_message_id not in self.message_dict:
            raise KeyError(f"Message ID {from_message_id} not found in session.")

        message = self.message_dict[from_message_id]
        if message.predecessor_id is None:
            raise ValueError("Cannot branch from a root message.")
        if message.predecessor_id not in self.message_dict:
            raise ValueError(
                "Cannot branch from a message whose predecessor is not in the session."
            )

        return self.to_conversation(from_message_id, ensure_leaf=False)

    @property
    def conversation(self) -> Conversation:
        """
        Get the conversation for the current leaf message.
        """
        return self.to_conversation(self.leaf, ensure_leaf=True)

    @property
    def leaves(self) -> list[Message]:
        """
        Get all leaf messages in the session.
        """
        leaf_messages = []
        for msg in self.message_dict.values():
            is_leaf = all(
                other_msg.predecessor_id != msg.message_id
                for other_msg in self.message_dict.values()
            )
            if is_leaf:
                leaf_messages.append(msg)
        return leaf_messages

    @property
    def conversations(self) -> list[Conversation]:
        """
        Get all conversations for each leaf in the session.
        """
        convos = []
        for leaf_msg in self.leaves:
            convo = self.to_conversation(leaf_msg.message_id, ensure_leaf=True)
            convos.append(convo)
        return convos

```

## FILE: src/conduit/domain/exceptions/exceptions.py
```py
class ConduitError(Exception):
    """Base class for all conduit-related exceptions."""

    pass


class EngineError(ConduitError):
    """Base class for all engine-related exceptions."""

    pass


class ModelError(ConduitError):
    """Exception raised for errors related to model operations."""

    pass


class OrchestrationError(ConduitError):
    """Exception raised for errors related to conduit operations."""

    pass


class ConversationError(ConduitError):
    """Exception raised for errors related to conversation operations."""

    pass


class ToolError(ConduitError):
    """Exception raised for errors related to tool operations."""

    pass


class ChatError(ConduitError):
    """Exception raised for errors related to chat operations."""

    pass


class CLIError(ConduitError):
    """Exception raised for errors related to CLI operations."""

    pass

```

## FILE: src/conduit/domain/message/message.py
```py
"""
 NOTE: Typing is a mess in python. We have two types for Message:
- Message: the base class for all messages, used for isinstance checks.
- MessageUnion: a discriminated union of all message types, used for parsing/serialization.

* Enums don't work as discriminators in pydantic, so we use a new field 'role_str' for that purpose.
"""

from __future__ import annotations
import base64
import functools
import logging
import mimetypes
import time
import uuid
from pathlib import Path
from typing import Literal, Any, Annotated, override, TYPE_CHECKING
from pydantic import BaseModel, Field, model_validator
from conduit.domain.message.role import Role

if TYPE_CHECKING:
    from rich.console import Console, ConsoleOptions, RenderResult

logger = logging.getLogger(__name__)


# Content types
class TextContent(BaseModel):
    type: Literal["text"] = "text"
    text: str


class ImageContent(BaseModel):
    """
    Internal representation of an image.
    Ideally normalized to base64 or a stable URL before reaching here.
    """

    type: Literal["image_url"] = "image_url"
    url: str  # data:image/png;base64,... or https://...
    detail: Literal["auto", "low", "high"] = "auto"

    @classmethod
    def from_file(cls, file_path: str | Path) -> ImageContent:
        """
        Load image from a file and encode as base64 data URL.
        """
        file_path = Path(file_path)
        with open(file_path, "rb") as f:
            image_bytes = f.read()
        image_b64 = base64.b64encode(image_bytes).decode("utf-8")
        mime_type, _ = mimetypes.guess_type(file_path)
        if mime_type is None:
            mime_type = "image/png"  # Default to PNG
        data_url = f"data:{mime_type};base64,{image_b64}"
        return cls(url=data_url)


class AudioContent(BaseModel):
    """
    Internal representation of input audio.
    """

    type: Literal["input_audio"] = "input_audio"
    data: str  # Base64 encoded audio
    format: Literal["wav", "mp3"] = "mp3"

    @classmethod
    def from_file(cls, file_path: str | Path) -> AudioContent:
        """
        Load audio from a file and encode as base64.
        """
        file_path = Path(file_path)
        with open(file_path, "rb") as f:
            audio_bytes = f.read()
        audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")
        return cls(data=audio_b64, format=file_path.suffix.lstrip("."))


class AudioOutput(BaseModel):
    """Native audio response (e.g. GPT-4o-audio)."""

    id: str
    data: str  # Base64 encoded audio
    transcript: str | None = None  # The text representation
    format: Literal["wav", "mp3", "pcm16"] = "wav"


class ImageOutput(BaseModel):
    """Generated image response (e.g. DALL-E 3)."""

    url: str | None = None
    b64_json: str | None = None
    revised_prompt: str | None = None  # DALL-E often rewrites the prompt


# All possible user content types
Content = str | dict | list[TextContent | ImageContent | AudioContent | str]


# tool primitive -- see conduit.capabilities.tool.Tool for more its mirror
type JsonPrimitive = str | int | float | bool | None
type JsonValue = JsonPrimitive | list[JsonValue] | dict[str, JsonValue]


class ToolCall(BaseModel):
    """
    Canonical request from the assistant to execute a function tool.
    """

    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: Literal["function"] = "function"

    function_name: str
    arguments: dict[str, JsonValue]

    # Optional but very useful for adapters/debugging
    provider: str | None = None  # "openai" | "anthropic" | "gemini" | "ollama" ...
    raw: dict[str, JsonValue] | None = None


# message types
class Message(BaseModel):
    """
    Base class for all message types.
    Use this for isinstance checks.
    """

    # Content -- quite simple.
    role: Role
    content: Content | None

    # Metadata
    created_at: int = Field(default_factory=lambda: int(time.time() * 1000))
    message_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    # Optional metadata for extensibility (like conversation tags like title)
    metadata: dict[str, Any] | None = None

    # For threading/conversation continuity
    predecessor_id: str | None = None
    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))

    @model_validator(mode="after")
    def _validate_session_continuity(self):
        """
        Validate that child messages explicitly specify their session context.

        Ensures that if a message references a predecessor (forming a child node in a
        conversation tree), the session_id must be explicitly provided rather than
        auto-generated. This prevents orphaned child messages that lack proper session
        continuity context.
        """
        # We only care if a predecessor exists (it's a child node)
        if (
            self.predecessor_id is not None
            and "session_id" not in self.model_fields_set
        ):
            raise ValueError(
                "Orphaned Child: If 'predecessor_id' is supplied, you must explicitly provide the existing 'session_id'."
            )
        return self

    @model_validator(mode="before")
    @classmethod
    def _disallow_base_instantiation(cls, data):
        if cls is Message:
            raise TypeError(
                "Message is an abstract base class. Use SystemMessage, UserMessage, AssistantMessage, or ToolMessage."
            )
        return data

    @property
    def time(self) -> str:
        """
        Human-readable time string.
        """
        return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.timestamp / 1000))

    @override
    def __hash__(self) -> int:
        return hash(self.role.value + str(self.content))

    def _extract_text_content(self) -> str:
        """Helper to safely extract string payload from complex Content types."""
        if self.content is None:
            return ""
        if isinstance(self.content, str):
            return self.content
        if isinstance(self.content, list):
            parts = []
            for item in self.content:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, TextContent):
                    parts.append(item.text)
                elif isinstance(item, ImageContent):
                    parts.append("[Image Content]")
                elif isinstance(item, AudioContent):
                    parts.append("[Audio Content]")
            return "\n".join(parts)
        return str(self.content)

    @override
    def __str__(self) -> str:
        """
        The "Pipe" View.
        Returns the pure text payload for piping/clipboard.
        """
        return self._extract_text_content()

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        """
        The "Pretty" View.
        """
        from rich.text import Text

        yield Text(str(self))


class SystemMessage(Message):
    """
    System instructions.
    """

    role: Role = Role.SYSTEM
    role_str: Literal["system"] = "system"
    content: Content | None

    @override
    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        from rich.panel import Panel
        from rich.markdown import Markdown
        from rich.box import ROUNDED

        yield Panel(
            Markdown(self._extract_text_content()),
            title=f"System • [dim]{self.time}[/dim]",
            title_align="left",
            border_style="white",
            box=ROUNDED,
            padding=(0, 2),
        )


class UserMessage(Message):
    """
    Messages sent by the human.
    """

    role: Role = Role.USER
    role_str: Literal["user"] = "user"
    content: Content | None
    name: str | None = None

    @override
    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        from rich.panel import Panel
        from rich.markdown import Markdown
        from rich.rule import Rule
        from rich.text import Text
        from rich.console import Group
        from rich.box import ROUNDED

        text_content = self._extract_text_content()
        header = f"User • [dim]{self.time}[/dim]"

        renderables = []
        if isinstance(self.content, list):
            images = [i for i in self.content if isinstance(i, ImageContent)]
            audio = [a for a in self.content if isinstance(a, AudioContent)]

            if images:
                renderables.append(
                    Text(f"{len(images)} Image(s) Attached", style="cyan italic")
                )
            if audio:
                renderables.append(
                    Text(f"{len(audio)} Audio Clip(s) Attached", style="cyan italic")
                )
            if images or audio:
                renderables.append(Rule(style="dim"))

        renderables.append(Markdown(text_content))

        yield Panel(
            Group(*renderables),
            title=header,
            title_align="left",
            border_style="green",
            box=ROUNDED,
            padding=(0, 2),
        )


class AssistantMessage(Message):
    """
    Messages generated by the LLM.
    """

    role: Role = Role.ASSISTANT
    role_str: Literal["assistant"] = "assistant"

    # text
    content: Content | None = None
    reasoning: str | None = None

    # action
    tool_calls: list[ToolCall] | None = None

    # multimodal
    audio: AudioOutput | None = None
    images: list[ImageOutput] | None = None

    # structured output
    parsed: BaseModel | list[BaseModel] | None = Field(default=None, exclude=True)

    @functools.cached_property
    def perplexity_content(self) -> Any:
        """
        Lazy factory for Perplexity responses with citations.
        """
        if not isinstance(self.content, dict):
            return None

        from conduit.core.clients.perplexity.perplexity_content import (
            PerplexityContent,
            PerplexityCitation,
        )

        if "text" in self.content and "citations" in self.content:
            citations = [PerplexityCitation(**c) for c in self.content["citations"]]
            return PerplexityContent(
                text=self.content["text"],
                citations=citations,
            )

        if "citations" in self.content and self.parsed is not None:
            text = self.parsed.model_dump_json(indent=2)
            citations = [PerplexityCitation(**c) for c in self.content["citations"]]
            return PerplexityContent(
                text=text,
                citations=citations,
            )

        return None

    @model_validator(mode="after")
    def validate_structure(self) -> AssistantMessage:
        has_payload = any(
            [
                self.content,
                self.tool_calls,
                self.reasoning,
                self.audio,
                self.images,
                self.parsed,
            ]
        )

        if not has_payload:
            raise ValueError(
                "AssistantMessage must have at least one of: content, tool_calls, reasoning, audio, images, or parsed output."
            )
        return self

    @override
    def __str__(self) -> str:
        if self.content:
            return self._extract_text_content()
        if self.tool_calls:
            return "\n".join(
                [f"[ToolCall: {tc.function_name}]" for tc in self.tool_calls]
            )
        return ""

    @override
    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        from rich.panel import Panel
        from rich.markdown import Markdown
        from rich.syntax import Syntax
        from rich.text import Text
        from rich.console import Group
        from rich.box import HEAVY

        renderables = []

        # 1. Reasoning
        if self.reasoning:
            renderables.append(
                Panel(
                    Markdown(self.reasoning),
                    title="Chain of Thought",
                    style="dim",
                    border_style="dim",
                )
            )

        # 2. Main Content
        if self.content:
            renderables.append(Markdown(self._extract_text_content()))

        # 3. Tool Calls
        if self.tool_calls:
            for tool in self.tool_calls:
                tool_code = f"{tool.function_name}({tool.arguments})"
                renderables.append(
                    Panel(
                        Syntax(tool_code, "python", theme="monokai", word_wrap=True),
                        title=f"Call: {tool.function_name}",
                        border_style="yellow",
                    )
                )

        # 4. Multimodal Outputs
        if self.images:
            renderables.append(
                Text(f"Generated {len(self.images)} Image(s)", style="cyan")
            )
        if self.audio:
            transcript = f" ({self.audio.transcript})" if self.audio.transcript else ""
            renderables.append(Text(f"Generated Audio{transcript}", style="cyan"))

        # 5. Structured Parsed Output
        if self.parsed:
            json_str = (
                self.parsed.model_dump_json(indent=2)
                if isinstance(self.parsed, BaseModel)
                else str(self.parsed)
            )
            renderables.append(
                Panel(
                    Syntax(json_str, "json", theme="monokai", word_wrap=True),
                    title="Structured Output",
                    border_style="cyan",
                )
            )

        yield Panel(
            Group(*renderables),
            title=f"Assistant • [dim]{self.time}[/dim]",
            title_align="left",
            border_style="blue",
            box=HEAVY,
            padding=(0, 2),
        )


class ToolMessage(Message):
    """
    The result of a tool execution, fed back to the LLM.
    """

    role: Role = Role.TOOL
    role_str: Literal["tool"] = "tool"
    content: str  # The output of the tool (stringified)
    tool_call_id: str  # Links this result to the Assistant's ToolCall.id
    name: str | None = None  # Optional: name of the tool function

    @classmethod
    def from_result(
        cls, result: Any, tool_call_id: str, name: str | None = None
    ) -> ToolMessage:
        """
        Factory to create a ToolMessage from any raw Python result (dict, list, pydantic model).
        This handles the serialization so your tool functions don't have to.
        """
        import json

        if isinstance(result, (dict, list, int, float, bool, type(None))):
            # Standard JSON serialization
            content = json.dumps(result, default=str)
        elif isinstance(result, BaseModel):
            # Pydantic serialization
            content = result.model_dump_json()
        else:
            # Fallback for strings or unknown objects
            content = str(result)

        return cls(content=content, tool_call_id=tool_call_id, name=name)

    @override
    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:
        import json
        from rich.panel import Panel
        from rich.syntax import Syntax
        from rich.markdown import Markdown
        from rich.box import ROUNDED

        # Attempt to parse strictly for display purposes
        try:
            # Check if it's valid JSON
            json.loads(self.content)
            # It is JSON: use Syntax highlighting
            content_renderable = Syntax(
                self.content, "json", theme="monokai", word_wrap=True
            )
        except (ValueError, TypeError):
            # It is NOT JSON: treat as plain text/markdown
            content_renderable = Markdown(self.content)

        yield Panel(
            content_renderable,
            title=f"Tool Output: {self.name or 'Unknown'} • [dim]{self.time}[/dim]",
            title_align="left",
            border_style="yellow",
            box=ROUNDED,
        )


# discriminated union
MessageUnion = Annotated[
    SystemMessage | UserMessage | AssistantMessage | ToolMessage,
    Field(discriminator="role_str"),
]

if __name__ == "__main__":
    # Test rich rendering
    from rich.console import Console

    console = Console()

    messages: list[Message] = [
        SystemMessage(content="You are a helpful assistant."),
        UserMessage(content="Hello, can you show me a picture of a cat?"),
        AssistantMessage(
            content=[
                TextContent(text="Sure! Here is a picture of a cat:"),
            ]
        ),
        ToolMessage(
            content='{"result": "Tool executed successfully."}',
            tool_call_id=str(uuid.uuid4()),
            name="fetch_cat_image",
        ),
    ]

    for msg in messages:
        console.print(msg)

```

## FILE: src/conduit/domain/message/role.py
```py
from enum import Enum


class Role(Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"
    TOOL = "tool"

```

## FILE: src/conduit/domain/request/generation_params.py
```py
"""
TO IMPLEMENT:
- params should cascade: Conduit defaults < Conversation overrides < Request final
"""

from __future__ import annotations
from pydantic import BaseModel, Field, model_validator, field_validator
from conduit.domain.request.output_type import OutputType
from typing import Any
import logging

logger = logging.getLogger(__name__)


class GenerationParams(BaseModel):
    """
    Standard tunable parameters for LLM inference.
    Shared by Conduit (defaults), Conversation (overrides), and Request (final payload).

    WARNING (cache determinism):
    GenerationParams participates in request cache keys. Any new field added here MUST be:
    - deterministic across runs when serialized (no timestamps, UUIDs, random/default_factory, env-derived values),
    - JSON-serializable via Pydantic (or explicitly normalized/excluded in GenerationRequest._normalize_params_for_cache),
    - and intentionally considered as affecting (or not affecting) cache identity.

    If a new field should NOT affect caching, exclude it explicitly in _normalize_params_for_cache.
    """

    output_type: OutputType = "text"
    model: str
    temperature: float | None = Field(default=None, ge=0.0, le=2.0)
    top_p: float | None = Field(default=None, ge=0.0, le=1.0)
    max_tokens: int | None = Field(default=None, ge=1)
    stop: list[str] | None = None
    stream: bool = False
    client_params: dict | None = None
    system: str | None = None
    tools: list[dict[str, Any]] | None = None

    # For structured responses; excluded from serialization, trust me
    response_model: type[BaseModel] | None = Field(default=None, exclude=True)
    # Generated field
    response_model_schema: dict[str, str] | None = None

    @field_validator("model")
    def _validate_model(cls, v: str) -> str:
        """
        Validate that the model is recognized.
        """
        from conduit.core.model.models.modelstore import ModelStore

        return ModelStore.validate_model(v)

    @model_validator(mode="after")
    def _populate_schema(self) -> GenerationParams:
        """
        If a Pydantic class is provided in response_model but no schema is set,
        generate the schema automatically. This ensures the request is cacheable/serializable.
        """
        if self.response_model and not self.response_model_schema:
            try:
                self.response_model_schema = self.response_model.model_json_schema()
            except AttributeError:
                logger.warning(f"Could not generate schema for {self.response_model}")
        return self

    @classmethod
    def defaults(cls, model_name: str) -> GenerationParams:
        """
        Return default generation parameters.
        """
        return cls(
            model=model_name,
            temperature=0.7,
            top_p=1.0,
            max_tokens=2048,
            stream=False,
            output_type="text",
        )

```

## FILE: src/conduit/domain/request/output_type.py
```py
from typing import Literal

OutputType = Literal["text", "image", "audio", "transcription", "structured_response"]

```

## FILE: src/conduit/domain/request/query_input.py
```py
from conduit.domain.message.message import Message, UserMessage
from collections.abc import Sequence


QueryInput = str | Sequence[Message] | Message


def constrain_query_input(query_input: QueryInput) -> Sequence[Message]:
    """
    Constrains the input to a list of Message objects.
    """
    if isinstance(query_input, str):
        return [UserMessage(content=query_input)]
    elif isinstance(query_input, Message):
        return [query_input]
    elif isinstance(query_input, list) and all(
        isinstance(msg, Message) for msg in query_input
    ):
        return query_input
    else:
        raise TypeError("Input must be a string, a Message, or a list of Messages.")

```

## FILE: src/conduit/domain/request/request.py
```py
"""
Model and client interaction:
- Model sends a Request, which is: conversation (list[MessageUnion]) + generation_params
- Request sends Response, which is: the request (list[MessageUnion]) + generation_params + the assistant message

Use MessageUnion (not Message) because it's a discriminated union.
"""

from __future__ import annotations
from pydantic import BaseModel
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.message.message import MessageUnion
from conduit.utils.progress.verbosity import Verbosity
from collections.abc import Sequence
import hashlib
import json
import logging
from typing import TYPE_CHECKING, override

if TYPE_CHECKING:
    from conduit.domain.message.message import Message
    from conduit.domain.conversation.conversation import Conversation
    from conduit.domain.request.query_input import QueryInput

logger = logging.getLogger(__name__)


# Hashing helpers
def _canonical_json_bytes(obj: object) -> bytes:
    """
    Canonical JSON encoding for hashing:
    - sorted keys for deterministic order
    - compact separators to avoid whitespace differences
    - UTF-8 bytes for stable hashing
    """
    return json.dumps(
        obj,
        sort_keys=True,
        separators=(",", ":"),
        ensure_ascii=False,
    ).encode("utf-8")


def _response_model_id(model_cls: type[BaseModel] | None) -> str | None:
    """
    Stable identifier for a Pydantic model class used for structured responses.
    Uses module + qualname to avoid collisions between same-named classes.
    """
    if model_cls is None:
        return None
    return f"{model_cls.__module__}.{model_cls.__qualname__}"


class GenerationRequest(BaseModel):
    """
    Inherits all params (temp, top_p) and adds required transport fields.
    """

    messages: Sequence[MessageUnion]
    params: GenerationParams
    options: ConduitOptions

    # Request param overrides
    use_cache: bool | None = True  # Technically: "if cache exists, use it"
    include_history: bool = True  # Whether to include conversation history
    verbosity_override: Verbosity | None = None

    def generate_cache_key(self) -> str:
        """
        Cache identity = canonical(params) + ordered(messages).

        - Params: all GenerationParams fields EXCEPT response_model and response_model_schema,
          plus response_model's module.qualname when present.
        - Messages: ordered list of {role, content} pairs only.
        - Canonical JSON (sorted keys) is hashed with SHA256.
        """
        key_payload = {
            "params": self._normalize_params_for_cache(),
            "messages": self._normalize_messages_for_cache(),
        }
        return hashlib.sha256(_canonical_json_bytes(key_payload)).hexdigest()

    def _normalize_params_for_cache(self) -> dict:
        # Dump JSON-safe primitives deterministically; exclude structured-response internals.
        params_dump = self.params.model_dump(
            mode="json",
            exclude_none=True,
            exclude={
                "response_model": True,
                "response_model_schema": True,
            },
        )

        # Add the semantic identity of the response model (if any).
        params_dump["response_model"] = _response_model_id(self.params.response_model)
        return params_dump

    def _normalize_messages_for_cache(self) -> list[dict]:
        # Ordered list; each message contributes only role + content.
        return [{"role": m.role_str, "content": m.content} for m in self.messages]

    @classmethod
    def from_conversation(
        cls,
        conversation: Conversation,
        params: GenerationParams,
        options: ConduitOptions,
    ) -> GenerationRequest:
        """
        Create a GenerationRequest from a Conversation, GenerationParams, and ConduitOptions.
        """
        return cls(
            messages=conversation.messages,
            params=params,
            options=options,
        )

    @classmethod
    def from_query_input(
        cls, query_input: QueryInput, params: GenerationParams, options: ConduitOptions
    ) -> GenerationRequest:
        """
        Create a GenerationRequest from a QueryInput object.
        """
        from conduit.domain.request.query_input import constrain_query_input

        messages = constrain_query_input(query_input)

        return cls(
            messages=messages,
            params=params,
            options=options,
        )

    @property
    def conversation(self) -> Conversation:
        """
        Convert the Request's messages into a Conversation object.
        """
        from conduit.domain.conversation.conversation import Conversation

        return Conversation(messages=self.messages)

    @override
    def __repr__(self) -> str:
        """
        Generate a detailed string representation of the Request instance.
        """
        return (
            f"Request(model={self.params.model!r}, messages={self.messages!r}, "
            f"temperature={self.params.temperature!r},"
            f"client_params={self.params.client_params!r}, response_model={self.params.response_model!r})"
        )

```

## FILE: src/conduit/domain/result/response.py
```py
"""
Model and client interaction:
- Model sends a Request, which is: conversation (list[Message]) + generation_params
- Request sends Response, which is: the request (list[Message]) + generation_params) + the assistant message + response metadata
"""

from __future__ import annotations
from conduit.domain.message.message import AssistantMessage, Message
from conduit.domain.message.role import Role
from conduit.domain.request.request import GenerationRequest
from conduit.domain.result.response_metadata import ResponseMetadata
from pydantic import BaseModel
from typing import TYPE_CHECKING, override
import logging

if TYPE_CHECKING:
    from conduit.domain.conversation.conversation import Conversation

logger = logging.getLogger(__name__)


class GenerationResponse(BaseModel):
    """
    Our class for a successful Result.
    Message + Request + ResponseMetadata
    """

    # Core attributes
    message: AssistantMessage
    request: GenerationRequest
    metadata: ResponseMetadata

    @property
    def prompt(self) -> str | None:
        """
        This is the last user message.
        """
        if not self.request.messages:
            raise ValueError("No messages in the request to extract prompt from.")
        if not self.request.messages[-1]:
            raise ValueError("The last message in the request is None.")
        if not self.request.messages[-1].role:
            raise ValueError("The last message in the request has no role defined.")
        if self.request.messages[-1].role != Role.USER:
            raise ValueError("The last message in the request is not from the user.")
        return str(self.request.messages[-1].content)

    @property
    def messages(self) -> list[Message]:
        return [*self.request.messages, self.message]

    @property
    def conversation(self) -> Conversation:
        from conduit.domain.conversation.conversation import Conversation

        return Conversation(messages=self.messages)

    @property
    def total_tokens(self) -> int:
        return self.metadata.input_tokens + self.metadata.output_tokens

    @property
    def content(self) -> str | object:
        """
        This is the last assistant message content.
        """
        if not self.message:
            raise ValueError("No message in the response to extract content from.")
        if not self.message.content:
            return ""
        return self.message.content

    @property
    def model(self) -> str:
        """
        This is the model used for the response.
        """
        return self.request.model

    @override
    def __str__(self) -> str:
        """
        We want to pass as string when possible.
        Allow json objects (dict) to be pretty printed.
        """
        content = self.content
        if content == None or content == "":
            return ""
        if content.__class__.__name__ == "PerplexityContent":
            output = content.text + "\n\n"
            for index, citation in enumerate(content.citations):
                output += f"{index + 1}. - [{citation.title}]({citation.url})\n"
        else:
            output = content
        return output

    # Interaction methods
    def play(self) -> None:
        """
        Play audio if available.
        """
        if self.request.params.output_type != "audio":
            raise ValueError("Response is not audio type.")
        from pydub import AudioSegment
        from pydub.playback import play
        import io
        import base64

        audio_data_b64 = self.message.content
        audio_data = base64.b64decode(audio_data_b64)
        audio_format = self.request.params.client_params.get("response_format", "mp3")
        audio_segment = AudioSegment.from_file(
            io.BytesIO(audio_data), format=audio_format
        )
        play(audio_segment)

    def display(self) -> None:
        """
        Display image if available, using viu in a subprocess.
        """
        if self.request.params.output_type != "image":
            raise ValueError("Response is not image type.")
        if self.message.images is None or len(self.message.images) == 0:
            raise ValueError("No images in the response to display.")
        import subprocess
        import base64
        import io
        from PIL import Image

        image_data_b64 = self.message.images[0].b64_json
        image_data = base64.b64decode(image_data_b64)
        image = Image.open(io.BytesIO(image_data))
        image.save("/tmp/temp_image.png")
        subprocess.run(["viu", "/tmp/temp_image.png"])

```

## FILE: src/conduit/domain/result/response_metadata.py
```py
from pydantic import BaseModel, Field
import time
from enum import Enum


class StopReason(str, Enum):
    STOP = "stop"  # Natural completion
    LENGTH = "length"  # Hit max_tokens (Context truncation risk!)
    TOOL_CALLS = "tool_calls"  # Model wants to act
    CONTENT_FILTER = "content_filter"  # Safety refusal
    ERROR = "error"  # API error or unknown


class ResponseMetadata(BaseModel):
    timestamp: int = Field(
        default_factory=lambda: int(time.time() * 1000),
        description="Timestamp in milliseconds since epoch",
    )
    duration: float = Field(
        ..., description="Time taken to process the request in milliseconds"
    )
    model_slug: str = Field(..., description="Identifier of the model used")
    input_tokens: int = Field(..., description="Number of input tokens processed")
    output_tokens: int = Field(..., description="Number of output tokens generated")
    stop_reason: str | StopReason = Field(
        ..., description="Reason for stopping the generation"
    )

```

## FILE: src/conduit/domain/result/result.py
```py
from conduit.domain.result.response import GenerationResponse
from conduit.core.parser.stream.protocol import SyncStream, AsyncStream

GenerationResult = GenerationResponse | SyncStream | AsyncStream

```

## FILE: src/conduit/examples/sample_models.py
```py
from pydantic import BaseModel

class MyTestPydanticContent(BaseModel):
    value: str
    number: int

class PydanticTestAnimal(BaseModel):
    name: str
    species: str
    age: int
    habitat: str

class PydanticTestFrog(BaseModel):
    species: str
    name: str
    legs: int
    color: str

```

## FILE: src/conduit/examples/sample_objects.py
```py
from conduit.domain.request.request import GenerationRequest
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.message.message import (
    UserMessage,
    AssistantMessage,
)
from conduit.domain.conversation.conversation import Conversation
from pathlib import Path

dir_path = Path(__file__).parent

# Messages
sample_message = UserMessage(content="Hello, world!")
sample_messages = [
    UserMessage(content="Hello, world!"),
    AssistantMessage(content="Hello! How can I assist you today?"),
    UserMessage(content="What is the weather like?"),
]
sample_conversation = Conversation(messages=sample_messages)

sample_audio_file = dir_path / "audio.mp3"
sample_image_file = dir_path / "image.png"

# Requests, results
sample_params = GenerationParams(model="gpt3")
sample_options = ConduitOptions(project_name="test")
sample_request = GenerationRequest(
    messages=sample_messages, params=sample_params, options=sample_options
)

# For Async testing
sample_async_prompt = """Name ten {{things}}."""
sample_input_variables_list = ["mammals", "birds", "villains"]
sample_prompt_strings = ["Name ten mammals.", "Name ten birds.", "Name ten villains."]

```

## FILE: src/conduit/examples/sample_requests.py
```py
from conduit.domain.request.request import GenerationRequest
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions
from conduit.domain.message.message import (
    UserMessage,
    AssistantMessage,
)
from conduit.domain.conversation.conversation import Conversation
from pathlib import Path

dir_path = Path(__file__).parent

sample_audio_file = dir_path / "audio.mp3"
sample_image_file = dir_path / "image.png"

# Messages
sample_message = UserMessage(content="Hello, world!")
sample_messages = [
    UserMessage(content="Hello, world!"),
    AssistantMessage(content="Hello! How can I assist you today?"),
    UserMessage(content="What is the weather like?"),
]

# Requests, results
sample_params = GenerationParams(model="gpt3")
sample_options = ConduitOptions(project_name="test")
sample_request = GenerationRequest(
    messages=sample_messages,
    params=sample_params,
    options=sample_options,
    use_cache=False,  # Since this is for testing purposes
)

# For Async testing
sample_async_prompt = """Name ten {{things}}."""
sample_input_variables_list = ["mammals", "birds", "villains"]
sample_prompt_strings = ["Name ten mammals.", "Name ten birds.", "Name ten villains."]

# Multimodal -- TBD -- need custom models + logic
# Audio request
# sample_audio_params = GenerationParams(model="audio-model", output_type="audio")
# sample_audio_request = GenerationRequest(
#     messages=sample_messages, params=sample_audio_params, options=sample_options
# )
# # Image request
# sample_image_params = GenerationParams(model="image-model", output_type="image")
# sample_image_request = GenerationRequest(
#     messages=sample_messages, params=sample_image_params, options=sample_options
# )
# # Structured response request
# sample_structured_params = GenerationParams(
#     model="structured-model", output_type="structured_response"
# )
# sample_structured_request = GenerationRequest(
#     messages=sample_messages, params=sample_structured_params, options=sample_options
# )

```

## FILE: src/conduit/extensions/compose/conduit_compose.py
```py
#!/usr/bin/env python3
"""
ConduitML to Mermaid DAG Converter

Converts ConduitML workflow files to Mermaid flowchart diagrams showing the DAG structure.

Usage:
    python conduitml_to_mermaid.py workflow.conduitml
    python conduitml_to_mermaid.py workflow.conduitml --output diagram.mmd
    python conduitml_to_mermaid.py workflow.conduitml --style detailed
"""

import yaml
import argparse
import sys
from pathlib import Path


class ConduitMLToMermaid:
    """Converts ConduitML workflows to Mermaid flowchart diagrams"""

    def __init__(self, style: str = "simple"):
        self.style = style  # "simple", "detailed", or "full"
        self.nodes: dict[str, dict] = {}
        self.edges: list[tuple] = []
        self.inputs: dict[str, dict] = {}
        self.outputs: dict[str, dict] = {}
        self.workflow_name = ""

    def parse_conduitml(self, conduitml_content: str):
        """Parse ConduitML content and extract DAG structure"""
        try:
            data = yaml.safe_load(conduitml_content)
            workflow = data.get("workflow", {})

            # Extract metadata
            self.workflow_name = workflow.get("name", "Unnamed Workflow")

            # Extract inputs
            self.inputs = workflow.get("inputs", {})

            # Extract outputs
            self.outputs = workflow.get("outputs", {})

            # Extract steps and build DAG
            steps = workflow.get("steps", {})
            self._parse_steps(steps)

        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in ConduitML file: {e}")
        except Exception as e:
            raise ValueError(f"Error parsing ConduitML: {e}")

    def _parse_steps(self, steps: dict[str, dict]):
        """Parse workflow steps and extract dependencies"""
        for step_name, step_config in steps.items():
            # Clean step name (remove 'step ' prefix if present)
            clean_name = step_name.replace("step ", "").strip()

            # Store step information
            self.nodes[clean_name] = {
                "name": clean_name,
                "model": step_config.get("model", "unknown"),
                "description": step_config.get("description", ""),
                "has_condition": "condition" in step_config,
                "has_parser": "parser" in step_config,
                "parser_type": (
                    step_config.get("parser", {}).get("type", "string")
                    if "parser" in step_config
                    else "string"
                ),
            }

            # Extract dependencies and create edges
            depends_on = step_config.get("depends_on", [])
            for dependency in depends_on:
                self.edges.append((dependency, clean_name))

    def _sanitize_node_id(self, name: str) -> str:
        """Convert node name to valid Mermaid ID"""
        # Replace spaces and special characters with underscores
        return name.replace(" ", "_").replace("-", "_").replace(".", "_")

    def _get_node_style(self, node_info: dict) -> str:
        """Determine node styling based on step properties"""
        if self.style == "simple":
            return ""

        styles = []

        # Color based on model type
        model = node_info["model"].lower()
        if "gpt" in model or "openai" in model:
            styles.append("fill:#e1f5fe")  # Light blue for OpenAI
        elif "claude" in model or "anthropic" in model:
            styles.append("fill:#f3e5f5")  # Light purple for Anthropic
        elif "gemini" in model or "google" in model:
            styles.append("fill:#e8f5e8")  # Light green for Google
        else:
            styles.append("fill:#fff3e0")  # Light orange for others

        # Border style for special properties
        if node_info["has_condition"]:
            styles.append(
                "stroke:#ff9800,stroke-width:3px,stroke-dasharray: 5 5"
            )  # Dashed for conditional
        elif node_info["has_parser"]:
            styles.append(
                "stroke:#4caf50,stroke-width:2px"
            )  # Green for structured output

        return (
            f"style {self._sanitize_node_id(node_info['name'])} {','.join(styles)}"
            if styles
            else ""
        )

    def _format_node_label(self, node_info: dict) -> str:
        """Format node label based on style setting"""
        name = node_info["name"]

        if self.style == "simple":
            return name
        elif self.style == "detailed":
            model = node_info["model"]
            # Shorten common model names
            model_short = model.replace("gpt-4o-mini", "GPT-4o-mini").replace(
                "claude-3-5-haiku", "Claude-Haiku"
            )
            return f"{name}<br/><small>{model_short}</small>"
        else:  # full
            model = node_info["model"]
            description = (
                node_info["description"][:50] + "..."
                if len(node_info["description"]) > 50
                else node_info["description"]
            )
            parser_info = (
                f"<br/><small>Parser: {node_info['parser_type']}</small>"
                if node_info["has_parser"]
                else ""
            )
            condition_info = (
                "<br/><small>⚠️ Conditional</small>"
                if node_info["has_condition"]
                else ""
            )
            return f"{name}<br/><small>{model}</small><br/><small>{description}</small>{parser_info}{condition_info}"

    def _add_input_output_nodes(self) -> str:
        """Add input and output nodes to the diagram"""
        lines = []

        # Add input nodes
        if self.inputs:
            for input_name in self.inputs.keys():
                input_id = f"input_{self._sanitize_node_id(input_name)}"
                lines.append(f"    {input_id}[📥 {input_name}]")
                if self.style != "simple":
                    lines.append(f"    style {input_id} fill:#e3f2fd,stroke:#1976d2")

        # Add output nodes
        if self.outputs:
            for output_name in self.outputs.keys():
                output_id = f"output_{self._sanitize_node_id(output_name)}"
                lines.append(f"    {output_id}[📤 {output_name}]")
                if self.style != "simple":
                    lines.append(f"    style {output_id} fill:#f1f8e9,stroke:#689f38")

        return "\n".join(lines)

    def _add_input_output_edges(self) -> str:
        """Add edges connecting inputs/outputs to workflow steps"""
        lines = []

        # Connect inputs to steps that use them (simplified - connects to steps with no dependencies)
        if self.inputs:
            root_steps = [
                name
                for name, info in self.nodes.items()
                if not any(edge[1] == name for edge in self.edges)
            ]

            for input_name in self.inputs.keys():
                input_id = f"input_{self._sanitize_node_id(input_name)}"
                for step_name in root_steps:
                    step_id = self._sanitize_node_id(step_name)
                    lines.append(f"    {input_id} --> {step_id}")

        # Connect final steps to outputs (simplified - connects from steps with no dependents)
        if self.outputs:
            final_steps = [
                name
                for name, info in self.nodes.items()
                if not any(edge[0] == name for edge in self.edges)
            ]

            for output_name in self.outputs.keys():
                output_id = f"output_{self._sanitize_node_id(output_name)}"
                for step_name in final_steps:
                    step_id = self._sanitize_node_id(step_name)
                    lines.append(f"    {step_id} --> {output_id}")

        return "\n".join(lines)

    def generate_mermaid(self) -> str:
        """Generate Mermaid flowchart from parsed ConduitML"""
        lines = []

        # Header
        lines.append("---")
        lines.append(f"title: {self.workflow_name}")
        lines.append("---")
        lines.append("flowchart TD")
        lines.append("")

        # Add input/output nodes if they exist
        io_nodes = self._add_input_output_nodes()
        if io_nodes:
            lines.append("    %% Input/Output Nodes")
            lines.append(io_nodes)
            lines.append("")

        # Add step nodes
        lines.append("    %% Workflow Steps")
        for node_name, node_info in self.nodes.items():
            node_id = self._sanitize_node_id(node_name)
            label = self._format_node_label(node_info)

            # Choose node shape based on properties
            if node_info["has_condition"]:
                lines.append(f"    {node_id}{{{label}}}")  # Diamond for conditional
            elif node_info["has_parser"]:
                lines.append(f"    {node_id}[{label}]")  # Rectangle for structured
            else:
                lines.append(f"    {node_id}({label})")  # Rounded for simple

        lines.append("")

        # Add step dependencies
        if self.edges:
            lines.append("    %% Dependencies")
            for source, target in self.edges:
                source_id = self._sanitize_node_id(source)
                target_id = self._sanitize_node_id(target)
                lines.append(f"    {source_id} --> {target_id}")
            lines.append("")

        # Add input/output connections
        io_edges = self._add_input_output_edges()
        if io_edges:
            lines.append("    %% Input/Output Connections")
            lines.append(io_edges)
            lines.append("")

        # Add styling
        if self.style != "simple":
            lines.append("    %% Styling")
            for node_name, node_info in self.nodes.items():
                style = self._get_node_style(node_info)
                if style:
                    lines.append(f"    {style}")

        return "\n".join(lines)

    def validate_dag(self) -> list[str]:
        """Validate that the workflow forms a valid DAG"""
        errors = []

        # Check for self-references
        for source, target in self.edges:
            if source == target:
                errors.append(f"Self-reference detected: {source} depends on itself")

        # Check for cycles using DFS
        visited = set()
        rec_stack = set()

        def has_cycle(node: str) -> bool:
            visited.add(node)
            rec_stack.add(node)

            # Get all nodes this one points to
            dependents = [target for source, target in self.edges if source == node]

            for dependent in dependents:
                if dependent not in visited:
                    if has_cycle(dependent):
                        return True
                elif dependent in rec_stack:
                    errors.append(f"Cycle detected involving: {node} -> {dependent}")
                    return True

            rec_stack.remove(node)
            return False

        # Check all nodes for cycles
        for node in self.nodes.keys():
            if node not in visited:
                has_cycle(node)

        # Check for orphaned dependencies
        all_referenced = set()
        for source, target in self.edges:
            all_referenced.add(source)
            all_referenced.add(target)

        for referenced in all_referenced:
            if referenced not in self.nodes:
                errors.append(f"Step '{referenced}' is referenced but not defined")

        return errors


def main():
    parser = argparse.ArgumentParser(
        description="Convert ConduitML workflow files to Mermaid DAG diagrams",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python conduitml_to_mermaid.py workflow.conduitnml
  python conduitml_to_mermaid.py workflow.conduitml --output diagram.mmd
  python conduitml.py workflow.conduitml --style detailed
  python conduitml_to_mermaid.py workflow.conduitml --style full --validate
        """,
    )

    parser.add_argument("condutml_file", help="Path to ConduitML workflow file")

    parser.add_argument(
        "--output", "-o", help="Output file for Mermaid diagram (default: stdout)"
    )

    parser.add_argument(
        "--style",
        "-s",
        choices=["simple", "detailed", "full"],
        default="detailed",
        help="Diagram style: simple (names only), detailed (+ models), full (+ descriptions)",
    )

    parser.add_argument(
        "--validate",
        "-v",
        action="store_true",
        help="Validate DAG structure and report errors",
    )

    args = parser.parse_args()

    # Read ConduitML file
    try:
        conduitml_path = Path(args.conduitml_file)
        if not conduitml_path.exists():
            print(f"Error: File '{args.conduitml_file}' not found", file=sys.stderr)
            sys.exit(1)

        with open(conduitml_path, "r", encoding="utf-8") as f:
            conduitml_content = f.read()

    except Exception as e:
        print(f"Error reading file: {e}", file=sys.stderr)
        sys.exit(1)

    # Convert to Mermaid
    try:
        converter = ConduitMLToMermaid(style=args.style)
        converter.parse_conduitml(conduitml_content)

        # Validate if requested
        if args.validate:
            errors = converter.validate_dag()
            if errors:
                print("DAG Validation Errors:", file=sys.stderr)
                for error in errors:
                    print(f"  - {error}", file=sys.stderr)
                sys.exit(1)
            else:
                print("✅ DAG validation passed", file=sys.stderr)

        # Generate Mermaid diagram
        mermaid_output = converter.generate_mermaid()

        # Output to file or stdout
        if args.output:
            output_path = Path(args.output)
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(mermaid_output)
            print(f"Mermaid diagram written to: {output_path}", file=sys.stderr)
        else:
            print(mermaid_output)

    except Exception as e:
        print(f"Error converting ConduitML: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/extensions/embeddings/generate_embeddings.py
```py
"""
Generate raw embeddings for a list of texts using a specified embedding model.
"""

from headwater_client.client.headwater_client import HeadwaterClient
from headwater_api.classes import (
    ChromaBatch,
    EmbeddingsRequest,
    EmbeddingsResponse,
    load_embedding_models,
)


def generate_embeddings(
    ids: list[str],
    documents: list[str],
    model: str = "sentence-transformers/all-mpnet-base-v2",
) -> list[list[float]]:
    """
    Get embeddings for a list of texts using the specified model.

    Args:
        texts (list of str): The texts to embed.
        model (str): The embedding model to use.

    Returns:
        list of list of float: The embeddings for the input texts.
    """
    # Validate inputs
    if validate_model(model) is False:
        raise ValueError(f"Model '{model}' is not a valid embedding model.")
    assert len(ids) == len(documents), "Length of ids and documents must match."
    # Prepare request
    chroma_batch = ChromaBatch(
        ids=ids,
        documents=documents,
    )
    request = EmbeddingsRequest(
        model=model,
        batch=chroma_batch,
    )
    # Generate embeddings
    client = HeadwaterClient()
    response: EmbeddingsResponse = client.embeddings.generate_embeddings(request)
    assert len(response.embeddings) == len(documents), (
        "Number of embeddings doesn't match number of documents."
    )
    return response.embeddings


def quick_embedding(query_string: str) -> list[list[float]]:
    """
    Get embedding for a single query (i.e. for querying chroma database).
    """
    embeddings = generate_embeddings(
        ids=["quick_embedding"],
        documents=[query_string],
    )
    return embeddings


def validate_model(model_name: str) -> bool:
    embedding_models = load_embedding_models()
    return model_name in embedding_models

```

## FILE: src/conduit/extensions/llm_decorator/decorator.py
```py
from conduit.core.model.model_sync import ModelSync
from conduit.core.prompt.prompt import Prompt
from conduit.core.conduit.sync_conduit import SyncConduit
from collections.abc import Callable
import inspect
import re
import logging

logger = logging.getLogger(__name__)

# Constants
param_string_pattern = re.compile("{{(.+)}}")


def _validate_template_params(func: Callable) -> None:
    """
    Validate that function parameters match template variables in docstring.

    Raises:
        ValueError: If parameters don't match template variables
    """
    # Get function signature parameters
    sig = inspect.signature(func)
    func_params = set()

    for param_name, param in sig.parameters.items():
        # Skip *args and **kwargs
        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
            continue
        func_params.add(param_name)

    # Extract template variables from docstring
    docstring = func.__doc__
    if not docstring:
        raise ValueError(
            f"Function '{func.__name__}' must have a docstring containing the prompt template"
        )

    # Find all {{variable}} patterns
    template_vars = set(re.findall(r"\{\{(\w+)\}\}", docstring))

    # Compare the sets
    missing_in_template = func_params - template_vars
    missing_in_signature = template_vars - func_params

    errors = []

    if missing_in_template:
        errors.append(
            f"Parameters in function signature but not in template: {missing_in_template}"
        )

    if missing_in_signature:
        errors.append(
            f"Variables in template but not in function signature: {missing_in_signature}"
        )

    if errors:
        raise ValueError(
            f"Template validation failed for function '{func.__name__}':\n"
            + "\n".join(f"  - {error}" for error in errors)
        )


def _wrap_params(prompt: str) -> str:
    """
    Generally speaking, wrapping context in xml tags is very helpful for LLMs.
    All prompts written as docstrings for an llm function get wrapped in XML.

    Example:

    {{param_name}} becomes:

    <param_name>
    {{param_name}}
    </param_name>
    """
    return (
        re.sub(
            param_string_pattern,
            lambda m: f"<{m.group(1)}>\n{{{{{m.group(1)}}}}}\n</{m.group(1)}> ",
            prompt,
        )
        + "\n"
    )


def llm(func: Callable = None, *, model="haiku") -> Callable:  # Note the *
    """
    Decorator to create a prompt function that can be used with an LLM.
    Adapts basic Conduit syntax to the decorator pattern for easy composition.

    How to compose:

    @llm
    def my_prompt_function(input: str):
        \"""
        Your prompt template with {{input}}.
        \"""
    """

    def decorator(f: Callable) -> Callable:
        _validate_template_params(f)

        def wrapper(*args, **kwargs):  # Accept both positional and keyword
            # Get function signature to bind args properly

            sig = inspect.signature(f)
            bound_args = sig.bind(*args, **kwargs)
            bound_args.apply_defaults()

            model_object = Model(model)
            # Wrap any parameters in XML tags
            prompt_string = _wrap_params(f.__doc__)
            # Docstrings are tabbed / spaced, so we need to remove spaces at beginning of lines
            prompt_string = "\n".join(
                line.lstrip() for line in prompt_string.splitlines()
            )
            prompt = Prompt(prompt_string)
            conduit = SyncConduit(prompt=prompt, model=model_object)
            response = conduit.run(input_variables=dict(bound_args.arguments))
            return response.content  # Note: .content to get just the text

        return wrapper

    if func is None:
        return decorator
    else:
        return decorator(func)

```

## FILE: src/conduit/extensions/summarize/generate.py
```py
from conduit.batch import (
    AsyncConduit,
    ModelAsync,
    Prompt,
    Response,
    Verbosity,
    ConduitCache,
)
from rich.console import Console
from pathlib import Path
from asyncio import Semaphore

CONSOLE = Console()
CACHE = ConduitCache("conduit")
PREFERRED_MODEL = "gemini3"
ModelAsync.console = CONSOLE
ModelAsync.conduit_cache = CACHE
ESSAY_DIR = Path(__file__).parent / "essays"
PROMPT_STR = """
You are a talented writer. You will generate well structure and highly detailed essays of a given length.
The purpose of these essays is to train and evaluate a text summarization model.

Please write an essay of the following length:
<length>
{{length}}
</length>
""".strip()

# generate meaningful lengths for assessing summary quality by LLMs (the hundreds, the thousands, 8192 is at the high end, we want to go past that as well, all the way up to 32k
token_lengths = [
    100,
    250,
    500,
    750,
    1000,
    1500,
    2000,
    3000,
    4000,
    5000,
    6000,
    7000,
    8000,
    10000,
    15000,
]
# Convert to word lengths (assuming 1 token ~ 0.75 words)
lengths = [int(tl / 0.75) for tl in token_lengths]


def generate_essays() -> list[str]:
    prompt = Prompt(PROMPT_STR)
    model = ModelAsync(PREFERRED_MODEL)
    conduit = AsyncConduit(model=model, prompt=prompt)
    input_variables_list = [{"length": str(length)} for length in lengths]
    semaphore = Semaphore(5)
    responses = conduit.run(
        input_variables_list=input_variables_list,
        verbose=Verbosity.PROGRESS,
        semaphore=semaphore,
    )
    assert all(isinstance(r, Response) for r in responses)
    essays = [str(r.content) for r in responses]
    return essays


if __name__ == "__main__":
    model = ModelAsync("gpt")
    essays = generate_essays()
    ESSAY_DIR.mkdir(exist_ok=True)
    for length, essay in zip(lengths, essays):
        essay_path = ESSAY_DIR / f"essay_{length}_words.txt"
        with open(essay_path, "w", encoding="utf-8") as f:
            f.write(essay)
        token_length = model.tokenize(essay)
        CONSOLE.log(f"Saved essay of {token_length} tokens to {essay_path}")

```

## FILE: src/conduit/extensions/summarize/strategies/atomic_proposition.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class AtomicPropositionStrategy(SummarizationStrategy):
    """
    One-shot strategy.

    Implements Propositional Decomposition for RAG ingestion.

    Workflow:
    1. Prompt the LLM to decompose the text into independent, self-contained
       atomic statements.
    2. Enforce De-referencing: Replace pronouns ('he', 'it', 'the company') with
       their full named entities ('Guido van Rossum', 'Python 3.11', 'Anthropic').
    3. Return a list of strings suitable for individual vector embedding.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/chain_of_density.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class RecursiveChainOfDensityStrategy(SummarizationStrategy):
    """
    Single-shot strategy.

    Implements the Iterative Densification workflow.

    Workflow:
    1. Generate an initial verbose summary (Draft 1).
    2. Prompt the model to identify 5-10 specific entities (people, dates, numbers)
       present in the source text but missing from Draft 1.
    3. Prompt the model to fuse these missing entities into Draft 1 without
       increasing the total word count.
    4. Repeat for N recursions (default 3) to maximize information density per token.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/cluster_select.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class ClusterSelectStrategy(SummarizationStrategy):
    """
    Massive multi-shot strategy.

    Implements Embedding-based Clustering for 'Snapshot' generation.

    Workflow:
    1. Chunk the entire document.
    2. Generate vector embeddings for all chunks (using a fast local model).
    3. Run K-Means clustering to identify K distinct semantic topics (e.g., K=10).
    4. Select the 'Centroid' chunk from each cluster (the most representative text).
    5. Feed the list of Centroids to the LLM to generate a 'Table of Contents'
       or high-level overview.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/hierarchical_tree.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class HierarchicalTreeStrategy(SummarizationStrategy):
    """
    Massive multi-shot strategy.

    Implements a RAPTOR-lite recursive summarization tree.

    Workflow:
    1. Bottom-Up: Chunk the text.
    2. Level 1: Summarize every chunk (optionally using Chain of Density).
    3. Grouping: Concatenate Level 1 summaries into groups that fit context.
    4. Level 2: Summarize the groups.
    5. Repeat until the entire corpus is compressed into a single Root summary.
    6. Return the Root summary (or the traversed tree for deep context).
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/map_dedupe_reduce.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class MapDedupeReduceStrategy(SummarizationStrategy):
    """
    Multi-shot strategy.

    Implements a parallel 'Map-Reduce' workflow optimized for extraction.

    Workflow:
    1. Chunk the text.
    2. Map (Parallel): Run a distinct extraction prompt (e.g., "list all tasks")
       on every chunk simultaneously using a lightweight model.
    3. Reduce: Collect all extraction lists.
    4. Dedupe: Run a final LLM pass to merge duplicates and normalize format.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/rolling_refine.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class RollingRefineStrategy(SummarizationStrategy):
    """
    Multi-shot strategy.

    Implements a sequential 'Refine' workflow for narrative continuity.

    Workflow:
    1. Chunk the text linearly.
    2. Summarize Chunk 1.
    3. Loop through remaining chunks:
       Input = (Current Summary) + (New Chunk Text)
       Prompt = "Update the Current Summary with new information from the New Chunk.
                 Do not delete existing relevant info. Do not repeat facts."
    4. Return the final evolved state of the summary.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategies/schema_extration.py
```py
from conduit.extensions.summarize.strategy import SummarizationStrategy


class SchemaExtractionStrategy(SummarizationStrategy):
    """
    One-shot strategy.

    Implements structured data extraction using strictly enforced schemas.

    Workflow:
    1. Define a Pydantic model or JSON schema representing the desired output
       (e.g., list[ActionItem], list[Argument], list[Decision]).
    2. Force the LLM (via grammar constraints or function calling) to output
       only valid JSON adhering to that schema.
    3. Parse and validate the output.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...

```

## FILE: src/conduit/extensions/summarize/strategy.py
```py
from abc import ABC, abstractmethod
from typing import Any


class SummarizationStrategy(ABC):
    """
    Abstract base class for all summarization logic in Siphon.
    """

    @abstractmethod
    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str:
        """
        Execute the summarization workflow.
        """
        ...


class PolishingStrategy(ABC):
    """
    Abstract base class for formatting/styling the final output.
    """

    @abstractmethod
    def polish(self, raw_summary: str, metadata: dict[str, Any]) -> str:
        """
        Apply specific formatting templates (Markdown, JSON, etc.) to the summary.
        """
        ...


# ==============================================================================
# Tier 1: The "Single-Shot" Zone (< Context Window)
# Goal: Maximum Semantic Density
# ==============================================================================


class RecursiveChainOfDensityStrategy(SummarizationStrategy):
    """
    Implements the Iterative Densification workflow.

    Workflow:
    1. Generate an initial verbose summary (Draft 1).
    2. Prompt the model to identify 5-10 specific entities (people, dates, numbers)
       present in the source text but missing from Draft 1.
    3. Prompt the model to fuse these missing entities into Draft 1 without
       increasing the total word count.
    4. Repeat for N recursions (default 3) to maximize information density per token.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


class SchemaExtractionStrategy(SummarizationStrategy):
    """
    Implements structured data extraction using strictly enforced schemas.

    Workflow:
    1. Define a Pydantic model or JSON schema representing the desired output
       (e.g., list[ActionItem], list[Argument], list[Decision]).
    2. Force the LLM (via grammar constraints or function calling) to output
       only valid JSON adhering to that schema.
    3. Parse and validate the output.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


class AtomicPropositionStrategy(SummarizationStrategy):
    """
    Implements Propositional Decomposition for RAG ingestion.

    Workflow:
    1. Prompt the LLM to decompose the text into independent, self-contained
       atomic statements.
    2. Enforce De-referencing: Replace pronouns ('he', 'it', 'the company') with
       their full named entities ('Guido van Rossum', 'Python 3.11', 'Anthropic').
    3. Return a list of strings suitable for individual vector embedding.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


# ==============================================================================
# Tier 2: The "Overflow" Zone (> Context Window, Linear)
# Goal: Coherence & Narrative Preservation
# ==============================================================================


class RollingRefineStrategy(SummarizationStrategy):
    """
    Implements a sequential 'Refine' workflow for narrative continuity.

    Workflow:
    1. Chunk the text linearly.
    2. Summarize Chunk 1.
    3. Loop through remaining chunks:
       Input = (Current Summary) + (New Chunk Text)
       Prompt = "Update the Current Summary with new information from the New Chunk.
                 Do not delete existing relevant info. Do not repeat facts."
    4. Return the final evolved state of the summary.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


class MapDedupeReduceStrategy(SummarizationStrategy):
    """
    Implements a parallel 'Map-Reduce' workflow optimized for extraction.

    Workflow:
    1. Chunk the text.
    2. Map (Parallel): Run a distinct extraction prompt (e.g., "list all tasks")
       on every chunk simultaneously using a lightweight model.
    3. Reduce: Collect all extraction lists.
    4. Dedupe: Run a final LLM pass to merge duplicates and normalize format.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


# ==============================================================================
# Tier 3: The "Massive" Zone (Books, Multi-hour Video)
# Goal: Navigability & Gist
# ==============================================================================


class ClusterSelectStrategy(SummarizationStrategy):
    """
    Implements Embedding-based Clustering for 'Snapshot' generation.

    Workflow:
    1. Chunk the entire document.
    2. Generate vector embeddings for all chunks (using a fast local model).
    3. Run K-Means clustering to identify K distinct semantic topics (e.g., K=10).
    4. Select the 'Centroid' chunk from each cluster (the most representative text).
    5. Feed the list of Centroids to the LLM to generate a 'Table of Contents'
       or high-level overview.
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


class HierarchicalTreeStrategy(SummarizationStrategy):
    """
    Implements a RAPTOR-lite recursive summarization tree.

    Workflow:
    1. Bottom-Up: Chunk the text.
    2. Level 1: Summarize every chunk (optionally using Chain of Density).
    3. Grouping: Concatenate Level 1 summaries into groups that fit context.
    4. Level 2: Summarize the groups.
    5. Repeat until the entire corpus is compressed into a single Root summary.
    6. Return the Root summary (or the traversed tree for deep context).
    """

    def summarize(self, text: str, context_limit: int = 8192, **kwargs) -> str: ...


# ==============================================================================
# The Finishing Layers
# ==============================================================================


class YouTubeCompanionStrategy(PolishingStrategy):
    """
    Formats the output for video consumption.

    Structure:
    - Hook / Teaser Paragraph
    - Timestamped Chapter Markers (derived from source metadata if available)
    - Key Takeaways (Bulleted)
    - Further Reading / Referenced URLs
    """

    def polish(self, raw_summary: str, metadata: dict[str, Any]) -> str: ...


class ObsidianVaultStrategy(PolishingStrategy):
    """
    Formats the output for Personal Knowledge Management (PKM).

    Structure:
    - Frontmatter (YAML tags, source link, date)
    - [[Wikilinks]] for key entities found in the text.
    - Markdown Headers (#, ##) for hierarchy.
    - 'Related Concepts' section at the bottom.
    """

    def polish(self, raw_summary: str, metadata: dict[str, Any]) -> str: ...


class ExecutiveBriefStrategy(PolishingStrategy):
    """
    Formats the output for decision making (BLUF).

    Structure:
    - BLUF (Bottom Line Up Front): 1 sentence thesis.
    - Critical Implications: 3 bullet points.
    - Decision Vectors / Action Items.
    """

    def polish(self, raw_summary: str, metadata: dict[str, Any]) -> str: ...

```

## FILE: src/conduit/middleware/context_manager.py
```py
from __future__ import annotations
from contextlib import asynccontextmanager
import time
import logging
from conduit.domain.result.response import GenerationResponse
from conduit.domain.request.request import GenerationRequest
from conduit.utils.progress.verbosity import Verbosity
from conduit.utils.progress.utils import extract_query_preview

logger = logging.getLogger(__name__)


# _get_progress_handler helper stays the same...
def _get_progress_handler(request: GenerationRequest):
    if request.options.console is not None:
        from conduit.utils.progress.rich_handler import RichProgressHandler

        return RichProgressHandler(request.options.console)
    else:
        from conduit.utils.progress.plain_handler import PlainProgressHandler

        return PlainProgressHandler()


@asynccontextmanager
async def middleware_context_manager(request: GenerationRequest):
    """
    The Central Conductor for Request execution.
    Fully async.
    """
    if request.verbosity_override is not None:
        verbosity = request.verbosity_override
    else:
        verbosity = request.options.verbosity

    # --- 1. SETUP ---
    handler = _get_progress_handler(request)
    model_name = request.params.model
    preview = extract_query_preview(request.messages)

    ctx = {"cache_hit": False, "result": None}
    start_time = time.time()

    # --- 2. UI START ---
    if verbosity >= Verbosity.PROGRESS:
        handler.show_spinner(
            model_name=model_name,
            query_preview=preview,
            verbosity=verbosity,
        )

    # Debug print block stays same...

    # --- 3. CACHE READ (Async) ---
    if request.options.cache is not None and request.options.use_cache:
        # AWAIT HERE
        cached_result = await request.options.cache.get(request)
        if isinstance(cached_result, GenerationResponse):
            ctx["cache_hit"] = True
            ctx["result"] = cached_result
            logger.info("Cache hit.")

    # --- 4. EXECUTION (YIELD) ---
    yield ctx

    # --- 5. UI STOP (SUCCESS) ---
    duration = time.time() - start_time
    result = ctx.get("result")

    if verbosity >= Verbosity.PROGRESS:
        if ctx["cache_hit"]:
            handler.show_cached(
                model_name=model_name,
                query_preview=preview,
                duration=duration,
                verbosity=verbosity,
            )
        else:
            handler.show_complete(
                model_name=model_name,
                query_preview=preview,
                duration=duration,
                verbosity=verbosity,
                response_obj=result if isinstance(result, GenerationResponse) else None,
            )

    # --- 6. TEARDOWN (IO) ---
    if isinstance(result, GenerationResponse):
        # Cache Write (Async)
        if not ctx["cache_hit"] and request.options.cache is not None:
            logger.debug("Persisting result to cache.")
            # AWAIT HERE
            await request.options.cache.set(request, result)

        # Telemetry (Async Flush)
        if not ctx["cache_hit"] and result.metadata.output_tokens > 0:
            from conduit.config import settings
            from conduit.storage.odometer.token_event import TokenEvent

            telemetry = settings.odometer_registry()

            # 1. Record to memory (Sync)
            event = TokenEvent(
                model=model_name,
                input_tokens=result.metadata.input_tokens,
                output_tokens=result.metadata.output_tokens,
            )
            telemetry.emit_token_event(event)

            # 2. Trigger Async Flush
            # We trigger the flush here to ensure near-realtime persistence
            # without blocking the user response (since it's async).
            # We also check for recovery (one-time check usually handles itself,
            # but we can optionally call recover here if we want to be aggressive).
            await telemetry.flush()

            # Optional: If this is the very first run, try to recover old files
            # Ideally this is done at app startup, but doing it here is safe/lazy.
            await telemetry.recover()

```

## FILE: src/conduit/middleware/middleware.py
```py
from __future__ import annotations
from conduit.middleware.context_manager import middleware_context_manager
from conduit.domain.request.request import GenerationRequest
import functools
import logging
from typing import TYPE_CHECKING
from collections.abc import Callable, Awaitable

if TYPE_CHECKING:
    from conduit.domain.result.result import GenerationResult

logger = logging.getLogger(__name__)


def get_request(*args, **kwargs) -> GenerationRequest:
    """
    Common logic to extract request from args and kwargs.
    The wrapped function is expected to have 'request' as the 2nd arg or a kwarg.
    """
    if len(args) > 1 and isinstance(args[1], GenerationRequest):
        request = args[1]
    elif "request" in kwargs and isinstance(kwargs["request"], GenerationRequest):
        request = kwargs["request"]
    else:
        raise ValueError(
            "The decorated function must have a GenerationRequest as its second positional argument or as a 'request' keyword argument."
        )
    return request


def middleware(
    func: Callable[..., Awaitable[GenerationResult]],
) -> Callable[..., Awaitable[GenerationResult]]:
    """
    Decorator that wraps Model execution with the middleware context manager.
    Handles caching, UI spinners, logging, and telemetry via `middleware_context_manager`.
    """

    @functools.wraps(func)
    async def async_wrapper(*args: object, **kwargs: object) -> GenerationResult:
        request = get_request(*args, **kwargs)

        # Use async with for the context manager (allows awaitable cache ops)
        async with middleware_context_manager(request) as ctx:
            # Check for cache hit (passed back from context manager)
            if ctx["cache_hit"] is True:
                result = ctx["result"]
            # If no cache hit, execute the function
            else:
                result = await func(*args, **kwargs)
                # Pass result back to context manager for post-processing
                ctx["result"] = result

        assert "result" in ctx, (
            "Something went wrong in the middleware context manager; no result found."
        )

        return result

    return async_wrapper

```

## FILE: src/conduit/remote.py
```py
"""
Convenience imports for Headwater server:

model = RemoteModel(preferred_model)
response = model.query(prompt_str)
"""

# Orchestration classes
from conduit.core.model.model_remote import (
    RemoteModelSync,
    RemoteModelAsync,
    remote_model_sync,
    remote_model_async,
)
from conduit.core.prompt.prompt import Prompt
from conduit.core.conduit.conduit_sync import ConduitSync
from conduit.core.conduit.conduit_async import ConduitAsync

# Primitives: dataclasses / enums
from conduit.domain.result.response import GenerationResponse
from conduit.domain.request.request import GenerationRequest
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.message.message import Message

# Configs
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

__all__ = [
    "ConduitAsync",
    "ConduitOptions",
    "ConduitSync",
    "GenerationParams",
    "GenerationRequest",
    "GenerationResponse",
    "Message",
    "Prompt",
    "RemoteModelAsync",
    "RemoteModelSync",
    "Verbosity",
    "remote_model_async",
    "remote_model_sync",
]

```

## FILE: src/conduit/storage/cache/postgres_cache_async.py
```py
from __future__ import annotations
import time
import json
import logging
import asyncio
from typing import TYPE_CHECKING
from conduit.domain.result.response import GenerationResponse
from conduit.domain.request.request import GenerationRequest

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from asyncpg import Pool


class AsyncPostgresCache:
    """
    Async Postgres-backed implementation of ConduitCache using asyncpg.
    Handles its own pool lifecycle to support restarting event loops (ModelSync).
    """

    def __init__(self, project_name: str, db_name: str = "conduit"):
        self.project_name = project_name
        self.db_name = db_name
        self._hits = 0
        self._misses = 0
        self._start_time = time.time()

        # Lazy pool management
        self._pool: Pool | None = None
        self._pool_loop: asyncio.AbstractEventLoop | None = None

    async def _get_pool(self) -> Pool:
        """
        Get or create a connection pool attached to the current event loop.
        """
        current_loop = asyncio.get_running_loop()

        # If we have a pool and the loop hasn't changed, reuse it
        if (
            self._pool
            and self._pool_loop is current_loop
            and not current_loop.is_closed()
        ):
            return self._pool

        # Otherwise (re)initialize
        logger.debug(f"Initializing asyncpg pool for cache '{self.project_name}'")
        from dbclients.clients.postgres import get_postgres_client

        # get_postgres_client("async", ...) returns a coroutine factory for the pool
        pool_factory = await get_postgres_client(
            client_type="async", dbname=self.db_name
        )

        # In the dbclients implementation, the factory return might be the pool itself
        # or a context manager depending on implementation details.
        # Assuming get_postgres_client returns the pool directly for "async" type based on common patterns,
        # or we might need to adjust based on your specific dbclients lib.
        # Let's assume strictly it returns the pool instance.
        self._pool = pool_factory
        self._pool_loop = current_loop

        await self.initialize_schema()
        return self._pool

    async def initialize_schema(self) -> None:
        """Ensure schema exists. Safe to call repeatedly."""
        if not self._pool:
            return

        async with self._pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS conduit_cache_entries (
                    cache_name  text      NOT NULL,
                    cache_key   text      NOT NULL,
                    payload     jsonb     NOT NULL,
                    created_at  timestamptz NOT NULL DEFAULT now(),
                    updated_at  timestamptz NOT NULL DEFAULT now(),
                    PRIMARY KEY (cache_name, cache_key)
                );
            """)

    async def get(self, request: GenerationRequest) -> GenerationResponse | None:
        pool = await self._get_pool()
        key = self._request_to_key(request)

        query = """
            SELECT payload
            FROM conduit_cache_entries
            WHERE cache_name = $1 AND cache_key = $2
        """

        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, self.project_name, key)

        if row is None:
            self._misses += 1
            return None

        self._hits += 1
        payload = json.loads(row["payload"])
        return GenerationResponse.model_validate(payload)

    async def set(
        self, request: GenerationRequest, response: GenerationResponse
    ) -> None:
        pool = await self._get_pool()
        key = self._request_to_key(request)
        payload = response.model_dump_json()

        query = """
            INSERT INTO conduit_cache_entries (cache_name, cache_key, payload)
            VALUES ($1, $2, $3)
            ON CONFLICT (cache_name, cache_key)
            DO UPDATE SET
                payload = EXCLUDED.payload,
                updated_at = now()
        """

        async with pool.acquire() as conn:
            await conn.execute(query, self.project_name, key, payload)

    async def wipe(self) -> None:
        pool = await self._get_pool()
        query = "DELETE FROM conduit_cache_entries WHERE cache_name = $1"
        async with pool.acquire() as conn:
            await conn.execute(query, self.project_name)

        self._hits = 0
        self._misses = 0
        self._start_time = time.time()

    async def cache_stats(self) -> dict[str, object]:
        pool = await self._get_pool()
        async with pool.acquire() as conn:
            row_count = await conn.fetchval(
                "SELECT COUNT(*) FROM conduit_cache_entries WHERE cache_name = $1",
                self.project_name,
            )
            size_bytes = await conn.fetchval(
                "SELECT COALESCE(SUM(pg_column_size(payload)), 0) FROM conduit_cache_entries WHERE cache_name = $1",
                self.project_name,
            )
            bounds = await conn.fetchrow(
                """
                SELECT 
                    to_char(MIN(created_at), 'YYYY-MM-DD') as oldest,
                    to_char(MAX(updated_at), 'YYYY-MM-DD') as latest
                FROM conduit_cache_entries
                WHERE cache_name = $1
            """,
                self.project_name,
            )

        uptime_seconds = time.time() - self._start_time

        return {
            "cache_name": self.project_name,
            "total_entries": row_count,
            "total_size_bytes": size_bytes,
            "uptime_seconds": uptime_seconds,
            "hits": self._hits,
            "misses": self._misses,
            "oldest_record": bounds["oldest"] if bounds else None,
            "latest_record": bounds["latest"] if bounds else None,
        }

    def _request_to_key(self, request: GenerationRequest) -> str:
        return request.generate_cache_key()

```

## FILE: src/conduit/storage/cache/protocol.py
```py
from __future__ import annotations
from typing import runtime_checkable, TYPE_CHECKING, Protocol
import logging

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.result.response import GenerationResponse


@runtime_checkable
class ConduitCache(Protocol):
    """
    Async interface for caching LLM responses.
    """

    async def get(self, request: GenerationRequest) -> GenerationResponse | None:
        """
        Return a cached Response for this Request, or None if not present.
        """
        ...

    async def set(
        self, request: GenerationRequest, response: GenerationResponse
    ) -> None:
        """
        Store or update the cached Response for this Request.
        """
        ...

    async def wipe(self) -> None:
        """
        Remove all entries for this cache instance.
        """
        ...

    async def cache_stats(self) -> dict[str, object]:
        """
        Return stats like total_entries, size, etc.
        """
        ...

```

## FILE: src/conduit/storage/odometer/odometer.py
```py
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from conduit.storage.odometer.token_event import TokenEvent
import logging

logger = logging.getLogger(__name__)


class Odometer(BaseModel):
    """
    In-memory odometer for tracking token usage and derived aggregates.
    """

    # Raw event storage
    events: list[TokenEvent] = Field(default_factory=list)

    # Aggregated totals
    total_input_tokens: int = Field(default=0)
    total_output_tokens: int = Field(default=0)
    total_tokens: int = Field(default=0)

    # Provider-level aggregation
    provider_totals: dict[str, dict[str, int]] = Field(default_factory=dict)

    # Model-level aggregation
    model_totals: dict[str, dict[str, int]] = Field(default_factory=dict)

    # Time-based aggregation (by date string YYYY-MM-DD)
    daily_totals: dict[str, dict[str, int]] = Field(default_factory=dict)

    # Session metadata
    session_start: datetime = Field(default_factory=datetime.now)
    last_updated: datetime = Field(default_factory=datetime.now)

    # Host tracking
    hosts: set[str] = Field(default_factory=set)

    def record(self, token_event: TokenEvent) -> None:
        """
        Add a TokenEvent to the odometer and update all aggregates.
        """
        logger.debug(f"Recording TokenEvent: {token_event}")
        self.events.append(token_event)

        # Update Aggregates (Keep running totals in memory)
        self.total_input_tokens += token_event.input_tokens
        self.total_output_tokens += token_event.output_tokens
        self.total_tokens += token_event.input_tokens + token_event.output_tokens

        # Provider totals
        provider_key = str(token_event.provider)
        self._update_aggregate(self.provider_totals, provider_key, token_event)

        # Model totals
        self._update_aggregate(self.model_totals, token_event.model, token_event)

        # Daily totals
        date_str = datetime.fromtimestamp(token_event.timestamp).strftime("%Y-%m-%d")
        self._update_aggregate(self.daily_totals, date_str, token_event)

        self.hosts.add(token_event.host)
        self.last_updated = datetime.now()

    def _update_aggregate(self, target_dict: dict, key: str, event: TokenEvent):
        entry = target_dict.setdefault(key, {"input": 0, "output": 0, "total": 0})
        entry["input"] += event.input_tokens
        entry["output"] += event.output_tokens
        entry["total"] += event.input_tokens + event.output_tokens

    def pop_events(self) -> list[TokenEvent]:
        """
        Atomic retrieval: Return all current events and clear the internal buffer.
        Used by flusher/rescue to ensure no data is duplicated or left behind.
        """
        if not self.events:
            return []

        # Atomic swap
        current_batch = self.events
        self.events = []
        return current_batch

    def requeue_events(self, failed_events: list[TokenEvent]) -> None:
        """
        Safety valve: If a DB write fails, put the events back at the front of the queue
        so they can be rescued by the file dump on exit.
        """
        self.events = failed_events + self.events

    # Simple query helpers
    def get_provider_breakdown(self) -> dict[str, dict[str, int]]:
        """Return aggregate totals by provider."""
        return self.provider_totals

    def get_model_breakdown(self) -> dict[str, dict[str, int]]:
        """Return aggregate totals by model."""
        return self.model_totals

    def get_daily_usage(self, date_str: str) -> dict[str, int]:
        """
        Return usage totals for a specific day (YYYY-MM-DD).
        Missing dates return zeros.
        """
        return self.daily_totals.get(date_str, {"input": 0, "output": 0, "total": 0})

    def get_recent_activity(self, hours: int = 24) -> list[TokenEvent]:
        """
        Return events whose timestamps fall within the last `hours`.
        """
        if not self.events:
            return []

        cutoff = datetime.now() - timedelta(hours=hours)
        cutoff_ts = int(cutoff.timestamp())
        return [e for e in self.events if (e.timestamp or 0) >= cutoff_ts]

    def clear(self) -> None:
        """
        Clear all events and aggregates.
        """
        self.events.clear()
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_tokens = 0
        self.provider_totals.clear()
        self.model_totals.clear()
        self.daily_totals.clear()
        self.hosts.clear()
        self.session_start = datetime.now()
        self.last_updated = self.session_start

    def stats(self) -> None:
        """
        Pretty-print the stats from the odometer using rich.
        """
        from rich.console import Console
        from rich.table import Table
        from rich.text import Text

        console = Console()
        table = Table(title="Odometer Stats")
        table.add_column("Category", justify="left", style="cyan")
        table.add_column("Input Tokens", justify="right", style="green")
        table.add_column("Output Tokens", justify="right", style="yellow")
        table.add_column("Total Tokens", justify="right", style="magenta")

        # Overall totals
        table.add_row(
            "Total",
            str(self.total_input_tokens),
            str(self.total_output_tokens),
            str(self.total_tokens),
        )

        # Providers
        for provider, totals in self.provider_totals.items():
            table.add_row(
                Text(f"Provider: {provider}", style="blue"),
                str(totals["input"]),
                str(totals["output"]),
                str(totals["total"]),
            )

        # Models
        for model, totals in self.model_totals.items():
            table.add_row(
                Text(f"Model: {model}", style="blue"),
                str(totals["input"]),
                str(totals["output"]),
                str(totals["total"]),
            )

        # Daily breakdown
        for date_str, totals in self.daily_totals.items():
            table.add_row(
                Text(f"Date: {date_str}", style="blue"),
                str(totals["input"]),
                str(totals["output"]),
                str(totals["total"]),
            )

        console.print(table)

```

## FILE: src/conduit/storage/odometer/odometer_registry.py
```py
from conduit.storage.odometer.odometer import Odometer
from conduit.storage.odometer.token_event import TokenEvent
from conduit.storage.odometer.pgres.postgres_backend_async import AsyncPostgresOdometer
from conduit.config import settings
import atexit
import signal
import sys
import json
import logging
from types import FrameType

logger = logging.getLogger(__name__)

RESCUE_FILE = settings.paths["DATA_DIR"] / "odometer_rescue.json"


class OdometerRegistry:
    """
    Central entry point for tracking token usage.

    Architecture:
    - Hot Path: flush() calls asyncpg to save to DB.
    - Cold Path: atexit/signal hooks dump to JSON file (Rescue File).
    - Recovery: On init, checks for Rescue File and ingests it.
    """

    def __init__(self):
        self.session_odometer: Odometer = Odometer()
        # We instantiate the backend here. Since it's lazy, it won't connect yet.
        self.async_backend = AsyncPostgresOdometer(db_name="conduit")
        self._saved_on_exit: bool = False

        # Register Hooks
        atexit.register(self._save_on_exit)
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def emit_token_event(self, event: TokenEvent) -> None:
        """Record an event in the in-memory buffer."""
        self.session_odometer.record(event)

    async def flush(self) -> None:
        """
        HOT PATH: Async flush to database.
        Call this periodically or at the end of requests.
        """
        # 1. Atomic Pop
        batch = self.session_odometer.pop_events()
        if not batch:
            return

        try:
            # 2. Async Write
            await self.async_backend.store_events(batch)
        except Exception as e:
            logger.error(f"Odometer async flush failed: {e}")
            # 3. Safety Requeue (so they are saved to disk if app crashes)
            self.session_odometer.requeue_events(batch)

    def _signal_handler(self, signum: int, frame: FrameType | None) -> None:
        """Handle interrupt signals by triggering save_on_exit."""
        self._save_on_exit()
        sys.exit(0)

    def _save_on_exit(self) -> None:
        """
        COLD PATH: Synchronous dump to JSON file.
        Executed during interpreter shutdown.
        """
        if self._saved_on_exit:
            return
        self._saved_on_exit = True

        # Grab everything remaining
        batch = self.session_odometer.pop_events()
        if not batch:
            return

        try:
            # Simple JSON dump
            data = [e.model_dump() for e in batch]

            # If file exists (rare double crash), append
            if RESCUE_FILE.exists():
                try:
                    existing = json.loads(RESCUE_FILE.read_text())
                    if isinstance(existing, list):
                        data = existing + data
                except (json.JSONDecodeError, OSError):
                    pass  # Overwrite corrupt file

            RESCUE_FILE.parent.mkdir(parents=True, exist_ok=True)
            RESCUE_FILE.write_text(json.dumps(data))
            # Use print because logging might be dead
            print(f"[Odometer] Rescued {len(batch)} unsaved events to {RESCUE_FILE}")
        except Exception as e:
            print(f"[Odometer] CRITICAL: Failed to rescue events: {e}")

    async def recover(self) -> None:
        """
        Run this on server startup/middleware init to ingest rescued events.
        """
        if not RESCUE_FILE.exists():
            return

        logger.info(f"Odometer found rescue file: {RESCUE_FILE}")
        try:
            content = RESCUE_FILE.read_text()
            if not content:
                RESCUE_FILE.unlink()
                return

            data = json.loads(content)
            events = [TokenEvent(**item) for item in data]

            if events:
                logger.info(f"Recovering {len(events)} events from previous session...")
                await self.async_backend.store_events(events)

            # Clean up
            RESCUE_FILE.unlink()
            logger.info("Rescue file recovered and deleted.")

        except Exception as e:
            logger.error(f"Odometer recovery failed: {e}")

```

## FILE: src/conduit/storage/odometer/pgres/persistence_backend.py
```py
from abc import ABC, abstractmethod
from datetime import date

from conduit.storage.odometer.token_event import TokenEvent


class PersistenceBackend(ABC):
    """Abstract interface for odometer persistence."""

    @abstractmethod
    def store_events(self, events: list[TokenEvent]) -> None:
        """Store raw token events."""
        raise NotImplementedError

    @abstractmethod
    def get_events(
        self,
        start_date: date | None = None,
        end_date: date | None = None,
        provider: str | None = None,
        model: str | None = None,
        host: str | None = None,
    ) -> list[TokenEvent]:
        """Query token events with filters."""
        raise NotImplementedError

    @abstractmethod
    def get_aggregates(
        self,
        group_by: str,  # "provider", "model", "host", "date"
        start_date: date | None = None,
        end_date: date | None = None,
    ) -> dict[str, dict[str, int]]:
        """Get aggregated statistics by group key."""
        raise NotImplementedError

```

## FILE: src/conduit/storage/odometer/pgres/postgres_backend_async.py
```py
from __future__ import annotations
import logging
import asyncio
from datetime import datetime, date
from typing import TYPE_CHECKING
from conduit.storage.odometer.token_event import TokenEvent

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from asyncpg import Pool


class AsyncPostgresOdometer:
    """
    Async Postgres backend for Odometer.
    Manages its own lazy connection pool to support restarts/different loops.
    """

    def __init__(self, db_name: str = "conduit"):
        self.db_name = db_name
        self._pool: Pool | None = None
        self._pool_loop: asyncio.AbstractEventLoop | None = None

    async def _get_pool(self) -> Pool:
        current_loop = asyncio.get_running_loop()

        if (
            self._pool
            and self._pool_loop is current_loop
            and not current_loop.is_closed()
        ):
            return self._pool

        logger.debug("Initializing asyncpg pool for Odometer")
        from dbclients.clients.postgres import get_postgres_client

        # get_postgres_client("async") returns the pool
        pool_factory = await get_postgres_client(
            client_type="async", dbname=self.db_name
        )
        self._pool = pool_factory
        self._pool_loop = current_loop

        # Ensure schema exists (lightweight check)
        await self._ensure_schema()
        return self._pool

    async def _ensure_schema(self):
        if not self._pool:
            return
        create_sql = """
        CREATE TABLE IF NOT EXISTS token_events (
            id SERIAL PRIMARY KEY,
            provider VARCHAR(50) NOT NULL,
            model VARCHAR(200) NOT NULL,
            input_tokens INTEGER NOT NULL,
            output_tokens INTEGER NOT NULL,
            timestamp BIGINT NOT NULL,
            host VARCHAR(100) NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        async with self._pool.acquire() as conn:
            await conn.execute(create_sql)

    async def store_events(self, events: list[TokenEvent]) -> None:
        if not events:
            return

        pool = await self._get_pool()
        insert_sql = """
        INSERT INTO token_events (provider, model, input_tokens, output_tokens, timestamp, host)
        VALUES ($1, $2, $3, $4, $5, $6)
        """
        records = [
            (e.provider, e.model, e.input_tokens, e.output_tokens, e.timestamp, e.host)
            for e in events
        ]
        async with pool.acquire() as conn:
            await conn.executemany(insert_sql, records)
            logger.debug(f"Async Odometer flush: stored {len(events)} events")

    # --- Read Methods for Reporting ---

    async def get_overall_stats(self) -> dict:
        """Get overall statistics."""
        pool = await self._get_pool()
        query = """
        SELECT 
            COUNT(*) as requests,
            SUM(input_tokens) as total_input,
            SUM(output_tokens) as total_output,
            COUNT(DISTINCT provider) as unique_providers,
            COUNT(DISTINCT model) as unique_models
        FROM token_events
        """
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query)
            if not row:
                return {}

            # Handle potential None returns from SUM on empty table
            t_input = row["total_input"] or 0
            t_output = row["total_output"] or 0

            return {
                "requests": row["requests"] or 0,
                "input": t_input,
                "output": t_output,
                "total_tokens": t_input + t_output,
                "providers": row["unique_providers"] or 0,
                "models": row["unique_models"] or 0,
            }

    async def get_aggregates(
        self,
        group_by: str,  # "provider", "model", "host", "date"
        start_date: date | None = None,
        end_date: date | None = None,
    ) -> dict[str, dict[str, int]]:
        """Get aggregated statistics."""
        pool = await self._get_pool()

        valid_groups = ["provider", "model", "host", "date"]
        if group_by not in valid_groups:
            raise ValueError(f"group_by must be one of {valid_groups}")

        where_conditions = []
        params = []
        param_idx = 1

        if start_date:
            start_ts = int(
                datetime.combine(start_date, datetime.min.time()).timestamp()
            )
            where_conditions.append(f"timestamp >= ${param_idx}")
            params.append(start_ts)
            param_idx += 1

        if end_date:
            end_ts = int(datetime.combine(end_date, datetime.max.time()).timestamp())
            where_conditions.append(f"timestamp <= ${param_idx}")
            params.append(end_ts)
            param_idx += 1

        if group_by == "date":
            # Postgres specific: convert epoch to date
            group_clause = "DATE(to_timestamp(timestamp))"
            select_clause = f"{group_clause} as group_key"
        else:
            group_clause = group_by
            select_clause = f"{group_by} as group_key"

        base_query = f"""
        SELECT 
            {select_clause},
            SUM(input_tokens) as total_input,
            SUM(output_tokens) as total_output,
            SUM(input_tokens + output_tokens) as total_tokens,
            COUNT(*) as event_count
        FROM token_events
        """

        if where_conditions:
            base_query += f" WHERE {' AND '.join(where_conditions)}"

        base_query += f" GROUP BY {group_clause} ORDER BY total_tokens DESC"

        async with pool.acquire() as conn:
            rows = await conn.fetch(base_query, *params)

            result = {}
            for row in rows:
                key = str(row["group_key"])
                result[key] = {
                    "input": row["total_input"],
                    "output": row["total_output"],
                    "total": row["total_tokens"],
                    "events": row["event_count"],
                }
            return result

```

## FILE: src/conduit/storage/odometer/pgres/wipe_database.py
```py
from dbclients import get_postgres_client
import logging

logger = logging.getLogger(__name__)

get_db_connection = get_postgres_client("context_db", dbname="chain")


def wipe_token_events_database(confirm: bool = False) -> bool:
    """
    Delete all token events from the database.

    Args:
        confirm: Must be True to actually perform the deletion (safety check)

    Returns:
        bool: True if successful, False otherwise
    """
    if not confirm:
        logger.warning(
            "wipe_token_events_database called without confirm=True. No action taken."
        )
        return False

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # Get count before deletion for logging
                cursor.execute("SELECT COUNT(*) FROM token_events")
                count_before = cursor.fetchone()[0]

                # Delete all records
                cursor.execute("DELETE FROM token_events")
                deleted_count = cursor.rowcount

                # Reset the sequence for the ID column
                cursor.execute("ALTER SEQUENCE token_events_id_seq RESTART WITH 1")

                conn.commit()
                logger.warning(
                    f"Database wiped: deleted {deleted_count} token events (was {count_before} total)"
                )
                return True

    except Exception as e:
        logger.error(f"Failed to wipe database: {e}")
        return False


def main():
    """
    Main function to execute the wipe operation.
    """
    confirm = (
        input("Are you sure you want to wipe the token events database? (yes/no): ")
        .strip()
        .lower()
    )
    if confirm == "yes":
        success = wipe_token_events_database(confirm=True)
        if success:
            print("Database wiped successfully.")
        else:
            print("Failed to wipe the database.")
    else:
        print("Operation cancelled. No changes made.")


if __name__ == "__main__":
    main()

```

## FILE: src/conduit/storage/odometer/token_event.py
```py
from pydantic import BaseModel, Field, model_validator
from conduit.core.model.models.provider import Provider
from typing import Any


class TokenEvent(BaseModel):
    # Required input fields
    model: str = Field(
        ..., description="Specific model by name (gpt-4o, claude-3.5-sonnet, etc.)."
    )
    input_tokens: int = Field(
        ..., description="Prompt tokens as defined and provided in API response."
    )
    output_tokens: int = Field(
        ..., description="Output tokens as defined and provided in API response."
    )

    # Generated fields (optional on input, filled automatically if missing/None)
    timestamp: int | None = Field(
        default=None, description="Unix epoch time in seconds."
    )
    host: str | None = Field(
        default=None, description="Simple host detection for multi-machine tracking."
    )
    provider: Provider | None = Field(
        default=None,
        description="Model provider (Anthropic, OpenAI, Google, etc.).",
    )

    @model_validator(mode="before")
    def fill_derived_fields(cls, data: dict[str, Any]):
        # timestamp: if missing or None, generate epoch seconds
        ts = data.get("timestamp")
        if ts is None:
            data["timestamp"] = cls._get_current_timestamp_s()

        # host: if missing or None, detect hostname
        host = data.get("host")
        if host is None:
            data["host"] = cls._get_hostname()

        # provider: if missing or None, infer from model
        prov = data.get("provider")
        if prov is None:
            model = data["model"]
            data["provider"] = cls._identify_provider(model)

        return data

    @staticmethod
    def _get_current_timestamp_s() -> int:
        import time

        # seconds to match PostgresBackend usage and existing data
        return int(time.time())

    @staticmethod
    def _get_hostname() -> str:
        import socket

        try:
            return socket.gethostname()
        except Exception:
            return "unknown"

    @staticmethod
    def _identify_provider(model: str) -> Provider:
        from conduit.core.model.models.modelstore import ModelStore

        return ModelStore.identify_provider(model)


if __name__ == "__main__":
    # Example usage
    event = TokenEvent(model="gpt-4o", input_tokens=150, output_tokens=50)
    print(event.model_dump_json(indent=2))

```

## FILE: src/conduit/storage/repository/persistence_mode.py
```py
from enum import Enum


class PersistenceMode(Enum):
    # Always create new conversation; discard existing
    OVERWRITE = "overwrite"

    # Load conversation if exists, else create new; start from last message
    RESUME = "resume"

    # Resume but also: generate a conversation title if missing
    CHAT = "chat"

```

## FILE: src/conduit/storage/repository/postgres_repository.py
```py
import json
import logging
from typing import Any
from pydantic import TypeAdapter

from conduit.domain.conversation.session import Session
from conduit.domain.conversation.conversation import Conversation
from conduit.domain.message.message import MessageUnion, Message

logger = logging.getLogger(__name__)


class AsyncPostgresSessionRepository:
    """
    Async Project-Scoped Session Repository (DAG Architecture).

    Architecture:
    - conduit_sessions: Partitioned by 'project_name'.
    - conduit_messages: Stored as a Graph (DAG) linked to sessions.
    """

    def __init__(self, project_name: str, pool: Any):
        """
        :param project_name: The scoping key (e.g., "summarizer", "chatbot-v1").
        :param pool: An initialized asyncpg.Pool instance.
        """
        self.project_name = project_name
        self.pool = pool
        self._message_adapter = TypeAdapter(MessageUnion)

    async def initialize(self) -> None:
        """Idempotent schema creation."""
        async with self.pool.acquire() as conn:
            await conn.execute("""
                -- 1. Scoped Sessions
                CREATE TABLE IF NOT EXISTS conduit_sessions (
                    session_id TEXT PRIMARY KEY,
                    project_name TEXT NOT NULL,
                    leaf_message_id TEXT,
                    title TEXT,
                    metadata JSONB DEFAULT '{}',
                    created_at BIGINT,
                    last_updated TIMESTAMPTZ DEFAULT NOW()
                );
                CREATE INDEX IF NOT EXISTS idx_sessions_project ON conduit_sessions(project_name);
                CREATE INDEX IF NOT EXISTS idx_sessions_updated ON conduit_sessions(last_updated);

                -- 2. DAG Messages (Graph Storage)
                CREATE TABLE IF NOT EXISTS conduit_messages (
                    message_id TEXT PRIMARY KEY,
                    session_id TEXT NOT NULL REFERENCES conduit_sessions(session_id) ON DELETE CASCADE,
                    predecessor_id TEXT REFERENCES conduit_messages(message_id),
                    role TEXT NOT NULL,
                    content JSONB,
                    created_at BIGINT,
                    
                    -- Specialized columns for query ease/indexing
                    metadata JSONB DEFAULT '{}',
                    tool_calls JSONB,
                    images JSONB,
                    audio JSONB,
                    parsed JSONB
                );
                CREATE INDEX IF NOT EXISTS idx_messages_session ON conduit_messages(session_id);
                CREATE INDEX IF NOT EXISTS idx_messages_predecessor ON conduit_messages(predecessor_id);
            """)

    # --- Read Operations ---

    async def get_session(self, session_id: str) -> Session | None:
        """
        Loads a Session and its full message graph.
        Enforces project ownership.
        """
        q_session = """
            SELECT session_id, leaf_message_id, title, metadata, created_at
            FROM conduit_sessions 
            WHERE session_id = $1 AND project_name = $2
        """
        q_messages = """
            SELECT * FROM conduit_messages WHERE session_id = $1
        """

        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(q_session, session_id, self.project_name)
            if not row:
                return None

            # Fetch all messages for this session
            msg_rows = await conn.fetch(q_messages, session_id)

        # Rehydrate Messages
        message_dict = {}
        for r in msg_rows:
            try:
                msg_obj = self._row_to_message(r)
                message_dict[msg_obj.message_id] = msg_obj
            except Exception as e:
                logger.error(f"Failed to hydrate message {r['message_id']}: {e}")
                continue

        # We construct the session using the data from DB
        session = Session(
            session_id=row["session_id"],
            leaf=row["leaf_message_id"],
            created_at=row["created_at"],
            message_dict=message_dict,
        )
        return session

    async def get_conversation(self, leaf_message_id: str) -> Conversation | None:
        """
        Rehydrates a linear Conversation view by walking backwards from a leaf.
        """
        # Recursive CTE to walk up the tree
        # We join on sessions to ensure project scoping logic applies
        query = """
        WITH RECURSIVE conversation_tree AS (
            -- Base Case: The Leaf
            SELECT m.*, 1 as depth
            FROM conduit_messages m
            JOIN conduit_sessions s ON m.session_id = s.session_id
            WHERE m.message_id = $1 AND s.project_name = $2
            
            UNION ALL
            
            -- Recursive Step: The Predecessor
            SELECT p.*, ct.depth + 1
            FROM conduit_messages p
            INNER JOIN conversation_tree ct ON p.message_id = ct.predecessor_id
        )
        SELECT * FROM conversation_tree ORDER BY depth DESC;
        """

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, leaf_message_id, self.project_name)

        if not rows:
            return None

        messages = [self._row_to_message(r) for r in rows]
        return Conversation(messages=messages)

    async def get_message(self, message_id: str) -> Message | None:
        """
        Fetch a single specific message by ID.
        Enforces project ownership via JOIN.
        """
        query = """
            SELECT m.*
            FROM conduit_messages m
            JOIN conduit_sessions s ON m.session_id = s.session_id
            WHERE m.message_id = $1 AND s.project_name = $2
        """
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(query, message_id, self.project_name)

        if not row:
            return None

        return self._row_to_message(row)

    async def list_sessions(self, limit: int = 20) -> list[dict[str, Any]]:
        """Lists sessions for the current project."""
        query = """
            SELECT session_id, title, created_at, leaf_message_id, last_updated
            FROM conduit_sessions
            WHERE project_name = $1
            ORDER BY last_updated DESC
            LIMIT $2
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, self.project_name, limit)

            return [
                {
                    "session_id": r["session_id"],
                    "title": r["title"] or "Untitled Session",
                    "created_at": r["created_at"],
                    "last_updated": r["last_updated"],
                    "leaf_id": r["leaf_message_id"],
                }
                for r in rows
            ]

    # --- Write Operations ---

    async def save_session(self, session: Session, name: str | None = None) -> None:
        """
        Upserts Session (scoped to Project) and its Messages.
        """
        # 1. Topological Sort for FK validity (Parent before Child)
        sorted_msgs = self._topological_sort(session.message_dict)

        # 2. Serialize Messages for Batch Insert
        msg_records = []
        for msg in sorted_msgs:
            d = msg.model_dump(mode="json")
            msg_records.append(
                (
                    d["message_id"],
                    d["session_id"],
                    d.get("predecessor_id"),
                    d["role"],
                    json.dumps(d.get("content")),
                    d["created_at"],
                    json.dumps(d.get("metadata") or {}),
                    json.dumps(d.get("tool_calls")),
                    json.dumps(d.get("images")),
                    json.dumps(d.get("audio")),
                    json.dumps(d.get("parsed")),
                )
            )

        upsert_session_sql = """
            INSERT INTO conduit_sessions (session_id, project_name, leaf_message_id, title, created_at, last_updated)
            VALUES ($1, $2, $3, $4, $5, NOW())
            ON CONFLICT (session_id) DO UPDATE SET
                leaf_message_id = EXCLUDED.leaf_message_id,
                title = COALESCE($4, conduit_sessions.title),
                last_updated = NOW(),
                project_name = EXCLUDED.project_name;
        """

        upsert_message_sql = """
            INSERT INTO conduit_messages (
                message_id, session_id, predecessor_id, role, content, 
                created_at, metadata, tool_calls, images, audio, parsed
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
            ON CONFLICT (message_id) DO NOTHING;
        """

        async with self.pool.acquire() as conn:
            async with conn.transaction():
                # A. Upsert Session first (Must exist for Message FKs)
                await conn.execute(
                    upsert_session_sql,
                    session.session_id,
                    self.project_name,
                    session.leaf,
                    name,
                    session.created_at,
                )

                # B. Batch Upsert Messages
                if msg_records:
                    await conn.executemany(upsert_message_sql, msg_records)

    # --- Maintenance Operations ---

    async def delete_session(self, session_id: str) -> None:
        """Delete specific session."""
        async with self.pool.acquire() as conn:
            # Enforce project ownership
            await conn.execute(
                "DELETE FROM conduit_sessions WHERE session_id = $1 AND project_name = $2",
                session_id,
                self.project_name,
            )

    async def wipe(self) -> None:
        """Delete ALL sessions for this project."""
        async with self.pool.acquire() as conn:
            await conn.execute(
                "DELETE FROM conduit_sessions WHERE project_name = $1",
                self.project_name,
            )

    # --- Helpers ---

    def _row_to_message(self, row: Any) -> Message:
        """Helper to rehydrate a Message object from a DB row."""
        msg_data = dict(row)

        # Deserialize JSONB fields
        for field in ["content", "tool_calls", "images", "audio", "parsed", "metadata"]:
            if isinstance(msg_data.get(field), str):
                try:
                    msg_data[field] = json.loads(msg_data[field])
                except (json.JSONDecodeError, TypeError):
                    pass  # Keep as is if decode fails

        # Inject discriminator if needed by Pydantic
        if "role" in msg_data:
            msg_data["role_str"] = msg_data["role"]

        return self._message_adapter.validate_python(msg_data)

    def _topological_sort(self, message_dict: dict[str, Message]) -> list[Message]:
        """
        Sorts messages Root -> Leaf to satisfy Foreign Key constraints.
        Simple BFS approach.
        """
        present_ids = set(message_dict.keys())
        children_map = {mid: [] for mid in present_ids}
        roots = []

        # Build graph
        for mid, msg in message_dict.items():
            pid = msg.predecessor_id
            if pid and pid in present_ids:
                children_map[pid].append(mid)
            else:
                # No predecessor in this batch -> Root
                roots.append(mid)

        # Traverse
        sorted_list = []
        queue = roots[:]

        while queue:
            current_id = queue.pop(0)
            if current_id in message_dict:
                sorted_list.append(message_dict[current_id])
                queue.extend(children_map.get(current_id, []))

        # Fallback: append disconnected nodes (shouldn't happen in valid DAG)
        if len(sorted_list) < len(message_dict):
            visited = set(m.message_id for m in sorted_list)
            for m in message_dict.values():
                if m.message_id not in visited:
                    sorted_list.append(m)

        return sorted_list


async def get_async_repository(project_name: str) -> AsyncPostgresSessionRepository:
    """
    Factory function to get an initialized AsyncPostgresSessionRepository.
    Requires 'asyncpg' installed and configured in dbclients.
    """
    from dbclients.clients.postgres import get_postgres_client

    # Must await the async factory to get the pool
    pool = await get_postgres_client(client_type="async", dbname="conduit")

    repo = AsyncPostgresSessionRepository(project_name=project_name, pool=pool)
    await repo.initialize()
    return repo

```

## FILE: src/conduit/storage/repository/protocol.py
```py
from __future__ import annotations
from typing import Protocol, runtime_checkable, Any, TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.domain.conversation.session import Session
    from conduit.domain.conversation.conversation import Conversation
    from conduit.domain.message.message import Message


@runtime_checkable
class AsyncSessionRepository(Protocol):
    """
    Async interface for persisting Conduit sessions (DAGs) and conversations (Linear Views).
    Strictly scoped to a specific 'project_name'.
    """

    async def initialize(self) -> None:
        """
        Ensure the underlying storage schema exists.
        """
        ...

    async def get_session(self, session_id: str) -> Session | None:
        """
        Rehydrates a full Session object, including the entire message graph
        (message_dict) associated with it.
        """
        ...

    async def get_conversation(self, leaf_message_id: str) -> Conversation | None:
        """
        Rehydrates a linear Conversation view by walking backwards from a specific leaf.
        """
        ...

    async def get_message(self, message_id: str) -> Message | None:
        """
        Fetch a single specific message by ID.
        """
        ...

    async def save_session(self, session: Session, name: str | None = None) -> None:
        """
        Upserts the Session metadata and *all* messages currently in the session.
        Uses topological sorting to ensure referential integrity.
        """
        ...

    async def list_sessions(self, limit: int = 20) -> list[dict[str, Any]]:
        """
        Returns lightweight metadata for recent sessions in this project.
        """
        ...

    async def delete_session(self, session_id: str) -> None:
        """
        Hard deletes a session and all its associated messages.
        """
        ...

    async def wipe(self) -> None:
        """
        Hard deletes ALL sessions for the current project.
        """
        ...

```

## FILE: src/conduit/sync.py
```py
# Orchestration classes
from conduit.core.conduit.conduit_sync import ConduitSync
from conduit.core.model.model_sync import ModelSync
from conduit.core.prompt.prompt import Prompt

# Primitives: dataclasses / enums
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.request.request import GenerationRequest
from conduit.domain.result.response import GenerationResponse

# Configs
from conduit.domain.request.generation_params import GenerationParams
from conduit.domain.config.conduit_options import ConduitOptions

Conduit = ConduitSync  # Alias for easier imports
Model = ModelSync  # Alias for easier imports


__all__ = [
    "Conduit",
    "ConduitOptions",
    "ConduitSync",
    "GenerationParams",
    "GenerationRequest",
    "GenerationResponse",
    "Model",
    "ModelSync",
    "Prompt",
    "Verbosity",
]

```

## FILE: src/conduit/utils/concurrency/warn.py
```py
import asyncio
import warnings


def _warn_if_loop_exists():
    try:
        asyncio.get_running_loop()
        warnings.warn(
            "Blocking call detected inside an event loop. Use the Async client or ConduitBatch to avoid blocking the main thread.",
            RuntimeWarning,
        )
    except RuntimeError:
        pass

```

## FILE: src/conduit/utils/logs/logging_config.py
```py
import logging
import os
from rich.logging import RichHandler


class PackagePathFilter(logging.Filter):
    """
    Prepends the root package name to record.pathname so Rich shows it in the right column.
    Example: my_app/utils/helpers.py -> my_app:helpers.py
    """

    def filter(self, record):
        record.root_package = record.name.split(".")[0]
        original_basename = os.path.basename(record.pathname)
        new_basename = f"{record.root_package}:{original_basename}"
        original_dirname = os.path.dirname(record.pathname)
        record.pathname = os.path.join(original_dirname, new_basename)
        return True


# --- Setup ---
log_level = int(os.getenv("PYTHON_LOG_LEVEL", "1"))  # Default to WARNING
levels = {1: logging.WARNING, 2: logging.INFO, 3: logging.DEBUG}

rich_handler = RichHandler(rich_tracebacks=True, markup=True)
rich_handler.addFilter(PackagePathFilter())

logging.basicConfig(
    level=levels.get(log_level, logging.INFO),
    format="%(message)s",
    datefmt="%Y-%m-%d %H:%M",
    handlers=[rich_handler],
)

```

## FILE: src/conduit/utils/progress/plain_formatters.py
```py
from __future__ import annotations
from typing import TYPE_CHECKING
from datetime import datetime
import json
from conduit.domain.result.response import GenerationResponse
from conduit.utils.progress.verbosity import Verbosity
from conduit.domain.message.message import UserMessage

if TYPE_CHECKING:
    from conduit.domain.request.request import GenerationRequest

# --- Helpers ---


def _extract_user_prompt(request: GenerationRequest) -> str:
    """Extract a preview of the user prompt from the request."""
    user_message = request.messages[-1].content
    assert isinstance(user_message, UserMessage)
    return str(user_message)


# --- Plain Text Formatters: GenerationResponse ---


def format_response_plain(response: GenerationResponse, verbosity: Verbosity) -> str:
    """Entry point for formatting a GenerationResponse object into plain text."""
    if verbosity == Verbosity.SUMMARY:
        return _response_summary_plain(response)
    elif verbosity == Verbosity.DETAILED:
        return _response_detailed_plain(response)
    elif verbosity == Verbosity.COMPLETE:
        return _response_complete_plain(response)
    elif verbosity == Verbosity.DEBUG:
        return _response_debug_plain(response)
    return ""


def _response_summary_plain(response: GenerationResponse) -> str:
    lines = []
    timestamp = datetime.now().strftime("%H:%M:%S")
    duration = getattr(response.metadata, "duration", 0)

    # Header
    lines.append(f"[{timestamp}] RESPONSE {duration:.1f}s")

    # Content (Truncated)
    content = str(response.content or "No content")
    if len(content) > 100:
        content = content[:100] + "..."
    lines.append(f"  {content}")

    return "\n".join(lines)


def _response_detailed_plain(response: GenerationResponse) -> str:
    lines = []
    timestamp = datetime.now().strftime("%H:%M:%S")
    duration = getattr(response.metadata, "duration", 0)

    # Header
    lines.append(f"[{timestamp}] CONVERSATION {duration:.1f}s (Detailed)")

    # User Prompt
    if response.request:
        user_prompt = _extract_user_prompt(response.request)
        if user_prompt:
            if len(user_prompt) > 200:
                user_prompt = user_prompt[:200] + "..."
            lines.append(f"User: {user_prompt}")

    # Assistant Content (Truncated)
    content = str(response.content or "No content")
    if len(content) > 200:
        content = content[:200] + "..."
    lines.append(f"Assistant: {content}")

    # Metadata
    meta = []
    if response.request:
        meta.append(f"Model: {response.request.params.model}")
        if response.request.params.temperature is not None:
            meta.append(f"Temp: {response.request.params.temperature}")

    if meta:
        lines.append("Metadata: " + " • ".join(meta))

    return "\n".join(lines)


def _response_complete_plain(response: GenerationResponse) -> str:
    lines = []
    timestamp = datetime.now().strftime("%H:%M:%S")
    duration = getattr(response.metadata, "duration", 0)

    # Header
    lines.append(f"[{timestamp}] FULL CONVERSATION {duration:.1f}s")

    # Full User Prompt
    if response.request:
        user_prompt = _extract_user_prompt(response.request)
        if user_prompt:
            lines.append(f"User: {user_prompt}")

    # Full Assistant Content
    content = str(response.content or "No content")
    lines.append(f"Assistant: {content}")

    # Detailed Metadata
    meta = []
    if response.request:
        meta.append(f"Model: {response.request.params.model}")
        if response.request.params.temperature is not None:
            meta.append(f"Temp: {response.request.params.temperature}")

        # Check for parser
        # Note: GenerationResponse model logic usually lives in params, tricky to extract name dynamically
        # if the class is hidden, but we try:
        if response.request.params.response_model:
            parser_name = str(response.request.params.response_model)
            meta.append(f"Parser: {parser_name}")

    if response.metadata:
        meta.append(f"Input Tokens: {response.metadata.input_tokens}")
        meta.append(f"Output Tokens: {response.metadata.output_tokens}")
        meta.append(f"Stop Reason: {response.metadata.stop_reason}")

    if meta:
        lines.append("Metadata: " + " • ".join(meta))

    return "\n".join(lines)


def _response_debug_plain(response: GenerationResponse) -> str:
    timestamp = datetime.now().strftime("%H:%M:%S")
    duration = getattr(response.metadata, "duration", 0)

    lines = [f"[{timestamp}] CONVERSATION DEBUG {duration:.1f}s"]

    # Construct clean debug object
    debug_data = {
        "model": response.request.params.model if response.request else "unknown",
        "duration": duration,
        "metadata": response.metadata,
    }

    if response.request:
        debug_data["user_prompt"] = _extract_user_prompt(response.request)
        debug_data["full_request"] = response.request

    # Handle content serialization
    content = response.content
    try:
        if hasattr(content, "model_dump"):
            debug_data["assistant_response"] = content.model_dump()
        else:
            debug_data["assistant_response"] = str(content)
    except Exception:
        debug_data["assistant_response"] = str(content)

    json_str = json.dumps(debug_data, indent=2, default=str)
    lines.append(json_str)

    return "\n".join(lines)

```

## FILE: src/conduit/utils/progress/plain_handler.py
```py
import sys
from typing import Any, override
from datetime import datetime
from conduit.utils.progress.protocol import DisplayHandler
from conduit.utils.progress.verbosity import Verbosity
from conduit.utils.progress.plain_formatters import (
    format_response_plain,
)


class PlainProgressHandler(DisplayHandler):
    """
    Append-only progress handler.
    Writes to STDERR so it doesn't corrupt piped output.
    """

    def _get_timestamp(self) -> str:
        return datetime.now().strftime("[%H:%M:%S]")

    @override
    def show_spinner(
        self, model_name: str, query_preview: str, verbosity: Verbosity
    ) -> None:
        if verbosity >= Verbosity.PROGRESS:
            ts = self._get_timestamp()
            # Added file=sys.stderr and flush=True
            print(
                f"{ts} [{model_name}] Starting: {query_preview}",
                file=sys.stderr,
                flush=True,
            )

    @override
    def show_complete(
        self,
        model_name: str,
        query_preview: str,
        duration: float,
        verbosity: Verbosity,
        response_obj: Any | None = None,
    ) -> None:
        if verbosity >= Verbosity.PROGRESS:
            ts = self._get_timestamp()
            print(
                f"{ts} [{model_name}] Completed ({duration:.2f}s)",
                file=sys.stderr,
                flush=True,
            )

        if response_obj and verbosity >= Verbosity.SUMMARY:
            text = format_response_plain(response_obj, verbosity)
            if text:
                print(text, file=sys.stderr, flush=True)

    @override
    def show_cached(
        self, model_name: str, query_preview: str, duration: float, verbosity: Verbosity
    ) -> None:
        if verbosity >= Verbosity.PROGRESS:
            ts = self._get_timestamp()
            print(
                f"{ts} [{model_name}] Cache Hit ({duration:.3f}s): {query_preview}",
                file=sys.stderr,
                flush=True,
            )

```

## FILE: src/conduit/utils/progress/protocol.py
```py
from typing import Protocol, runtime_checkable, Any
from conduit.utils.progress.verbosity import Verbosity


@runtime_checkable
class DisplayHandler(Protocol):
    """
    Protocol for handling UI feedback (spinners, logs, completion markers).
    Abstracts the difference between Rich (TUI) and Plain (CLI/Logs).
    """

    def show_spinner(
        self, model_name: str, query_preview: str, verbosity: Verbosity
    ) -> None:
        """
        Display the initial loading state (e.g., spinner or 'Starting...' log).
        """
        ...

    def show_complete(
        self,
        model_name: str,
        query_preview: str,
        duration: float,
        verbosity: Verbosity,
        response_obj: Any | None = None,
    ) -> None:
        """
        Display success state (e.g., Green Check or 'Completed').
        Should also handle detailed object printing if verbosity allows (SUMMARY/DETAILED/DEBUG).
        """
        ...

    def show_cached(
        self, model_name: str, query_preview: str, duration: float, verbosity: Verbosity
    ) -> None:
        """
        Display cache hit state (e.g., Lightning Bolt or 'Cached').
        """
        ...

```

## FILE: src/conduit/utils/progress/rich_formatters.py
```py
from __future__ import annotations
import json
from conduit.domain.result.response import GenerationResponse
from conduit.utils.progress.verbosity import Verbosity
from conduit.utils.progress.plain_formatters import _extract_user_prompt

from rich.console import RenderableType
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from rich.syntax import Syntax
from rich import box


# --- Rich Formatters: GenerationResponse ---
def format_response_rich(
    response: GenerationResponse, verbosity: Verbosity
) -> RenderableType | None:
    """Entry point for formatting a GenerationResponse object into a Rich Renderable."""
    if verbosity == Verbosity.SUMMARY:
        return _response_summary_rich(response)
    elif verbosity == Verbosity.DETAILED:
        return _response_detailed_rich(response)
    elif verbosity == Verbosity.COMPLETE:
        return _response_complete_rich(response)
    elif verbosity == Verbosity.DEBUG:
        return _response_debug_rich(response)
    return None


def _response_summary_rich(response: GenerationResponse) -> Panel:
    """
    Returns a Panel containing the rich representation of the response message.
    """
    # The response.message object knows how to render itself via __rich_console__
    return Panel(response.message, border_style="blue", expand=False)


def _response_detailed_rich(response: GenerationResponse) -> Panel:
    """Detailed view: User prompt + GenerationResponse content (truncated) + Metadata."""
    grid = Table.grid(padding=(0, 1))
    grid.add_column("Label", style="bold yellow", justify="right")
    grid.add_column("Content")

    # User
    if response.request:
        user_prompt = _extract_user_prompt(response.request)
        if len(user_prompt) > 300:
            user_prompt = user_prompt[:300] + "..."
        grid.add_row("User:", user_prompt)

    # Spacer
    grid.add_row("", "")

    # Assistant
    content = str(response.content or "No content")
    if len(content) > 500:
        content = content[:500] + "..."
    grid.add_row("[bold blue]Assistant:[/bold blue]", content)

    # Footer Metadata
    meta_text = f"Model: {response.request.params.model}"
    if response.request.params.temperature:
        meta_text += f" • Temp: {response.request.params.temperature}"

    return Panel(
        grid,
        title="[bold]Conversation Detail[/bold]",
        subtitle=f"[dim]{meta_text}[/dim]",
        subtitle_align="right",
        border_style="blue",
    )


def _response_complete_rich(response: GenerationResponse) -> Panel:
    """Complete view: Full messages, no truncation."""
    grid = Table.grid(padding=(0, 1))
    grid.add_column("Role", style="bold", width=10)
    grid.add_column("Content")

    if response.request:
        for msg in response.request.messages:
            role_style = "green" if msg.role == "system" else "yellow"
            grid.add_row(
                f"[{role_style}]{msg.role.value.upper()}[/{role_style}]",
                str(msg.content),
            )
            grid.add_row("", "")  # Spacer

    # GenerationResponse
    grid.add_row("[blue]ASSISTANT[/blue]", str(response.content))

    return Panel(
        grid,
        title="[bold]Full Conversation[/bold]",
        border_style="green",
        box=box.ROUNDED,
    )


def _response_debug_rich(response: GenerationResponse) -> Panel:
    """Debug view: Full JSON syntax highlighting."""
    debug_data = response.model_dump(mode="json", exclude_none=True)
    if response.request:
        debug_data["_user_prompt_preview"] = _extract_user_prompt(response.request)

    json_str = json.dumps(debug_data, indent=2)
    syntax = Syntax(json_str, "json", theme="monokai", word_wrap=True)

    return Panel(
        syntax,
        title="[bold red]DEBUG: GenerationResponse Object[/bold red]",
        border_style="red",
    )

```

## FILE: src/conduit/utils/progress/rich_handler.py
```py
from typing import Any, override
from rich.console import Console
from rich.status import Status
from conduit.utils.progress.protocol import DisplayHandler
from conduit.utils.progress.verbosity import Verbosity
from conduit.utils.progress.rich_formatters import (
    format_response_rich,
)


class RichProgressHandler(DisplayHandler):
    """
    Stateful progress handler for Rich TUI.
    Manages the active spinner and renders formatted panels.
    """

    def __init__(self, console: Console):
        self.console = console
        # We hold the active status object so we can stop it later
        self._status: Status | None = None

    def _stop_spinner(self):
        """Helper to cleanly stop any active spinner."""
        if self._status:
            self._status.stop()
            self._status = None

    @override
    def show_spinner(
        self, model_name: str, query_preview: str, verbosity: Verbosity
    ) -> None:
        if verbosity == Verbosity.SILENT:
            return

        # Stop any existing spinner to avoid visual glitches
        self._stop_spinner()

        if verbosity >= Verbosity.PROGRESS:
            # Create and start the spinner
            # We don't use 'with' here because the context manager logic
            # lives in the middleware, not the handler.
            status_text = f"[bold gold1]{model_name}[/bold gold1] | {query_preview}"
            self._status = self.console.status(status_text, spinner="dots")
            self._status.start()

    @override
    def show_complete(
        self,
        model_name: str,
        query_preview: str,
        duration: float,
        verbosity: Verbosity,
        response_obj: Any | None = None,
    ) -> None:
        self._stop_spinner()

        if verbosity == Verbosity.SILENT:
            return

        if verbosity >= Verbosity.PROGRESS:
            # 1. The One-Liner
            self.console.print(
                f"[green]✓[/green] [bold white]{model_name}[/bold white] | {query_preview} | [dim]({duration:.2f}s)[/dim]"
            )

            # 2. The Detail Panel (delegated to formatter)
            if response_obj and verbosity >= Verbosity.SUMMARY:
                panel = format_response_rich(response_obj, verbosity)
                if panel:
                    self.console.print(panel)

    @override
    def show_cached(
        self, model_name: str, query_preview: str, duration: float, verbosity: Verbosity
    ) -> None:
        self._stop_spinner()

        if verbosity == Verbosity.SILENT:
            return

        if verbosity >= Verbosity.PROGRESS:
            self.console.print(
                f"⚡ [bold cyan]{model_name}[/bold cyan] | {query_preview} | [cyan]Cached[/cyan] [dim]({duration:.3f}s)[/dim]"
            )

```

## FILE: src/conduit/utils/progress/utils.py
```py
from __future__ import annotations
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from conduit.domain.request.request import GenerationRequest
    from conduit.domain.message.message import Message


def extract_query_preview(
    input_data: GenerationRequest | list[Message] | str, max_length: int = 100
) -> str:
    """
    Extracts a representative string from various input types for display purposes.

    Handles:
    - GenerationRequest objects (finds last user message)
    - List of Messages (finds last user message)
    - Multimodal content (Text + [Image])
    - Simple strings
    """
    content = ""

    # Case 1: It's a GenerationRequest object
    if hasattr(input_data, "messages"):
        content = _extract_from_messages(input_data.messages)

    # Case 2: It's a list of Messages
    elif isinstance(input_data, list) and input_data and hasattr(input_data[0], "role"):
        content = _extract_from_messages(input_data)

    # Case 3: It's a string or other primitive
    else:
        content = str(input_data)

    # Clean up whitespace for preview
    content = content.strip().replace("\n", " ").replace("\r", " ")

    # Truncate if necessary
    if len(content) > max_length:
        return content[:max_length] + "..."
    return content


def _extract_from_messages(messages: list[Message]) -> str:
    """Helper to find the last user message text."""
    if not messages:
        return "No messages"

    # Walk backwards to find the last user message
    for message in reversed(messages):
        if message.role == "user":
            return _format_message_content(message)

    # Fallback to the last message if no user message found
    return _format_message_content(messages[-1])


def _format_message_content(message: Message) -> str:
    """Handle text vs multimodal content lists."""
    content = message.content

    # 1. Simple String
    if isinstance(content, str):
        return content

    # 2. Multimodal List (Text + Image/Audio)
    if isinstance(content, list):
        text_parts = []
        attachments = []

        for block in content:
            # Check Pydantic models or dicts
            block_type = getattr(block, "type", "unknown")

            if block_type == "text":
                text = getattr(block, "text", "")
                text_parts.append(text)
            elif block_type == "image_url":
                attachments.append("[Image]")
            elif block_type == "input_audio":
                attachments.append("[Audio]")

        combined = " ".join(text_parts)
        if attachments:
            combined += " " + " ".join(attachments)
        return combined

    # 3. Fallback
    return str(content)

```

## FILE: src/conduit/utils/progress/verbosity.py
```py
from enum import Enum


class Verbosity(Enum):
    """
    SILENT - Obviously nothing shown
    PROGRESS - Just the spinner/completion (your current default)
    SUMMARY - Basic request/response info (level 2 in your spec)
    DETAILED - Truncated messages in panels (level 3)
    COMPLETE - Full messages in panels (level 4)
    DEBUG - Full JSON with syntax highlighting (level 5)
    """

    SILENT = 0
    PROGRESS = 1
    SUMMARY = 2
    DETAILED = 3
    COMPLETE = 4
    DEBUG = 5

    @classmethod
    def from_input(cls, value):
        """
        Converts various input types to a Verbosity instance.
        """
        if value is False:
            return cls.SILENT
        elif value is True:
            return cls.PROGRESS
        elif isinstance(value, cls):
            return value
        elif isinstance(value, str):
            # Map string values to enum members
            string_map = {
                "": cls.SILENT,
                "v": cls.PROGRESS,
                "vv": cls.SUMMARY,
                "vvv": cls.DETAILED,
                "vvvv": cls.COMPLETE,
                "vvvvv": cls.DEBUG,
            }
            if value in string_map:
                return string_map[value]
            raise ValueError(f"Invalid verbosity: {value}")
        else:
            raise ValueError(f"Invalid verbosity type: {type(value)}")

    def __bool__(self) -> bool:
        """
        Returns True if the verbosity level is not SILENT.
        """
        return self != Verbosity.SILENT

    def __lt__(self, other):
        """Less than comparison based on enum values."""
        if self.__class__ is other.__class__:
            return self.value < other.value
        return NotImplemented

    def __le__(self, other):
        """Less than or equal comparison based on enum values."""
        if self.__class__ is other.__class__:
            return self.value <= other.value
        return NotImplemented

    def __gt__(self, other):
        """Greater than comparison based on enum values."""
        if self.__class__ is other.__class__:
            return self.value > other.value
        return NotImplemented

    def __ge__(self, other):
        """Greater than or equal comparison based on enum values."""
        if self.__class__ is other.__class__:
            return self.value >= other.value
        return NotImplemented

    def __str__(self) -> str:
        """String representation for logging and display."""
        return self.name

    def __repr__(self) -> str:
        """Detailed representation for debugging."""
        return f"Verbosity.{self.name}"

```

