from __future__ import annotations
from conduit.domain.message.role import Role
from pydantic import BaseModel, Field, model_validator
from typing import Literal, Any, Annotated
import time
import uuid


# content types
class TextContent(BaseModel):
    type: Literal["text"] = "text"
    text: str


class ImageContent(BaseModel):
    """
    Internal representation of an image.
    Ideally normalized to base64 or a stable URL before reaching here.
    """

    type: Literal["image_url"] = "image_url"
    url: str  # data:image/png;base64,... or https://...
    detail: Literal["auto", "low", "high"] = "auto"


class AudioContent(BaseModel):
    """
    Internal representation of input audio.
    """

    type: Literal["input_audio"] = "input_audio"
    data: str  # Base64 encoded audio
    format: Literal["wav", "mp3"] = "mp3"


class AudioOutput(BaseModel):
    """Native audio response (e.g. GPT-4o-audio)."""

    id: str
    data: str  # Base64 encoded audio
    transcript: str | None = None  # The text representation
    format: Literal["wav", "mp3", "pcm16"] = "wav"


class ImageOutput(BaseModel):
    """Generated image response (e.g. DALL-E 3)."""

    url: str | None = None
    b64_json: str | None = None
    revised_prompt: str | None = None  # DALL-E often rewrites the prompt


# All possible user content types
Content = str | list[TextContent | ImageContent | AudioContent]


# tool primitive
class ToolCall(BaseModel):
    """
    Represents a request from the Assistant to execute a function.
    """

    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: Literal["function"] = "function"
    function_name: str
    arguments: dict[str, Any]


# message types
class MessageBase(BaseModel):
    """
    Base class for all message types.
    """

    timestamp: int = Field(default_factory=lambda: int(time.time() * 1000))

    @property
    def time(self) -> str:
        """
        Human-readable time string.
        """
        return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.timestamp / 1000))


class SystemMessage(MessageBase):
    """
    System instructions.
    """

    role: Literal[Role.SYSTEM] = Role.SYSTEM
    content: str


class UserMessage(MessageBase):
    """
    Messages sent by the human.
    Supports simple strings or complex multimodal chains (Text + Image).
    """

    role: Literal[Role.USER] = Role.USER
    content: Content
    name: str | None = None


class AssistantMessage(BaseModel):
    """
    Messages generated by the LLM.
    Supports Text, Reasoning, Tools, Audio, Images, and Structured Objects.
    """

    role: Literal[Role.ASSISTANT] = Role.ASSISTANT

    # text
    content: str | None = None  # The visible text / JSON string
    reasoning: str | None = None  # Hidden Chain of Thought

    # action
    tool_calls: list[ToolCall] | None = None

    # multimodal
    audio: AudioOutput | None = None
    images: list[ImageOutput] | None = None

    # Special structured output (e.g. parsed JSON, XML, etc.) from Instructor
    parsed: Any | None = Field(default=None, exclude=True)

    @model_validator(mode="after")
    def validate_structure(self) -> AssistantMessage:
        # We need explicit presence of at least one payload
        has_payload = any(
            [
                self.content,
                self.tool_calls,
                self.reasoning,
                self.audio,
                self.images,
                self.parsed,
            ]
        )

        if not has_payload:
            raise ValueError(
                "AssistantMessage must have at least one of: content, tool_calls, reasoning, audio, images, or parsed output."
            )
        return self


class ToolMessage(MessageBase):
    """
    The result of a tool execution, fed back to the LLM.
    """

    role: Literal[Role.TOOL] = Role.TOOL
    content: str  # The output of the tool (usually JSON stringified)
    tool_call_id: str  # Links this result to the Assistant's ToolCall.id
    name: str | None = None  # Optional: name of the tool function


# discriminated union
Message = Annotated[
    SystemMessage | UserMessage | AssistantMessage | ToolMessage,
    Field(discriminator="role"),
]
