{"_default": {"18": {"model": "llama3.1:latest", "description": "Llama3.1:latest is an earlier release in Meta's Llama 3 series, available in multiple parameter sizes (e.g., 8B, 70B). It is a text-only model for tasks like chat, code generation, summarization, and general language understanding. The model does not process images, audio, or video. Context window is typically around 8,000 to 32,000 tokens. Strong in reasoning and instruction following.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": "varies", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "24": {"model": "gpt-4.1", "description": "GPT-4.1 is OpenAI's flagship large language model, designed for complex tasks, reasoning, and high-fidelity text generation. It is based on a transformer architecture and supports both text and image inputs (multimodal). The model features an expanded context window \u2014 typically 128k tokens \u2014 enabling it to manage lengthy interactions and context-rich documents. GPT-4.1 supports advanced reasoning and function calling. Known for robust performance on diverse benchmarks, it was released in early 2025 and is intended for applications requiring nuanced understanding and generation, including content creation, education, and consultation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "25": {"model": "gpt-4o", "description": "GPT-4o (\"omni\") is OpenAI's most advanced flagship model as of April 2025. It is fully multimodal, accepting text, image, and audio inputs, and can generate text and audio outputs. It features real-time, low-latency capabilities for conversation and supports a large context window, reportedly up to 128k tokens. GPT-4o is suitable for complex reasoning, function calling, and integration into conversational and assistant applications. Designed for seamless multimodal interaction, it excels in accessibility and real-world user engagement.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "26": {"model": "gpt-4-turbo", "description": "GPT-4 Turbo is an optimized variant of GPT-4 offering faster inference and lower costs, while maintaining the core GPT-4 architecture's capabilities. It is primarily focused on text and code generation with an extended context window (up to 128k tokens). Turbo does not natively support image, audio, or video processing. Released in late 2023, it is intended for scalable applications such as chatbots, code assistance, and generative tools where throughput and performance are important.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "27": {"model": "gpt-3.5-turbo-0125", "description": "GPT-3.5 Turbo (0125) is a cost-effective, high-performance text generation model based on the GPT-3.5 architecture. It supports text generation and completion tasks with a context window of up to 16,385 tokens. While capable in conversation, coding, and basic reasoning, GPT-3.5 Turbo does not have multimodal capabilities (image, audio, or video). It was released in early 2024 and is most commonly used for chatbots, drafting, summarization, and general text-based automation.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16385, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "28": {"model": "gpt-4o-mini", "description": "GPT-4o Mini is a lighter, more efficient variant of the GPT-4o model. It is designed to balance speed and capability, featuring support for text and image input while maintaining strong reasoning abilities. Its context window is smaller than GPT-4o, optimized for lower latency and cost. Intended use cases include lightweight conversational agents and tasks where full GPT-4o performance is not required. Unlike GPT-4o, it does not natively process audio or video.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "29": {"model": "gpt-4o-audio-preview", "description": "GPT-4o Audio Preview is a specialized variant of GPT-4o, focused on low-latency speech recognition (speech-to-text), speech translation, and text-to-speech (audio generation). It is designed for \"speech in, speech out\" conversational systems and supports multimodal input. With real-time audio capabilities and strong reasoning, it is well-suited for accessibility tools, voice assistants, and rapid dialogue applications. Its context window size and parameter count are not publicly detailed; released in 2025.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": true, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "30": {"model": "o1-preview", "description": "O1-Preview is part of OpenAI\u2019s reasoning-focused \"o-series\" models, aimed at complex, multi-step analytical and logical reasoning. It is a general-purpose model built for improved problem solving and knowledge tasks, with a context window likely between 8k and 32k tokens. Unlike flagship GPT models, o1-preview does not support image, audio, or video input. The model is primarily intended for high-quality text generation and in-depth reasoning applications.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "31": {"model": "o1-mini", "description": "O1-Mini is a deprecated, lightweight variant of the o1 reasoning model. It focuses on efficient text processing and basic reasoning and is designed for lower resource consumption. The model lacks multimodal capabilities, handling only text input and output, with a modest context window (typically 4k\u20138k tokens). O1-Mini is intended for simple chatbot, summarization, and text classification tasks.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "32": {"model": "o3-mini", "description": "O3-Mini is a small-scale variant of OpenAI's o3 reasoning model, engineered for efficiency and speed. It is suited for applications demanding rapid response and lower compute costs, supporting advanced text generation and reasoning. O3-Mini has a limited context window (8k tokens) and does not feature multimodal or generative audio/video capabilities. Its design targets chatbots, simple assistants, and text automation workflows.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "33": {"model": "o3-mini-high", "description": "O3-Mini-High is an enhanced version of O3-Mini, offering improved reasoning, accuracy, and performance at a small model size. It retains the same primary focus: text generation and complex analytical tasks, with an 8k context window. Like other mini models, O3-Mini-High does not support multimodal processing. It is suitable for efficient deployment in assistants, summarization, and logical inference use cases.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 8192, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "34": {"model": "o4-mini", "description": "O4-Mini is a compact, high-speed reasoning model in OpenAI\u2019s o-series, optimized for lower latency and deployment cost. It excels at text generation and multi-step reasoning with a context window of 8k\u201316k tokens. O4-Mini does not support image, audio, or video input/output. Intended uses include chatbots, digital assistants, and lightweight analysis tools where performance-to-cost ratio is key.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "35": {"model": "claude-3-5-haiku-20241022", "description": "Claude 3.5 Haiku (October 2024) is part of Anthropic's third-generation model lineup, designed as the fastest and most cost-effective tier. Built on a large transformer architecture, it excels in rapid text processing, general-purpose text and code tasks, and supports image analysis (multimodal input) but does not generate images or audio. It typically features a substantial context window (up to 200K tokens in advanced versions) and is optimized for applications requiring high throughput and responsiveness. Main use cases include content generation, summarization, and analysis at scale. Notable limitations include a lack of image, audio, and video generation capabilities.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "36": {"model": "claude-3-7-sonnet-20250219", "description": "Claude 3.7 Sonnet (February 2025) is Anthropic's mid-size transformer-based model in the Claude 3 family, targeting a balance of capability, speed, and operating cost. It supports robust text generation, strong reasoning, and image analysis through multimodal inputs, but does not generate images, audio, or video. With a large context window (likely up to 200K tokens), it is intended for high-volume applications such as data analysis, code synthesis, search, and workflow orchestration. Limitations include no image or audio output and no video understanding or production.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "37": {"model": "claude-opus-4-20250514", "description": "Claude Opus 4 (May 2025) is Anthropic's flagship large multimodal model, boasting advanced reasoning, coding abilities, and sophisticated tool use. Built on a large transformer architecture, it offers a 200K token context window and is designed for enterprise-grade AI agent applications and autonomous multi-step workflows. Opus 4 delivers top-tier performance on complex tasks, including research synthesis and orchestrating cross-functional activities, with vision (image analysis) support but no image, audio, or video generation features.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "38": {"model": "claude-sonnet-4-20250514", "description": "Claude Sonnet 4 (May 2025) is a mid-size transformer model in Anthropic\u2019s Claude portfolio, optimized for quality, responsiveness, and cost-efficiency. With a 200K token context window, it tackles high-volume use cases such as workflow automation, data analysis, and code generation. Sonnet 4 supports robust text generation, advanced reasoning, and image understanding (vision input), but does not create images, audio, or video. It is well-suited for scalable task-specific applications within larger AI systems.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 200000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "39": {"model": "gemini-2.5-flash-preview-05-20", "description": "Gemini 2.5 Flash (Preview 05-20) is a multimodal language model released by Google in 2025 as part of the Gemini family. It is optimized for rapid response and efficiency in handling text, image, video, and audio inputs, with a context window sufficient for most practical applications (reportedly up to 1 million tokens in the Gemini 1.5 family; specifics for 2.5 are not fully disclosed but are similar). The Flash variant is designed for price-performance and supports advanced reasoning and step-by-step thinking capabilities. It is suited for conversational agents, document analysis, and multimodal data tasks, but lacks image, audio, and video generation functionalities. Intended for scalable, production-grade deployments.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "40": {"model": "gemini-2.5-pro-exp-03-25", "description": "Gemini 2.5 Pro (Experimental 03-25) is an advanced multimodal LLM by Google, focused on high-level reasoning, complex problem-solving, and robust comprehension across text, image, audio, code, and video. It features an input size limit of 500 MB and supports large context windows (similar to earlier Gemini 1.5 models, likely up to 1 million tokens). Designed for challenging applications such as scientific research, advanced analytics, and enterprise-scale tasks. Known for deep multimodal understanding, but does not support content generation in image, audio, or video formats.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "41": {"model": "gemini-2.5-pro-preview-05-06", "description": "Gemini 2.5 Pro (Preview 05-06) is Google's most advanced reasoning model in the Gemini series, released in 2025. It is capable of handling and integrating information from large, diverse datasets, including text, audio, imagery, video, and code repositories. Its context window is similar to the Gemini 2.5 series (on the order of 1 million tokens). The model is optimized for deep reasoning, multimodal analysis, research, and enterprise use. It can analyze but not generate images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "42": {"model": "gemini-2.0-flash-001", "description": "Gemini 2.0 Flash 001 is an earlier fast-response variant in Google\u2019s Gemini line, designed for both multimodal data ingestion (text, image, audio, video) and rapid inference, but with smaller context windows and fewer advanced features than subsequent Gemini 2.5 models. It is intended for scenarios requiring quick, scalable responses with solid reasoning and multimodal analysis, but it does not support generation of images, audio, or video. Used primarily for chatbots, Q&A, and basic document/image analysis.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 128000, "parameter_count": null, "knowledge_cutoff": "2023-12", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "43": {"model": "gemini-1.5-pro", "description": "Gemini 1.5 Pro is a leading multimodal model from Google, offering advanced reasoning and comprehension across text, documents, code, images, audio, and video. It features a context window of up to 1 million tokens, enabling analysis of large inputs and complex cross-modal interactions. Designed for tasks involving deep research, analytics, or enterprise-grade analysis, it can process but not generate image/audio/video. Released in 2024, it supports conversational use, document/image/video understanding, and complex problem solving.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "44": {"model": "gemini-1.5-flash", "description": "Gemini 1.5 Flash is a fast, efficient multimodal model released by Google in early 2024. It is designed for rapid inference, handling text, images, audio, and video inputs. This model trades off some depth of reasoning for latency, making it suitable for chat assistants and high-throughput applications. It supports large context windows (up to 1 million tokens), with strengths in multimodal content analysis and quick response. The model does not support generating new images, audio, or video.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "45": {"model": "gemini-1.5-flash-8b", "description": "Gemini 1.5 Flash-8B is a smaller, efficient multimodal variant of Gemini Flash, likely featuring 8 billion parameters (exact size not official). Released in 2024, it is engineered for speed and moderate resource usage, supporting text, image, audio, and video analysis but not media generation. It is especially suited for latency-sensitive applications, chatbots, and batch document/image/video processing with a context window similar to the Gemini 1.5 series. Unlike the Pro versions, it sacrifices some analytical depth for efficiency.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 1000000, "parameter_count": "8b", "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "46": {"model": "llama3-8b-8192", "description": "Llama3-8b-8192 is an 8 billion parameter transformer-based language model, featuring an 8,192 token context window. Designed primarily for text-based tasks, it excels in text completion, dialogue, and reasoning. The model supports function calling and is optimized for rapid inference. It is primarily intended for chatbot, code generation, and conversational AI use cases, leveraging Groq\u2019s fast inference hardware for low-latency outputs.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "8B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "47": {"model": "llama3-70b-8192", "description": "Llama3-70b-8192 is a 70 billion parameter transformer-based language model with an 8,192 token context window. It is highly optimized for dialogue, content generation, and complex reasoning tasks. The model maintains a strong MMLU score (79.5%) and supports function calling and tool use. Designed for production-ready, fast, and consistent outputs, it is widely used for advanced chatbots, research, and large-scale automation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "48": {"model": "mixtral-8x7b-32768", "description": "Mixtral-8x7b-32768 is a mixture-of-experts model with 8 experts of 7B parameters each (total active parameters per forward pass: ~12.9B), featuring a 32,768 token context window. It is designed for advanced text generation, reasoning, and cost-effective inference at scale. The model focuses on open-ended dialogue, code generation, and large-document processing, leveraging Groq\u2019s speed for real-time interaction.", "provider": "google", "temperature_range": [], "context_window": 32768, "parameter_count": "8x7B (active ~12.9B per pass)", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "49": {"model": "gemma2-9b-it", "description": "Gemma2-9b-it is a 9 billion parameter language model from Google\u2019s Gemma 2 series, likely deployed on Groq for fast inference. It features a transformer architecture and is optimized for instruction following and text-based tasks. The model supports text completion and reasoning, focusing on instruction-based dialogue, summarization, and knowledge extraction. Multimodal or image/audio/video capabilities are not available.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "9B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "50": {"model": "llama-3.3-70b-versatile", "description": "Llama-3.3-70b-versatile is a 70 billion parameter transformer-based model, likely a variant of Meta\u2019s Llama 3, optimized for versatile use cases with a large context window. It supports advanced reasoning, function calling, and tool use. The model is designed for complex language understanding, automation, and research applications, but it does not support image, audio, or video analysis or generation.", "provider": "google", "temperature_range": [], "context_window": 8192, "parameter_count": "70B", "knowledge_cutoff": "Recent", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "53": {"model": "sonar-reasoning", "description": "Sonar-reasoning is likely a variant of the Sonar model, optimized for logical reasoning tasks. It is built on top of the Llama 3.3 70B architecture and focuses on providing accurate and factual responses. It does not support multimodal inputs like image or audio analysis. The model is part of Perplexity's suite, designed for real-time information retrieval and high-quality responses.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "54": {"model": "sonar-pro", "description": "Sonar-Pro is a variant of the Sonar model, designed for professional users. It is built on the Llama 3.3 70B architecture and enhances factuality and readability, making it suitable for interactive applications. It does not support multimodal inputs like image or audio analysis. It is optimized for speed and real-time information access.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "55": {"model": "sonar", "description": "Sonar is a cutting-edge AI model built on the Llama 3.3 70B architecture. It is optimized for enhanced factuality, readability, and speed. Sonar provides real-time access to information and supports text-based interactions but lacks multimodal capabilities like image or audio analysis. It is designed for fast and accurate responses in search environments.", "provider": "perplexity", "temperature_range": [0.0, 1.0], "context_window": 131072, "parameter_count": null, "knowledge_cutoff": null, "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "56": {"model": "gpt-4.1-mini", "description": "GPT-4.1 Mini is a mid-sized transformer-based large language model developed by OpenAI, released in April 2025. It features a 1 million token context window and delivers performance competitive with GPT-4o but with significantly reduced latency and cost. GPT-4.1 Mini supports both text and image understanding (multimodal input for vision tasks), strong code generation, and advanced instruction following. It is suitable for interactive applications requiring high throughput, but it does not generate images, audio, or video, and lacks audio/video analysis or synthesis capabilities. Fine-tuning is supported for domain-specific tasks.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 1000000, "parameter_count": null, "knowledge_cutoff": "2025-04", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "57": {"model": "gemini-2.5-pro-preview-tts", "description": "Gemini 2.5 Pro Preview TTS is a large multimodal language model from Google, released in June 2025. It is designed primarily for advanced text-to-speech (TTS) generation, supporting both single and multi-speaker audio outputs. The model offers fine-grained control over style, emotion, pace, accent, and pronunciation in generated speech, with a 32k token context window. Gemini 2.5 Pro Preview TTS accepts only text input and outputs audio, making it ideal for podcasts, audiobooks, and dynamic audio content. It does not process images, video, or audio inputs, nor does it generate images or video.", "provider": "google", "temperature_range": [0.0, 2.0], "context_window": 32000, "parameter_count": null, "knowledge_cutoff": "2025-06", "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": false}, "58": {"model": "gemini-2.5-flash-preview-tts", "description": "Gemini 2.5 Flash Preview TTS is a text-to-speech model supporting over 80 languages and enabling podcast creation with multiple speakers. It is designed for efficient performance and offers dynamic control over its thinking budget. The model is part of the Gemini 2.5 family, which excels in reasoning and multimodal capabilities. It is optimized for speed and cost-effectiveness.", "provider": "google", "temperature_range": [], "context_window": 1, "parameter_count": null, "knowledge_cutoff": null, "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": true, "video_analysis": false, "video_gen": false, "reasoning": true}, "59": {"model": "openai/whisper-base", "description": "openai/whisper-base is a 74 million parameter Transformer-based encoder-decoder (sequence-to-sequence) model from OpenAI in the Whisper family, designed for automatic speech recognition (ASR) and speech translation. It processes audio inputs and outputs transcription or translation text, optionally with timestamps. The model is available in English-only and multilingual configurations, supports 96 languages, and is optimized for general-purpose speech transcription and translation. Its primary use case is robust audio-to-text modeling; it does not support text, image, or video generation or analysis beyond audio. Released in 2022, its context window consists of approximately 30 seconds of audio per inference batch.", "provider": "openai", "temperature_range": [0.0, 1.0], "context_window": 448, "parameter_count": "74M", "knowledge_cutoff": "2022-09", "text_completion": false, "image_analysis": false, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "61": {"model": "dall-e-3", "description": "DALL-E 3 is a text-to-image generative model developed by OpenAI, released in 2023 as a major improvement over previous versions. It is built on a transformer architecture related to GPT-3, focusing on producing highly detailed and accurate images from complex textual prompts. While its parameter count and exact context window are undisclosed, DALL-E 3 is designed exclusively for image generation and refinement based on detailed natural language instructions. It supports integration with ChatGPT Plus for conversational, iterative image creation; however, it does not natively support text, audio, or video analysis or generation. Its primary use case is creative, designer-level image synthesis, and prompt-based visual art generation.", "provider": "openai", "temperature_range": [0.0, 1.0], "context_window": 0, "parameter_count": null, "knowledge_cutoff": "2023-10", "text_completion": false, "image_analysis": false, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "63": {"model": "imagen-3.0-generate-002", "description": "Imagen 3.0 (model: imagen-3.0-generate-002) is a state-of-the-art text-to-image diffusion model developed by Google DeepMind, released in 2024 and integrated into Google Gemini and other platforms. The model specializes in generating high-resolution (up to 2048x2048px), photorealistic, and visually faithful images from natural language prompts. It supports nuanced control over composition and visual style, as well as multi-subject scene synthesis, but does not process text, audio, or video inputs. The model is not designed for text, audio, or video analysis or generation tasks. Intended uses include creative image generation, storyboarding, design prototyping, and advertising. Technical specifications beyond its generative focus (such as architecture details or parameter size) are not disclosed by Google.", "provider": "google", "temperature_range": [0.0, 1.0], "context_window": 0, "parameter_count": null, "knowledge_cutoff": "2025-07", "text_completion": false, "image_analysis": false, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "64": {"model": "Jlonge4/flux-dev-fp8", "description": "Jlonge4/flux-dev-fp8 is a complete HuggingFace Diffusers pipeline implementing the Flux Dev 1 architecture, optimized for FP8 (8-bit floating point) precision. Released in March 2025 and designed for AWS SageMaker and ComfyUI integration, its primary capability is high-performance text-to-image generation, particularly for ComfyUI workflows. It does not support text-only completion, nor multimodal analysis (audio/video), but is specifically engineered for efficient image synthesis. The model requires a GPU with at least 16GB VRAM supporting FP8 operations. Intended use cases are image generation tasks within automated UI or cloud deployments; notable features include custom deployment packaging and direct ComfyUI endpoint integration. Other modalities, such as text, audio, or video analysis and generation, are not supported.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 0, "parameter_count": null, "knowledge_cutoff": "2025-03-09", "text_completion": false, "image_analysis": false, "image_gen": true, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": false}, "65": {"model": "gpt-oss:latest", "description": "GPT-OSS is an open-source large language model series released by OpenAI and integrated by Ollama, available in two primary sizes: gpt-oss-20b (21B parameters) and gpt-oss-120b (117B parameters). Both use a Mixture-of-Experts (MoE) architecture with 4-bit (MXFP4) quantization for efficient inference. The models are optimized for instruction following, chain-of-thought reasoning, and agentic capabilities such as function calling and tool use. They support text completion and structured chats, and can run on a range of hardware\u2014from consumer RTX GPUs (20b) to high-end servers (120b). There is support for extended context window sizes, though the exact maximum is not specified. The primary intended use is as a general-purpose text LLM for reasoning, code, and workflow automation. The models do not natively support image, audio, or video analysis or generation.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32768, "parameter_count": "21B/117B", "knowledge_cutoff": "2025-08", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "66": {"model": "cogito:32b", "description": "Cogito:32B is a 32-billion parameter large language model released under an open license as part of the Cogito v1 Preview by DeepCogito in April 2025. Built using a transformer-based architecture, it was trained via Iterated Distillation and Amplification (IDA), emphasizing scalable self-improvement and alignment. The model is designed for advanced natural language understanding, answering questions, and self-reflective reasoning. Cogito:32B is available on Ollama and HuggingFace, focused on general-purpose text generation and reasoning tasks, but does not support image, audio, or video inputs.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32768, "parameter_count": "32B", "knowledge_cutoff": "2025-04", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "68": {"model": "llama4:16x17b", "description": "Llama 4:16x17b is a Mixture-of-Experts (MoE) large language model released by Meta and distributed via Ollama. It features 16 expert pathways with a total of 17 billion active parameters and natively supports both text and image inputs (multimodality), optimized for tasks such as visual recognition, image reasoning, captioning, advanced text generation, code, and multilingual understanding. The architecture leverages a large context window of up to 128K tokens. Llama 4 models also support tool (function) calling with real-time streaming and are designed for sophisticated reasoning and instruction following; however, they do not provide native audio or video generation/analysis capabilities. Llama 4:16x17b was released in early 2024 and is intended for research, production assistants, visual reasoning, and complex problem solving.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "16x17B (MoE)", "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "69": {"model": "phi4:14b", "description": "Phi-4-14B is a dense decoder-only transformer language model developed by Microsoft Research with 14 billion parameters. Released in January 2024, it is optimized for reasoning, mathematics, and natural language understanding, with notable performance in instruction following. It supports only text inputs and outputs (no multimodal capabilities) and is suitable for chat, code, and general NLP tasks. Context window size is not definitively documented, but optimal operation is recommended with 16GB+ GPU VRAM. The model does not natively support image, audio, or video inputs and lacks multimodal or function-calling capabilities present in newer variants like Phi-4-multimodal.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 8000, "parameter_count": "14B", "knowledge_cutoff": "2024-01", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "70": {"model": "llama3.2:latest", "description": "Llama 3.2:latest, available via Ollama, is a Meta-developed multilingual large language model (LLM). It is released in 1B and 3B parameter sizes for text-only (text in/text out) tasks, with a context window of 128K tokens. This model specializes in text generation, comprehension, multilingual tasks, and basic reasoning. Vision-capable (multimodal) variants are separately designated (e.g., llama3.2-vision), but the standard 'llama3.2:latest' model supports only text-based functionality. Released in late 2024, Llama 3.2 targets private, efficient on-device text applications.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "1B, 3B (text-only variant)", "knowledge_cutoff": "2024-09", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "71": {"model": "gemma3:27b", "description": "Gemma 3 27B is a large-scale, multimodal language model developed by Google and deployable on platforms such as Ollama. It is built with 27 billion parameters and supports both text and vision-language tasks, including image analysis. The model features a 128k-token context window, strong multilingual abilities (over 140 languages), advanced reasoning (including math and code), and supports function/tool calling via structured outputs. Released in March 2025, its primary use cases include AI assistants, code and document generation, vision-text workflows, and business process automation. Notable limitations include an absence of direct capabilities for audio or video generation and audio analysis.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "27B", "knowledge_cutoff": "2025-03", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": true, "video_gen": false, "reasoning": true}, "72": {"model": "llama3.3:latest", "description": "Llama 3.3:latest is a text-only, multilingual large language model (LLM) based on Meta's Llama 3 architecture, available through Ollama. It is available in instruction-tuned variants up to 70B parameters, supports a context window of up to 128,000 tokens, and is optimized for text generation, dialogue, and reasoning tasks. Llama 3.3:latest was released about 8 months ago (circa late 2024), and is designed primarily for advanced language understanding and generation in multiple languages. It does not natively support image, audio, or video processing, nor does it offer function calling or multimodal features natively, making it unsuitable for tasks involving media analysis or synthesis.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 128000, "parameter_count": "70B", "knowledge_cutoff": "2024-12", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "73": {"model": "llava:34b", "description": "LLaVA:34B is a large multimodal model developed for Ollama, based on the LLaVA (Large Language-and-Vision Assistant) architecture, which integrates a vision encoder with a Vicuna-based language model. It features 34 billion parameters and supports multimodal interaction by accepting both text and image inputs, offering advanced capabilities in visual reasoning, OCR, and general language generation. The context window accommodates multiple images at high resolution (up to 1344x336 or 672x672). The model was updated to version 1.6 in early 2024, improving visual understanding, logical reasoning, and session management. It is primarily designed for tasks requiring combined visual and textual context, such as detailed image analysis, document understanding, and conversational AI with multimodal inputs. It does not support generating images, audio, or video content.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 4096, "parameter_count": "34B", "knowledge_cutoff": "2024-02", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "74": {"model": "llama3.2-vision:11b", "description": "Llama 3.2-vision:11b is an 11-billion parameter multimodal large language model from Meta, supporting both text and image inputs with text-based outputs. It features a context window of up to 128,000 tokens and is designed to handle tasks such as image captioning, visual question answering, document understanding, and text generation. The model integrates an image encoder with cross-attention layers, enabling advanced image analysis and reasoning over visual data. Released in 2024 as part of the Llama 3.2 series, it is suitable for applications including education, business automation, data extraction, and interactive AI systems. Notably, it does not support direct image or video generation, nor does it handle audio inputs or outputs.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 128000, "parameter_count": "11B", "knowledge_cutoff": "2024-06", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "75": {"model": "mistral:latest", "description": "Mistral (mistral:latest) on Ollama is a 7-billion parameter transformer-based language model, distributed under the Apache license and released initially in September 2023, with version 0.3 (May 2024) supporting function calling. It operates in instruction-following and text completion modes, features a 32,000 token context window in Ollama, and is optimized for local deployment with high performance in code generation, data extraction, and multilingual support. This model is text-only\u2014no image, audio, or video processing\u2014and is primarily intended for text-based tasks such as chatbots, data analytics, and local knowledge bases. Notable limitations include the lack of multimodal (image/audio/video) input/output capabilities.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32000, "parameter_count": "7B", "knowledge_cutoff": "2024-05-22", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "77": {"model": "qwen3:30b", "description": "Qwen3-30B-A3B is a 30.5B parameter Mixture-of-Experts (MoE) large language model from Alibaba, designed as a causal language model with 48 layers and a native context window of 32,768 tokens (expandable to 131,072 with YaRN). Deployed through Ollama (and compatible with tools like llama.cpp), it excels at text generation, reasoning, and agentic tasks with tool-calling via JSON-defined interfaces. Notable for its enhanced logical reasoning, code generation, and agent integration, it supports over 100 languages but does not provide multimodal capabilities such as image or audio processing. Released in mid-2025, it is optimized for language understanding, tool use, coding assistance, and multilingual scenarios.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32768, "parameter_count": "30.5B", "knowledge_cutoff": "2025-07", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "81": {"model": "qwen3:14b", "description": "Qwen3-14B is a dense causal language model with 14.8 billion parameters (13.2B for non-embedding), 40 layers, and a context window of 32,768 tokens natively (expandable to 131,072 tokens with YaRN). It supports advanced multilingual instruction following and translation in 100+ languages, as well as reasoning, code generation, and multi-turn dialogue, but it is not multimodal (no image, audio, or video input capabilities). Released in 2025, its primary focus is on advanced text applications requiring robust reasoning and multilingual support, especially in LLM deployments via Ollama.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32768, "parameter_count": "14.8B", "knowledge_cutoff": "2025-06", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "82": {"model": "llava:7b", "description": "LLaVA:7b is a 7-billion-parameter multimodal model that combines a vision encoder with a language backbone (Vicuna 7B) for joint image and text understanding, intended for general visual reasoning and conversation tasks. Featured in the Ollama library as version 1.6, it supports higher image input resolutions (up to 672x672 or equivalent) and improved OCR and visual reasoning, but its performance is limited compared to larger variants. Its primary use cases include analyzing images in conjunction with natural language prompts, document and chart understanding, and multimodal chat. The context window size typically matches Vicuna 7B (around 4k tokens). It does not natively generate images, audio, or video, and is mainly focused on multimodal text-image understanding.", "provider": "ollama", "temperature_range": [0.0, 1.0], "context_window": 4096, "parameter_count": "7B", "knowledge_cutoff": "2023-08", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "83": {"model": "llava:13b", "description": "LLaVA (Large Language-and-Vision Assistant) 13B is a multimodal transformer-based model with 13 billion parameters, designed for both text and image understanding. Running locally via Ollama, it supports input of both text and images\u2014enabling capabilities such as image captioning, visual question answering, object detection, text recognition within images, and contextual visual reasoning. LLaVA 13B (notably as of version 1.6, released in early 2024) does not generate images, audio, or video, and is intended for multimodal assistant tasks including document and chart analysis, scene interpretation, and text-based reasoning. The model's context window is not explicitly specified, but typical for models of its architecture (LLaMA 2-based). It is distributed under a community or open license.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 4096, "parameter_count": "13B", "knowledge_cutoff": "2024-01", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "84": {"model": "qwen2.5vl:32b", "description": "Qwen2.5-VL-32B is a large multimodal language model developed by Qwen and available on Ollama, featuring 32 billion parameters. It supports both text and image inputs for tasks such as image captioning, visual question answering, and document understanding, but does not support image generation, audio, or video processing. The model can process images of various resolutions and supports context windows up to at least 8,000 tokens for long-form generation. It was released in 2025 and is designed for advanced multimodal reasoning and complex text/image understanding.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 8000, "parameter_count": "32b", "knowledge_cutoff": "2025-07", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "85": {"model": "minicpm-v:8b", "description": "MiniCPM-Llama3-V 2.5 is a multimodal large language model with 8B parameters, built using SigLip-400M and Llama3-8B-Instruct architectures. Released in mid-2025, it supports both text and image inputs, featuring advanced OCR, table understanding, and robust reasoning abilities. The model can process images of up to 1.8 million pixels, supports complex instruction following, and is primarily designed for use cases involving multimodal document understanding, visual reasoning, and interactive AI assistants. Notable limitations include lack of native image or audio generation capabilities. Context window details are not specified in public sources.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 32000, "parameter_count": "8B", "knowledge_cutoff": "2025-07", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "87": {"model": "cogito:14b", "description": "Cogito v1 14B is a 14-billion parameter large language model (LLM) released under an open license, intended for high-performance local and API-based text generation. It is based on transformer architecture and trained using Iterated Distillation and Amplification (IDA) for scalable alignment and advanced reasoning. The model supports standard LLM interactions including complex logical reasoning and self-reflection before answering but is not multimodal. It is available through Ollama with support for local inference. Its primary use cases include general conversational AI, reasoning tasks, and research. Context window and other specific limits are not explicitly published as of April 2025.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 0, "parameter_count": "14B", "knowledge_cutoff": "2025-04", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "89": {"model": "gpt-5", "description": "GPT-5 (OpenAI, released August 2025) is a unified multimodal architecture available in three variants (regular, mini, nano), designed to support sophisticated text reasoning and complex analytical tasks. It supports both text and image as input, with a maximum input context window of 272,000 tokens and output limit of 128,000 tokens. Capabilities include deep reasoning, multi-turn voice conversations, real-time web search integration, and a creative canvas workspace. Knowledge cutoff is September 30, 2024. Primary use cases include advanced dialogue, research, creative ideation, and multimodal analysis. Notable features: dynamic model selection, context-aware mode switching, support for custom grammars, and a focus on a seamless, holistic AI workbench. Limitations: image output, audio and video generation/analysis are not yet supported.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 272000, "parameter_count": null, "knowledge_cutoff": "2024-09-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": true, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "90": {"model": "gpt-5-mini", "description": "GPT-5-mini is a scaled-down variant of OpenAI's GPT-5 family, designed for efficient real-time applications with minimal latency. Released in August 2025, its knowledge cutoff is May 30th, 2024. The model offers a context window of 272,000 tokens for input and 128,000 tokens for output. GPT-5-mini supports both text and image inputs, but only generates text as output. It is available for direct API access at various reasoning levels (minimal, low, medium, high), accommodating a wide range of task complexities. Its primary focus is on scalable, high-throughput text and multimodal reasoning tasks. Notable limitations include the absence of image, audio, and video generation or audio/video analysis capabilities.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 272000, "parameter_count": null, "knowledge_cutoff": "2024-05-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "91": {"model": "gpt-5-nano", "description": "GPT-5 Nano is a developer-focused, compact variant of OpenAI's GPT-5 family. It is designed for efficiency and high-throughput applications, supporting text and image inputs, with text-only output. The model supports function calling, agentic tool use, and offers developer controls such as adjustable reasoning effort levels and verbosity control. It has a context window of up to 400,000 tokens. Its knowledge cutoff is May 30, 2024. Primary use cases include scalable text generation, reasoning tasks, and agentic workflows, but it does not generate images, audio, or video.", "provider": "openai", "temperature_range": [0.0, 2.0], "context_window": 400000, "parameter_count": null, "knowledge_cutoff": "2024-05-30", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "92": {"model": "llama3.3:70b", "description": "Llama 3.3:70B is a large language model in the Llama family, developed by Meta and hosted on Ollama, featuring 70 billion parameters. It is a pretrained and instruction-tuned transformer-based model, optimized for multilingual dialogue and text completion tasks across supported languages (including English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai). The model supports only text in/text out functionality and is not designed for multimodal applications (no image, audio, or video support). It introduces a longer context window and offers strong performance in code generation, summarization, and logical reasoning. Released in December 2024, Llama 3.3:70B targets chat, document processing, and multilingual conversational AI, but does not natively support tool use or function calling. Developers may integrate it with external tools, but such integration is not inherent in the architecture.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 1000000, "parameter_count": "70B", "knowledge_cutoff": "2024-12", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "97": {"model": "gpt-oss:120b", "description": "gpt-oss:120b is a large open-weight transformer-based language model from OpenAI, featuring 117B parameters and built with a Mixture-of-Experts (MoE) architecture utilizing 128 experts per layer. It features a 16,384-token context window (some sources mention up to 128k tokens) and leverages 4.25-bit quantization (MXFP4) for efficient inference on capable hardware. Released in August 2025, the model is purpose-built for advanced text generation, reasoning, tool use (via function calling), and developer applications. It does not support image, audio, or video analysis or generation natively; its design focus is on robust text-based tasks, agentic workflows, and structured chat. The model is compatible with Ollama, vLLM, and OpenAI-style APIs, and is intended for local or managed deployments.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 16384, "parameter_count": "117B", "knowledge_cutoff": "2025-08", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "98": {"model": "claude-opus-4-1-20250805", "description": "Claude Opus 4.1, released by Anthropic on August 5, 2025, is a large language model designed for advanced reasoning, coding, agentic tasks, and research-grade content synthesis. It operates via a transformer architecture (specific parameter count undisclosed) and supports a large context window and long outputs (up to 32K tokens). The model offers hybrid reasoning capabilities, excelling at multi-step logical analysis, code generation across large files, and complex business or research workflows. It is accessible via API and major cloud platforms. Claude Opus 4.1 does not natively support image, audio, or video processing or generation\u2014it is strictly text and code oriented, with primary use cases in software engineering, intensive research, and agentic workflows.", "provider": "anthropic", "temperature_range": [0.0, 1.0], "context_window": 32000, "parameter_count": null, "knowledge_cutoff": "2025-05", "text_completion": true, "image_analysis": false, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}, "99": {"model": "llama4:latest", "description": "Llama 4 (llama4:latest) is a natively multimodal large language model developed by Meta and available on Ollama. It utilizes a mixture-of-experts (MoE) architecture, featuring up to 109 billion parameters with 17 billion active parameters per forward pass. Llama 4 supports both text and image inputs (multilingual text and images) and can produce multilingual text and code outputs. Its context window is 10 million tokens. Released in 2024, the model is optimized for text generation, visual recognition, image reasoning, captioning, and answering questions about images. It does not generate images, audio, or video content, and is primarily designed for complex reasoning, multilingual tasks, and visual-language understanding.", "provider": "ollama", "temperature_range": [0.0, 2.0], "context_window": 10000000, "parameter_count": "109B (17B active per forward pass, MoE)", "knowledge_cutoff": "2024-04", "text_completion": true, "image_analysis": true, "image_gen": false, "audio_analysis": false, "audio_gen": false, "video_analysis": false, "video_gen": false, "reasoning": true}}}